{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML2021Spring - HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **Homework 1: COVID-19 Cases Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeZnPAiwDRWG"
      },
      "source": [
        "Author: Heng-Jui Chang\n",
        "\n",
        "Slides: https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.pdf  \n",
        "Video: TBA\n",
        "\n",
        "Objectives:\n",
        "* Solve a regression problem with deep neural networks (DNN).\n",
        "* Understand basic DNN training tips.\n",
        "* Get familiar with PyTorch.\n",
        "\n",
        "If any questions, please contact the TAs via TA hours, NTU COOL, or email.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx3x1nDkG-Uy"
      },
      "source": [
        "# **Download Data**\n",
        "\n",
        "\n",
        "If the Google drive links are dead, you can download data from [kaggle](https://www.kaggle.com/c/ml2021spring-hw1/data), and upload data manually to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMj55YDKG6ch"
      },
      "source": [
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "\n",
        "# !gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
        "# !gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "# prevent from using gpu working on other's tasks in lab\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "myseed = 52728  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"cuda_available\")\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda_available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtE3b6JEH7rw"
      },
      "source": [
        "# **Some Utilities**\n",
        "\n",
        "You do not need to modify this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWMT3uf1NGQp"
      },
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "    ''' Plot prediction of your DNN '''\n",
        "    if preds is None or targets is None:\n",
        "        model.eval()\n",
        "        preds, targets = [], []\n",
        "        for x, y in dv_set:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with torch.no_grad():\n",
        "                pred = model(x)\n",
        "                preds.append(pred.detach().cpu())\n",
        "                targets.append(y.detach().cpu())\n",
        "        preds = torch.cat(preds, dim=0).numpy()\n",
        "        targets = torch.cat(targets, dim=0).numpy()\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    plt.xlabel('ground truth value')\n",
        "    plt.ylabel('predicted value')\n",
        "    plt.title('Ground Truth v.s. Prediction')\n",
        "    plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39U_XFX6KOoj"
      },
      "source": [
        "# **Preprocess**\n",
        "\n",
        "We have three kinds of datasets:\n",
        "* `train`: for training\n",
        "* `dev`: for validation\n",
        "* `test`: for testing (w/o target value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ-MdwpLL7Dt"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "The `COVID19Dataset` below does:\n",
        "* read `.csv` files\n",
        "* extract features\n",
        "* split `covid.train.csv` into train/dev sets\n",
        "* normalize features\n",
        "\n",
        "Finishing `TODO` below might make you pass medium baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zlpIp9ANJRU"
      },
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    def __init__(self,\n",
        "                 path,\n",
        "                 mode='train',\n",
        "                 target_only=False):\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "        \n",
        "        if not target_only:\n",
        "            feats = list(range(93))\n",
        "        else:\n",
        "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n",
        "            feats = list(range(40)) + [57, 75] + list(range(76, 76+4)) + list(range(40, 40+12))\n",
        "            pass\n",
        "\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            self.data = torch.FloatTensor(data)\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            data = data[:, feats]\n",
        "            \n",
        "            # Splitting training data into train & dev sets\n",
        "            if mode == 'train':\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
        "            \n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        # Normalize features (you may remove this part to see what will happen)\n",
        "        # self.data[:, 40:] = \\\n",
        "        #     (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\n",
        "        #     / self.data[:, 40:].std(dim=0, keepdim=True)\n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlhTlkE7MDo3"
      },
      "source": [
        "## **DataLoader**\n",
        "\n",
        "A `DataLoader` loads data from a given `Dataset` into batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhLk5t6MBX3"
      },
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False, split_indices=None, mean=0, std=0, ae=None, device='cuda'):\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size,\n",
        "        shuffle=(mode != 'test'), drop_last=False,\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "    return dataloader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuycwR0MeQB"
      },
      "source": [
        "# **Deep Neural Network**\n",
        "\n",
        "`NeuralNet` is an `nn.Module` designed for regression.\n",
        "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
        "This module also included a function `cal_loss` for calculating loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AE(nn.Module):\n",
        "    def __init__(self, input_dim, feat_dim):\n",
        "        super(AE, self).__init__()\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        act = nn.SELU\n",
        "        # act = nn.ReLU\n",
        "        self.feat_dim = feat_dim\n",
        "        \n",
        "        num_1 = 70\n",
        "        num_2 = 30\n",
        "\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(input_dim, num_1),\n",
        "            act(),\n",
        "            nn.Linear(num_1, num_2),\n",
        "            act(),\n",
        "            nn.Linear(num_2, feat_dim),\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            act(),\n",
        "            nn.Linear(feat_dim, num_2),\n",
        "            act(),\n",
        "            nn.Linear(num_2, num_1),\n",
        "            act(),\n",
        "            nn.Linear(num_1, input_dim),\n",
        "        )\n",
        "        self.net = nn.Sequential(self.enc, self.dec)\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "        self.init_w()\n",
        "    \n",
        "    def init_w(self):\n",
        "        for param in self.net.parameters():\n",
        "            # biases zero\n",
        "            if len(param.shape) == 1:\n",
        "                nn.init.constant_(param, 0)\n",
        "            # others using lecun-normal initialization\n",
        "            else:\n",
        "                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='linear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        # TODO: you may implement L2 regularization here\n",
        "        return self.criterion(pred, target)\n",
        "    \n",
        "    def encode(self, x):\n",
        "        return self.enc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49-uXYovOAI0"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim, ae):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.seq2 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ae.feat_dim+40, 15),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(15, 1)\n",
        "        )\n",
        "\n",
        "        self.enc = ae.enc\n",
        "        self.net = nn.Sequential(\n",
        "            self.enc,\n",
        "            self.seq2\n",
        "        )\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        f = self.enc(x[..., 40:])\n",
        "        i = torch.cat((x[..., :40], f), 1)\n",
        "        return self.seq2(i).squeeze(1)\n",
        "\n",
        "        # return self.net(x).squeeze(1)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        return self.criterion(pred, target)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvFWVjZ5Nvga"
      },
      "source": [
        "# **Train/Dev/Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAM8QecJOyqn"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOqcmYzMO7jB"
      },
      "source": [
        "def train(tr_set, dv_set, tt_set, model, ae, config, device):\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    fix_ae = config['fix_ae']\n",
        "    # Setup optimizer\n",
        "    if fix_ae:\n",
        "        optim_reg = getattr(torch.optim, config['optimizer'])(\n",
        "            model.seq2.parameters(), **config['optim_hparas'])\n",
        "    else:\n",
        "        optim_reg = getattr(torch.optim, config['optimizer'])(\n",
        "            model.parameters(), **config['optim_hparas'])\n",
        "    optim_ae = getattr(torch.optim, config['optimizer'])(\n",
        "        ae.parameters(), **config['ae_optim_hparas'])\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'ae': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    pretrain = config['pretrain']\n",
        "    aestop = config['aestop']\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        ae.train()\n",
        "        tr_loss = []\n",
        "        ae_loss = []\n",
        "        tt_loss = []\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            \n",
        "            # auto-enc\n",
        "            if epoch <= aestop:\n",
        "                optim_ae.zero_grad()\n",
        "                xx = x[..., 40:]\n",
        "                pred = ae(xx)\n",
        "                mse_loss = ae.cal_loss(pred, xx)\n",
        "                mse_loss.backward()\n",
        "                optim_ae.step()\n",
        "                ae_loss.append(mse_loss.detach().cpu().item())\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    xx = x[..., 40:]\n",
        "                    pred = ae(xx)\n",
        "                    mse_loss = ae.cal_loss(pred, xx)\n",
        "                ae_loss.append(mse_loss.detach().cpu().item())\n",
        "\n",
        "            # regression\n",
        "            if epoch >= pretrain:\n",
        "            # if True:\n",
        "                optim_reg.zero_grad()               # set gradient to zero\n",
        "                pred = model(x)                     # forward pass (compute output)\n",
        "                mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "                mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "                optim_reg.step()                    # update model with optimizer\n",
        "                tr_loss.append(mse_loss.detach().cpu().item())\n",
        "            else:\n",
        "                tr_loss.append(300)\n",
        "        for x in tt_set:\n",
        "            x = x.to(device)[..., 40:]\n",
        "            if not fix_ae:\n",
        "            # if epoch <= aestop:\n",
        "                optim_ae.zero_grad()\n",
        "                pred = ae(x)\n",
        "                mse_loss = ae.cal_loss(pred, x)\n",
        "                mse_loss.backward()\n",
        "                optim_ae.step()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    pred = ae(x)\n",
        "                    mse_loss = ae.cal_loss(pred, x)\n",
        "            tt_loss.append(mse_loss.detach().cpu().item())\n",
        "\n",
        "\n",
        "        loss_record['train'] += tr_loss\n",
        "        loss_record['ae'] += ae_loss\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            print('Saving model (epoch = {:4d}, ae_loss = {:.4f}, tt_loss = {:.4f}, tr_loss = {:.4f}, dv_loss = {:.4f})'\n",
        "                .format(epoch + 1, np.mean(ae_loss), np.mean(tt_loss), np.mean(tr_loss), min_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            torch.save(ae.state_dict(), config['ae_save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            # print('Saving model (epoch = {:4d}, tr_loss = {:.4f}, dv_loss = {:.4f})'\n",
        "            #     .format(epoch + 1, mse_loss, dev_mse))\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ae_train(tr_sets, model, config, device):\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "\n",
        "    # Setup optimizer\n",
        "    reg = config.get('reg')\n",
        "    if reg:\n",
        "    # if False:\n",
        "        print(\"Not apply l2-reg on bias\")\n",
        "        weight_p, bias_p = [], []\n",
        "        for name, p in model.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                bias_p += [p]\n",
        "            else:\n",
        "                weight_p += [p]\n",
        "        optimizer = getattr(torch.optim, config['optimizer'])([\n",
        "            {'params' : weight_p, 'weight_decay' : config['reg']},\n",
        "            {'params' : bias_p, 'weight_decay' : 0}\n",
        "            ], **config['optim_hparas'])\n",
        "    else:\n",
        "        print(\"Apply l2-reg on bias\")\n",
        "        optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "            model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = []      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    save_count = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        tr_loss = []\n",
        "        for s in tr_sets:\n",
        "            for x in s:                     # iterate through the dataloader\n",
        "                x = x[0][..., 40:]\n",
        "                optimizer.zero_grad()               # set gradient to zero\n",
        "                x = x.to(device)   # move data to device (cpu/cuda)\n",
        "                \n",
        "                pred = model(x)                     # forward pass (compute output)\n",
        "                mse_loss = model.cal_loss(pred, x)  # compute loss\n",
        "                mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "                optimizer.step()                    # update model with optimizer\n",
        "                tr_loss.append(mse_loss.detach().cpu().item())\n",
        "        \n",
        "        loss_record += tr_loss\n",
        "\n",
        "        print('Saving model (epoch = {:4d}, tr_loss = {:.4f})'\n",
        "                .format(epoch + 1, np.mean(tr_loss)))\n",
        "\n",
        "        epoch += 1\n",
        "        \n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return loss_record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hSd4Bn3O2PL"
      },
      "source": [
        "## **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrxrD3YsN3U2"
      },
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ae_dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x = x.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, x)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0pdrhQAO41L"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSBMRFlYN5tB"
      },
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ae_test(tt_set, ae, device):\n",
        "    ae.eval()\n",
        "    err = []\n",
        "    for x in tt_set:\n",
        "        x = x.to(device)[..., 40:]\n",
        "        with torch.no_grad():\n",
        "            pred = ae(x)\n",
        "            mse_loss = ae.cal_loss(pred, x)\n",
        "            mse_loss = mse_loss.detach().cpu().item()\n",
        "        err.append(mse_loss)\n",
        "        print(\"err: {:.4f}\".format(mse_loss))\n",
        "    print(np.mean(err))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvckkF5dvf0j"
      },
      "source": [
        "# **Setup Hyper-parameters**\n",
        "\n",
        "`config` contains hyper-parameters for training and the path to save your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPXpdumwPjE7"
      },
      "source": [
        "\n",
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "# print(device)\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "# target_only = True                   # TODO: Using 40 states & 2 tested_positive features\n",
        "target_only = False\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "ae_config = {\n",
        "    'n_epochs': 5000,                # maximum number of epochs\n",
        "    'batch_size': 135,               # mini-batch size for dataloader\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 1e-5,                 # learning rate of SGD\n",
        "        # 'momentum': 0.8,              # momentum for SGD\n",
        "        'weight_decay': 1e-5,\n",
        "    },\n",
        "    'early_stop': 300,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/ae_model.pth'  # your model will be saved here\n",
        "}\n",
        "\n",
        "config = {\n",
        "    'n_epochs': 10000,                # maximum number of epochs\n",
        "    'batch_size': 135,               # mini-batch size for dataloader\n",
        "    'feat_dim': 20,\n",
        "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 1e-4,                 # learning rate of SGD\n",
        "        # 'momentum': 0.8,              # momentum for SGD\n",
        "        'weight_decay': 1e-2,\n",
        "    },\n",
        "    'fix_ae': False,\n",
        "    'ae_optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 1e-5,                 # learning rate of SGD\n",
        "        # 'momentum': 0.3,              # momentum for SGD\n",
        "        'weight_decay': 1e-3,\n",
        "    },\n",
        "    'pretrain': 0,\n",
        "    'aestop': 1000,\n",
        "    'early_stop': 300,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth',  # your model will be saved here\n",
        "    'ae_save_path': 'models/ae_model.pth'  # your model will be saved here\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1eOV3TOH-j"
      },
      "source": [
        "# **Load data and model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNrYBMmePLKm",
        "outputId": "dd60f03a-49fb-43d9-b277-96bb609c0b99"
      },
      "source": [
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished reading the train set of COVID19 Dataset (2430 samples found, each dim = 93)\nFinished reading the dev set of COVID19 Dataset (270 samples found, each dim = 93)\nFinished reading the test set of COVID19 Dataset (893 samples found, each dim = 93)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_ae = lambda : AE(tr_set.dataset.dim-40, config['feat_dim']).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHylSirLP9oh"
      },
      "source": [
        "# ae = AE(tr_set.dataset.dim, config['feat_dim']).to(device)\n",
        "ae = create_ae()\n",
        "model = NeuralNet(tr_set.dataset.dim, ae).to(device)  # Construct model and move to device"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7127\n12425\n"
          ]
        }
      ],
      "source": [
        "print(np.sum([p.flatten().shape for p in model.parameters()]))\n",
        "print(np.sum([p.flatten().shape for p in ae.parameters()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model (epoch = 4567, tr_loss = 0.1833)\n",
            "Saving model (epoch = 4568, tr_loss = 0.1832)\n",
            "Saving model (epoch = 4569, tr_loss = 0.1832)\n",
            "Saving model (epoch = 4570, tr_loss = 0.1832)\n",
            "Saving model (epoch = 4571, tr_loss = 0.1832)\n",
            "Saving model (epoch = 4572, tr_loss = 0.1831)\n",
            "Saving model (epoch = 4573, tr_loss = 0.1832)\n",
            "Saving model (epoch = 4574, tr_loss = 0.1829)\n",
            "Saving model (epoch = 4575, tr_loss = 0.1830)\n",
            "Saving model (epoch = 4576, tr_loss = 0.1829)\n",
            "Saving model (epoch = 4577, tr_loss = 0.1829)\n",
            "Saving model (epoch = 4578, tr_loss = 0.1829)\n",
            "Saving model (epoch = 4579, tr_loss = 0.1828)\n",
            "Saving model (epoch = 4580, tr_loss = 0.1827)\n",
            "Saving model (epoch = 4581, tr_loss = 0.1827)\n",
            "Saving model (epoch = 4582, tr_loss = 0.1826)\n",
            "Saving model (epoch = 4583, tr_loss = 0.1827)\n",
            "Saving model (epoch = 4584, tr_loss = 0.1826)\n",
            "Saving model (epoch = 4585, tr_loss = 0.1823)\n",
            "Saving model (epoch = 4586, tr_loss = 0.1823)\n",
            "Saving model (epoch = 4587, tr_loss = 0.1824)\n",
            "Saving model (epoch = 4588, tr_loss = 0.1824)\n",
            "Saving model (epoch = 4589, tr_loss = 0.1824)\n",
            "Saving model (epoch = 4590, tr_loss = 0.1823)\n",
            "Saving model (epoch = 4591, tr_loss = 0.1823)\n",
            "Saving model (epoch = 4592, tr_loss = 0.1821)\n",
            "Saving model (epoch = 4593, tr_loss = 0.1825)\n",
            "Saving model (epoch = 4594, tr_loss = 0.1821)\n",
            "Saving model (epoch = 4595, tr_loss = 0.1821)\n",
            "Saving model (epoch = 4596, tr_loss = 0.1822)\n",
            "Saving model (epoch = 4597, tr_loss = 0.1820)\n",
            "Saving model (epoch = 4598, tr_loss = 0.1820)\n",
            "Saving model (epoch = 4599, tr_loss = 0.1819)\n",
            "Saving model (epoch = 4600, tr_loss = 0.1818)\n",
            "Saving model (epoch = 4601, tr_loss = 0.1819)\n",
            "Saving model (epoch = 4602, tr_loss = 0.1818)\n",
            "Saving model (epoch = 4603, tr_loss = 0.1816)\n",
            "Saving model (epoch = 4604, tr_loss = 0.1816)\n",
            "Saving model (epoch = 4605, tr_loss = 0.1818)\n",
            "Saving model (epoch = 4606, tr_loss = 0.1817)\n",
            "Saving model (epoch = 4607, tr_loss = 0.1815)\n",
            "Saving model (epoch = 4608, tr_loss = 0.1817)\n",
            "Saving model (epoch = 4609, tr_loss = 0.1814)\n",
            "Saving model (epoch = 4610, tr_loss = 0.1816)\n",
            "Saving model (epoch = 4611, tr_loss = 0.1815)\n",
            "Saving model (epoch = 4612, tr_loss = 0.1814)\n",
            "Saving model (epoch = 4613, tr_loss = 0.1814)\n",
            "Saving model (epoch = 4614, tr_loss = 0.1813)\n",
            "Saving model (epoch = 4615, tr_loss = 0.1816)\n",
            "Saving model (epoch = 4616, tr_loss = 0.1813)\n",
            "Saving model (epoch = 4617, tr_loss = 0.1812)\n",
            "Saving model (epoch = 4618, tr_loss = 0.1812)\n",
            "Saving model (epoch = 4619, tr_loss = 0.1811)\n",
            "Saving model (epoch = 4620, tr_loss = 0.1811)\n",
            "Saving model (epoch = 4621, tr_loss = 0.1811)\n",
            "Saving model (epoch = 4622, tr_loss = 0.1811)\n",
            "Saving model (epoch = 4623, tr_loss = 0.1810)\n",
            "Saving model (epoch = 4624, tr_loss = 0.1810)\n",
            "Saving model (epoch = 4625, tr_loss = 0.1809)\n",
            "Saving model (epoch = 4626, tr_loss = 0.1807)\n",
            "Saving model (epoch = 4627, tr_loss = 0.1809)\n",
            "Saving model (epoch = 4628, tr_loss = 0.1810)\n",
            "Saving model (epoch = 4629, tr_loss = 0.1808)\n",
            "Saving model (epoch = 4630, tr_loss = 0.1807)\n",
            "Saving model (epoch = 4631, tr_loss = 0.1807)\n",
            "Saving model (epoch = 4632, tr_loss = 0.1805)\n",
            "Saving model (epoch = 4633, tr_loss = 0.1805)\n",
            "Saving model (epoch = 4634, tr_loss = 0.1807)\n",
            "Saving model (epoch = 4635, tr_loss = 0.1805)\n",
            "Saving model (epoch = 4636, tr_loss = 0.1807)\n",
            "Saving model (epoch = 4637, tr_loss = 0.1807)\n",
            "Saving model (epoch = 4638, tr_loss = 0.1804)\n",
            "Saving model (epoch = 4639, tr_loss = 0.1805)\n",
            "Saving model (epoch = 4640, tr_loss = 0.1803)\n",
            "Saving model (epoch = 4641, tr_loss = 0.1801)\n",
            "Saving model (epoch = 4642, tr_loss = 0.1802)\n",
            "Saving model (epoch = 4643, tr_loss = 0.1803)\n",
            "Saving model (epoch = 4644, tr_loss = 0.1805)\n",
            "Saving model (epoch = 4645, tr_loss = 0.1801)\n",
            "Saving model (epoch = 4646, tr_loss = 0.1801)\n",
            "Saving model (epoch = 4647, tr_loss = 0.1800)\n",
            "Saving model (epoch = 4648, tr_loss = 0.1800)\n",
            "Saving model (epoch = 4649, tr_loss = 0.1800)\n",
            "Saving model (epoch = 4650, tr_loss = 0.1800)\n",
            "Saving model (epoch = 4651, tr_loss = 0.1798)\n",
            "Saving model (epoch = 4652, tr_loss = 0.1797)\n",
            "Saving model (epoch = 4653, tr_loss = 0.1796)\n",
            "Saving model (epoch = 4654, tr_loss = 0.1797)\n",
            "Saving model (epoch = 4655, tr_loss = 0.1798)\n",
            "Saving model (epoch = 4656, tr_loss = 0.1799)\n",
            "Saving model (epoch = 4657, tr_loss = 0.1798)\n",
            "Saving model (epoch = 4658, tr_loss = 0.1796)\n",
            "Saving model (epoch = 4659, tr_loss = 0.1797)\n",
            "Saving model (epoch = 4660, tr_loss = 0.1796)\n",
            "Saving model (epoch = 4661, tr_loss = 0.1795)\n",
            "Saving model (epoch = 4662, tr_loss = 0.1795)\n",
            "Saving model (epoch = 4663, tr_loss = 0.1796)\n",
            "Saving model (epoch = 4664, tr_loss = 0.1795)\n",
            "Saving model (epoch = 4665, tr_loss = 0.1796)\n",
            "Saving model (epoch = 4666, tr_loss = 0.1794)\n",
            "Saving model (epoch = 4667, tr_loss = 0.1792)\n",
            "Saving model (epoch = 4668, tr_loss = 0.1794)\n",
            "Saving model (epoch = 4669, tr_loss = 0.1792)\n",
            "Saving model (epoch = 4670, tr_loss = 0.1794)\n",
            "Saving model (epoch = 4671, tr_loss = 0.1794)\n",
            "Saving model (epoch = 4672, tr_loss = 0.1794)\n",
            "Saving model (epoch = 4673, tr_loss = 0.1793)\n",
            "Saving model (epoch = 4674, tr_loss = 0.1791)\n",
            "Saving model (epoch = 4675, tr_loss = 0.1791)\n",
            "Saving model (epoch = 4676, tr_loss = 0.1790)\n",
            "Saving model (epoch = 4677, tr_loss = 0.1787)\n",
            "Saving model (epoch = 4678, tr_loss = 0.1790)\n",
            "Saving model (epoch = 4679, tr_loss = 0.1788)\n",
            "Saving model (epoch = 4680, tr_loss = 0.1788)\n",
            "Saving model (epoch = 4681, tr_loss = 0.1787)\n",
            "Saving model (epoch = 4682, tr_loss = 0.1789)\n",
            "Saving model (epoch = 4683, tr_loss = 0.1787)\n",
            "Saving model (epoch = 4684, tr_loss = 0.1786)\n",
            "Saving model (epoch = 4685, tr_loss = 0.1786)\n",
            "Saving model (epoch = 4686, tr_loss = 0.1785)\n",
            "Saving model (epoch = 4687, tr_loss = 0.1785)\n",
            "Saving model (epoch = 4688, tr_loss = 0.1784)\n",
            "Saving model (epoch = 4689, tr_loss = 0.1784)\n",
            "Saving model (epoch = 4690, tr_loss = 0.1782)\n",
            "Saving model (epoch = 4691, tr_loss = 0.1784)\n",
            "Saving model (epoch = 4692, tr_loss = 0.1783)\n",
            "Saving model (epoch = 4693, tr_loss = 0.1783)\n",
            "Saving model (epoch = 4694, tr_loss = 0.1783)\n",
            "Saving model (epoch = 4695, tr_loss = 0.1783)\n",
            "Saving model (epoch = 4696, tr_loss = 0.1782)\n",
            "Saving model (epoch = 4697, tr_loss = 0.1781)\n",
            "Saving model (epoch = 4698, tr_loss = 0.1780)\n",
            "Saving model (epoch = 4699, tr_loss = 0.1780)\n",
            "Saving model (epoch = 4700, tr_loss = 0.1781)\n",
            "Saving model (epoch = 4701, tr_loss = 0.1780)\n",
            "Saving model (epoch = 4702, tr_loss = 0.1780)\n",
            "Saving model (epoch = 4703, tr_loss = 0.1779)\n",
            "Saving model (epoch = 4704, tr_loss = 0.1780)\n",
            "Saving model (epoch = 4705, tr_loss = 0.1780)\n",
            "Saving model (epoch = 4706, tr_loss = 0.1778)\n",
            "Saving model (epoch = 4707, tr_loss = 0.1779)\n",
            "Saving model (epoch = 4708, tr_loss = 0.1777)\n",
            "Saving model (epoch = 4709, tr_loss = 0.1776)\n",
            "Saving model (epoch = 4710, tr_loss = 0.1776)\n",
            "Saving model (epoch = 4711, tr_loss = 0.1775)\n",
            "Saving model (epoch = 4712, tr_loss = 0.1774)\n",
            "Saving model (epoch = 4713, tr_loss = 0.1776)\n",
            "Saving model (epoch = 4714, tr_loss = 0.1775)\n",
            "Saving model (epoch = 4715, tr_loss = 0.1773)\n",
            "Saving model (epoch = 4716, tr_loss = 0.1773)\n",
            "Saving model (epoch = 4717, tr_loss = 0.1774)\n",
            "Saving model (epoch = 4718, tr_loss = 0.1774)\n",
            "Saving model (epoch = 4719, tr_loss = 0.1773)\n",
            "Saving model (epoch = 4720, tr_loss = 0.1772)\n",
            "Saving model (epoch = 4721, tr_loss = 0.1773)\n",
            "Saving model (epoch = 4722, tr_loss = 0.1771)\n",
            "Saving model (epoch = 4723, tr_loss = 0.1770)\n",
            "Saving model (epoch = 4724, tr_loss = 0.1772)\n",
            "Saving model (epoch = 4725, tr_loss = 0.1770)\n",
            "Saving model (epoch = 4726, tr_loss = 0.1769)\n",
            "Saving model (epoch = 4727, tr_loss = 0.1769)\n",
            "Saving model (epoch = 4728, tr_loss = 0.1768)\n",
            "Saving model (epoch = 4729, tr_loss = 0.1769)\n",
            "Saving model (epoch = 4730, tr_loss = 0.1767)\n",
            "Saving model (epoch = 4731, tr_loss = 0.1766)\n",
            "Saving model (epoch = 4732, tr_loss = 0.1766)\n",
            "Saving model (epoch = 4733, tr_loss = 0.1768)\n",
            "Saving model (epoch = 4734, tr_loss = 0.1767)\n",
            "Saving model (epoch = 4735, tr_loss = 0.1768)\n",
            "Saving model (epoch = 4736, tr_loss = 0.1767)\n",
            "Saving model (epoch = 4737, tr_loss = 0.1766)\n",
            "Saving model (epoch = 4738, tr_loss = 0.1766)\n",
            "Saving model (epoch = 4739, tr_loss = 0.1764)\n",
            "Saving model (epoch = 4740, tr_loss = 0.1764)\n",
            "Saving model (epoch = 4741, tr_loss = 0.1765)\n",
            "Saving model (epoch = 4742, tr_loss = 0.1764)\n",
            "Saving model (epoch = 4743, tr_loss = 0.1763)\n",
            "Saving model (epoch = 4744, tr_loss = 0.1764)\n",
            "Saving model (epoch = 4745, tr_loss = 0.1764)\n",
            "Saving model (epoch = 4746, tr_loss = 0.1763)\n",
            "Saving model (epoch = 4747, tr_loss = 0.1762)\n",
            "Saving model (epoch = 4748, tr_loss = 0.1761)\n",
            "Saving model (epoch = 4749, tr_loss = 0.1760)\n",
            "Saving model (epoch = 4750, tr_loss = 0.1761)\n",
            "Saving model (epoch = 4751, tr_loss = 0.1762)\n",
            "Saving model (epoch = 4752, tr_loss = 0.1759)\n",
            "Saving model (epoch = 4753, tr_loss = 0.1759)\n",
            "Saving model (epoch = 4754, tr_loss = 0.1759)\n",
            "Saving model (epoch = 4755, tr_loss = 0.1758)\n",
            "Saving model (epoch = 4756, tr_loss = 0.1758)\n",
            "Saving model (epoch = 4757, tr_loss = 0.1759)\n",
            "Saving model (epoch = 4758, tr_loss = 0.1757)\n",
            "Saving model (epoch = 4759, tr_loss = 0.1758)\n",
            "Saving model (epoch = 4760, tr_loss = 0.1758)\n",
            "Saving model (epoch = 4761, tr_loss = 0.1757)\n",
            "Saving model (epoch = 4762, tr_loss = 0.1757)\n",
            "Saving model (epoch = 4763, tr_loss = 0.1758)\n",
            "Saving model (epoch = 4764, tr_loss = 0.1756)\n",
            "Saving model (epoch = 4765, tr_loss = 0.1756)\n",
            "Saving model (epoch = 4766, tr_loss = 0.1757)\n",
            "Saving model (epoch = 4767, tr_loss = 0.1755)\n",
            "Saving model (epoch = 4768, tr_loss = 0.1755)\n",
            "Saving model (epoch = 4769, tr_loss = 0.1755)\n",
            "Saving model (epoch = 4770, tr_loss = 0.1753)\n",
            "Saving model (epoch = 4771, tr_loss = 0.1753)\n",
            "Saving model (epoch = 4772, tr_loss = 0.1752)\n",
            "Saving model (epoch = 4773, tr_loss = 0.1752)\n",
            "Saving model (epoch = 4774, tr_loss = 0.1753)\n",
            "Saving model (epoch = 4775, tr_loss = 0.1753)\n",
            "Saving model (epoch = 4776, tr_loss = 0.1750)\n",
            "Saving model (epoch = 4777, tr_loss = 0.1751)\n",
            "Saving model (epoch = 4778, tr_loss = 0.1749)\n",
            "Saving model (epoch = 4779, tr_loss = 0.1748)\n",
            "Saving model (epoch = 4780, tr_loss = 0.1747)\n",
            "Saving model (epoch = 4781, tr_loss = 0.1749)\n",
            "Saving model (epoch = 4782, tr_loss = 0.1749)\n",
            "Saving model (epoch = 4783, tr_loss = 0.1748)\n",
            "Saving model (epoch = 4784, tr_loss = 0.1748)\n",
            "Saving model (epoch = 4785, tr_loss = 0.1748)\n",
            "Saving model (epoch = 4786, tr_loss = 0.1748)\n",
            "Saving model (epoch = 4787, tr_loss = 0.1747)\n",
            "Saving model (epoch = 4788, tr_loss = 0.1746)\n",
            "Saving model (epoch = 4789, tr_loss = 0.1746)\n",
            "Saving model (epoch = 4790, tr_loss = 0.1746)\n",
            "Saving model (epoch = 4791, tr_loss = 0.1744)\n",
            "Saving model (epoch = 4792, tr_loss = 0.1745)\n",
            "Saving model (epoch = 4793, tr_loss = 0.1744)\n",
            "Saving model (epoch = 4794, tr_loss = 0.1744)\n",
            "Saving model (epoch = 4795, tr_loss = 0.1743)\n",
            "Saving model (epoch = 4796, tr_loss = 0.1742)\n",
            "Saving model (epoch = 4797, tr_loss = 0.1742)\n",
            "Saving model (epoch = 4798, tr_loss = 0.1743)\n",
            "Saving model (epoch = 4799, tr_loss = 0.1741)\n",
            "Saving model (epoch = 4800, tr_loss = 0.1742)\n",
            "Saving model (epoch = 4801, tr_loss = 0.1741)\n",
            "Saving model (epoch = 4802, tr_loss = 0.1740)\n",
            "Saving model (epoch = 4803, tr_loss = 0.1741)\n",
            "Saving model (epoch = 4804, tr_loss = 0.1740)\n",
            "Saving model (epoch = 4805, tr_loss = 0.1740)\n",
            "Saving model (epoch = 4806, tr_loss = 0.1739)\n",
            "Saving model (epoch = 4807, tr_loss = 0.1738)\n",
            "Saving model (epoch = 4808, tr_loss = 0.1739)\n",
            "Saving model (epoch = 4809, tr_loss = 0.1738)\n",
            "Saving model (epoch = 4810, tr_loss = 0.1737)\n",
            "Saving model (epoch = 4811, tr_loss = 0.1737)\n",
            "Saving model (epoch = 4812, tr_loss = 0.1737)\n",
            "Saving model (epoch = 4813, tr_loss = 0.1737)\n",
            "Saving model (epoch = 4814, tr_loss = 0.1737)\n",
            "Saving model (epoch = 4815, tr_loss = 0.1738)\n",
            "Saving model (epoch = 4816, tr_loss = 0.1735)\n",
            "Saving model (epoch = 4817, tr_loss = 0.1734)\n",
            "Saving model (epoch = 4818, tr_loss = 0.1733)\n",
            "Saving model (epoch = 4819, tr_loss = 0.1736)\n",
            "Saving model (epoch = 4820, tr_loss = 0.1735)\n",
            "Saving model (epoch = 4821, tr_loss = 0.1733)\n",
            "Saving model (epoch = 4822, tr_loss = 0.1733)\n",
            "Saving model (epoch = 4823, tr_loss = 0.1734)\n",
            "Saving model (epoch = 4824, tr_loss = 0.1735)\n",
            "Saving model (epoch = 4825, tr_loss = 0.1733)\n",
            "Saving model (epoch = 4826, tr_loss = 0.1731)\n",
            "Saving model (epoch = 4827, tr_loss = 0.1731)\n",
            "Saving model (epoch = 4828, tr_loss = 0.1731)\n",
            "Saving model (epoch = 4829, tr_loss = 0.1731)\n",
            "Saving model (epoch = 4830, tr_loss = 0.1733)\n",
            "Saving model (epoch = 4831, tr_loss = 0.1730)\n",
            "Saving model (epoch = 4832, tr_loss = 0.1730)\n",
            "Saving model (epoch = 4833, tr_loss = 0.1730)\n",
            "Saving model (epoch = 4834, tr_loss = 0.1731)\n",
            "Saving model (epoch = 4835, tr_loss = 0.1729)\n",
            "Saving model (epoch = 4836, tr_loss = 0.1729)\n",
            "Saving model (epoch = 4837, tr_loss = 0.1727)\n",
            "Saving model (epoch = 4838, tr_loss = 0.1728)\n",
            "Saving model (epoch = 4839, tr_loss = 0.1727)\n",
            "Saving model (epoch = 4840, tr_loss = 0.1726)\n",
            "Saving model (epoch = 4841, tr_loss = 0.1724)\n",
            "Saving model (epoch = 4842, tr_loss = 0.1728)\n",
            "Saving model (epoch = 4843, tr_loss = 0.1727)\n",
            "Saving model (epoch = 4844, tr_loss = 0.1726)\n",
            "Saving model (epoch = 4845, tr_loss = 0.1724)\n",
            "Saving model (epoch = 4846, tr_loss = 0.1727)\n",
            "Saving model (epoch = 4847, tr_loss = 0.1726)\n",
            "Saving model (epoch = 4848, tr_loss = 0.1723)\n",
            "Saving model (epoch = 4849, tr_loss = 0.1722)\n",
            "Saving model (epoch = 4850, tr_loss = 0.1723)\n",
            "Saving model (epoch = 4851, tr_loss = 0.1723)\n",
            "Saving model (epoch = 4852, tr_loss = 0.1722)\n",
            "Saving model (epoch = 4853, tr_loss = 0.1721)\n",
            "Saving model (epoch = 4854, tr_loss = 0.1721)\n",
            "Saving model (epoch = 4855, tr_loss = 0.1722)\n",
            "Saving model (epoch = 4856, tr_loss = 0.1719)\n",
            "Saving model (epoch = 4857, tr_loss = 0.1720)\n",
            "Saving model (epoch = 4858, tr_loss = 0.1722)\n",
            "Saving model (epoch = 4859, tr_loss = 0.1721)\n",
            "Saving model (epoch = 4860, tr_loss = 0.1720)\n",
            "Saving model (epoch = 4861, tr_loss = 0.1719)\n",
            "Saving model (epoch = 4862, tr_loss = 0.1718)\n",
            "Saving model (epoch = 4863, tr_loss = 0.1717)\n",
            "Saving model (epoch = 4864, tr_loss = 0.1717)\n",
            "Saving model (epoch = 4865, tr_loss = 0.1717)\n",
            "Saving model (epoch = 4866, tr_loss = 0.1718)\n",
            "Saving model (epoch = 4867, tr_loss = 0.1716)\n",
            "Saving model (epoch = 4868, tr_loss = 0.1716)\n",
            "Saving model (epoch = 4869, tr_loss = 0.1715)\n",
            "Saving model (epoch = 4870, tr_loss = 0.1714)\n",
            "Saving model (epoch = 4871, tr_loss = 0.1714)\n",
            "Saving model (epoch = 4872, tr_loss = 0.1715)\n",
            "Saving model (epoch = 4873, tr_loss = 0.1716)\n",
            "Saving model (epoch = 4874, tr_loss = 0.1714)\n",
            "Saving model (epoch = 4875, tr_loss = 0.1714)\n",
            "Saving model (epoch = 4876, tr_loss = 0.1715)\n",
            "Saving model (epoch = 4877, tr_loss = 0.1713)\n",
            "Saving model (epoch = 4878, tr_loss = 0.1712)\n",
            "Saving model (epoch = 4879, tr_loss = 0.1712)\n",
            "Saving model (epoch = 4880, tr_loss = 0.1713)\n",
            "Saving model (epoch = 4881, tr_loss = 0.1712)\n",
            "Saving model (epoch = 4882, tr_loss = 0.1712)\n",
            "Saving model (epoch = 4883, tr_loss = 0.1712)\n",
            "Saving model (epoch = 4884, tr_loss = 0.1711)\n",
            "Saving model (epoch = 4885, tr_loss = 0.1710)\n",
            "Saving model (epoch = 4886, tr_loss = 0.1710)\n",
            "Saving model (epoch = 4887, tr_loss = 0.1710)\n",
            "Saving model (epoch = 4888, tr_loss = 0.1709)\n",
            "Saving model (epoch = 4889, tr_loss = 0.1709)\n",
            "Saving model (epoch = 4890, tr_loss = 0.1708)\n",
            "Saving model (epoch = 4891, tr_loss = 0.1711)\n",
            "Saving model (epoch = 4892, tr_loss = 0.1707)\n",
            "Saving model (epoch = 4893, tr_loss = 0.1707)\n",
            "Saving model (epoch = 4894, tr_loss = 0.1707)\n",
            "Saving model (epoch = 4895, tr_loss = 0.1707)\n",
            "Saving model (epoch = 4896, tr_loss = 0.1706)\n",
            "Saving model (epoch = 4897, tr_loss = 0.1706)\n",
            "Saving model (epoch = 4898, tr_loss = 0.1705)\n",
            "Saving model (epoch = 4899, tr_loss = 0.1707)\n",
            "Saving model (epoch = 4900, tr_loss = 0.1706)\n",
            "Saving model (epoch = 4901, tr_loss = 0.1704)\n",
            "Saving model (epoch = 4902, tr_loss = 0.1703)\n",
            "Saving model (epoch = 4903, tr_loss = 0.1703)\n",
            "Saving model (epoch = 4904, tr_loss = 0.1703)\n",
            "Saving model (epoch = 4905, tr_loss = 0.1705)\n",
            "Saving model (epoch = 4906, tr_loss = 0.1703)\n",
            "Saving model (epoch = 4907, tr_loss = 0.1703)\n",
            "Saving model (epoch = 4908, tr_loss = 0.1702)\n",
            "Saving model (epoch = 4909, tr_loss = 0.1701)\n",
            "Saving model (epoch = 4910, tr_loss = 0.1701)\n",
            "Saving model (epoch = 4911, tr_loss = 0.1700)\n",
            "Saving model (epoch = 4912, tr_loss = 0.1700)\n",
            "Saving model (epoch = 4913, tr_loss = 0.1698)\n",
            "Saving model (epoch = 4914, tr_loss = 0.1699)\n",
            "Saving model (epoch = 4915, tr_loss = 0.1699)\n",
            "Saving model (epoch = 4916, tr_loss = 0.1699)\n",
            "Saving model (epoch = 4917, tr_loss = 0.1697)\n",
            "Saving model (epoch = 4918, tr_loss = 0.1697)\n",
            "Saving model (epoch = 4919, tr_loss = 0.1699)\n",
            "Saving model (epoch = 4920, tr_loss = 0.1696)\n",
            "Saving model (epoch = 4921, tr_loss = 0.1696)\n",
            "Saving model (epoch = 4922, tr_loss = 0.1698)\n",
            "Saving model (epoch = 4923, tr_loss = 0.1698)\n",
            "Saving model (epoch = 4924, tr_loss = 0.1695)\n",
            "Saving model (epoch = 4925, tr_loss = 0.1696)\n",
            "Saving model (epoch = 4926, tr_loss = 0.1696)\n",
            "Saving model (epoch = 4927, tr_loss = 0.1693)\n",
            "Saving model (epoch = 4928, tr_loss = 0.1692)\n",
            "Saving model (epoch = 4929, tr_loss = 0.1692)\n",
            "Saving model (epoch = 4930, tr_loss = 0.1692)\n",
            "Saving model (epoch = 4931, tr_loss = 0.1692)\n",
            "Saving model (epoch = 4932, tr_loss = 0.1693)\n",
            "Saving model (epoch = 4933, tr_loss = 0.1691)\n",
            "Saving model (epoch = 4934, tr_loss = 0.1690)\n",
            "Saving model (epoch = 4935, tr_loss = 0.1691)\n",
            "Saving model (epoch = 4936, tr_loss = 0.1690)\n",
            "Saving model (epoch = 4937, tr_loss = 0.1691)\n",
            "Saving model (epoch = 4938, tr_loss = 0.1690)\n",
            "Saving model (epoch = 4939, tr_loss = 0.1689)\n",
            "Saving model (epoch = 4940, tr_loss = 0.1690)\n",
            "Saving model (epoch = 4941, tr_loss = 0.1688)\n",
            "Saving model (epoch = 4942, tr_loss = 0.1689)\n",
            "Saving model (epoch = 4943, tr_loss = 0.1688)\n",
            "Saving model (epoch = 4944, tr_loss = 0.1687)\n",
            "Saving model (epoch = 4945, tr_loss = 0.1686)\n",
            "Saving model (epoch = 4946, tr_loss = 0.1685)\n",
            "Saving model (epoch = 4947, tr_loss = 0.1687)\n",
            "Saving model (epoch = 4948, tr_loss = 0.1685)\n",
            "Saving model (epoch = 4949, tr_loss = 0.1686)\n",
            "Saving model (epoch = 4950, tr_loss = 0.1685)\n",
            "Saving model (epoch = 4951, tr_loss = 0.1684)\n",
            "Saving model (epoch = 4952, tr_loss = 0.1685)\n",
            "Saving model (epoch = 4953, tr_loss = 0.1686)\n",
            "Saving model (epoch = 4954, tr_loss = 0.1685)\n",
            "Saving model (epoch = 4955, tr_loss = 0.1686)\n",
            "Saving model (epoch = 4956, tr_loss = 0.1683)\n",
            "Saving model (epoch = 4957, tr_loss = 0.1683)\n",
            "Saving model (epoch = 4958, tr_loss = 0.1682)\n",
            "Saving model (epoch = 4959, tr_loss = 0.1680)\n",
            "Saving model (epoch = 4960, tr_loss = 0.1681)\n",
            "Saving model (epoch = 4961, tr_loss = 0.1681)\n",
            "Saving model (epoch = 4962, tr_loss = 0.1682)\n",
            "Saving model (epoch = 4963, tr_loss = 0.1679)\n",
            "Saving model (epoch = 4964, tr_loss = 0.1680)\n",
            "Saving model (epoch = 4965, tr_loss = 0.1680)\n",
            "Saving model (epoch = 4966, tr_loss = 0.1680)\n",
            "Saving model (epoch = 4967, tr_loss = 0.1678)\n",
            "Saving model (epoch = 4968, tr_loss = 0.1678)\n",
            "Saving model (epoch = 4969, tr_loss = 0.1680)\n",
            "Saving model (epoch = 4970, tr_loss = 0.1680)\n",
            "Saving model (epoch = 4971, tr_loss = 0.1678)\n",
            "Saving model (epoch = 4972, tr_loss = 0.1677)\n",
            "Saving model (epoch = 4973, tr_loss = 0.1676)\n",
            "Saving model (epoch = 4974, tr_loss = 0.1676)\n",
            "Saving model (epoch = 4975, tr_loss = 0.1675)\n",
            "Saving model (epoch = 4976, tr_loss = 0.1677)\n",
            "Saving model (epoch = 4977, tr_loss = 0.1676)\n",
            "Saving model (epoch = 4978, tr_loss = 0.1674)\n",
            "Saving model (epoch = 4979, tr_loss = 0.1674)\n",
            "Saving model (epoch = 4980, tr_loss = 0.1676)\n",
            "Saving model (epoch = 4981, tr_loss = 0.1674)\n",
            "Saving model (epoch = 4982, tr_loss = 0.1671)\n",
            "Saving model (epoch = 4983, tr_loss = 0.1673)\n",
            "Saving model (epoch = 4984, tr_loss = 0.1674)\n",
            "Saving model (epoch = 4985, tr_loss = 0.1674)\n",
            "Saving model (epoch = 4986, tr_loss = 0.1674)\n",
            "Saving model (epoch = 4987, tr_loss = 0.1672)\n",
            "Saving model (epoch = 4988, tr_loss = 0.1673)\n",
            "Saving model (epoch = 4989, tr_loss = 0.1671)\n",
            "Saving model (epoch = 4990, tr_loss = 0.1669)\n",
            "Saving model (epoch = 4991, tr_loss = 0.1669)\n",
            "Saving model (epoch = 4992, tr_loss = 0.1669)\n",
            "Saving model (epoch = 4993, tr_loss = 0.1668)\n",
            "Saving model (epoch = 4994, tr_loss = 0.1670)\n",
            "Saving model (epoch = 4995, tr_loss = 0.1667)\n",
            "Saving model (epoch = 4996, tr_loss = 0.1668)\n",
            "Saving model (epoch = 4997, tr_loss = 0.1667)\n",
            "Saving model (epoch = 4998, tr_loss = 0.1667)\n",
            "Saving model (epoch = 4999, tr_loss = 0.1666)\n",
            "Saving model (epoch = 5000, tr_loss = 0.1665)\n",
            "Finished training after 5000 epochs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[969.4332275390625,\n",
              " 953.3667602539062,\n",
              " 963.5753173828125,\n",
              " 957.6304931640625,\n",
              " 961.166015625,\n",
              " 961.8948974609375,\n",
              " 957.173828125,\n",
              " 956.8772583007812,\n",
              " 958.1430053710938,\n",
              " 967.6138305664062,\n",
              " 962.451416015625,\n",
              " 956.3955688476562,\n",
              " 960.4403686523438,\n",
              " 949.3460083007812,\n",
              " 953.3322143554688,\n",
              " 952.7872314453125,\n",
              " 956.4413452148438,\n",
              " 950.2208251953125,\n",
              " 953.6275024414062,\n",
              " 953.0460205078125,\n",
              " 950.1307983398438,\n",
              " 952.0824584960938,\n",
              " 954.7013549804688,\n",
              " 959.9443969726562,\n",
              " 953.4217529296875,\n",
              " 951.9844360351562,\n",
              " 951.0736694335938,\n",
              " 948.1134033203125,\n",
              " 946.9901123046875,\n",
              " 946.7302856445312,\n",
              " 934.4735717773438,\n",
              " 942.8558959960938,\n",
              " 950.8539428710938,\n",
              " 944.35693359375,\n",
              " 932.0897827148438,\n",
              " 952.714599609375,\n",
              " 938.7844848632812,\n",
              " 942.9588623046875,\n",
              " 942.7852783203125,\n",
              " 942.071044921875,\n",
              " 940.1351318359375,\n",
              " 942.3674926757812,\n",
              " 935.0709838867188,\n",
              " 949.5866088867188,\n",
              " 938.0651245117188,\n",
              " 933.8348999023438,\n",
              " 943.1187744140625,\n",
              " 936.37451171875,\n",
              " 939.5879516601562,\n",
              " 933.7474365234375,\n",
              " 938.2386474609375,\n",
              " 933.3108520507812,\n",
              " 935.74853515625,\n",
              " 935.5281372070312,\n",
              " 929.6469116210938,\n",
              " 934.51220703125,\n",
              " 924.952880859375,\n",
              " 931.8275756835938,\n",
              " 929.6722412109375,\n",
              " 932.9833984375,\n",
              " 930.6380615234375,\n",
              " 919.0153198242188,\n",
              " 922.0393676757812,\n",
              " 928.9722900390625,\n",
              " 923.0219116210938,\n",
              " 929.2571411132812,\n",
              " 919.72998046875,\n",
              " 927.7020874023438,\n",
              " 928.0911254882812,\n",
              " 931.2642822265625,\n",
              " 935.4036254882812,\n",
              " 926.8532104492188,\n",
              " 926.8645629882812,\n",
              " 923.23046875,\n",
              " 921.4735107421875,\n",
              " 919.9524536132812,\n",
              " 917.38232421875,\n",
              " 922.81201171875,\n",
              " 920.734375,\n",
              " 919.3377075195312,\n",
              " 916.7987060546875,\n",
              " 912.7498168945312,\n",
              " 913.0027465820312,\n",
              " 919.289306640625,\n",
              " 922.74365234375,\n",
              " 912.1268920898438,\n",
              " 905.9509887695312,\n",
              " 912.3802490234375,\n",
              " 925.1102294921875,\n",
              " 922.1531372070312,\n",
              " 911.623291015625,\n",
              " 911.3622436523438,\n",
              " 912.9773559570312,\n",
              " 910.3599853515625,\n",
              " 920.03759765625,\n",
              " 909.849853515625,\n",
              " 907.4556274414062,\n",
              " 901.798828125,\n",
              " 900.5985717773438,\n",
              " 916.1940307617188,\n",
              " 907.8758544921875,\n",
              " 916.1779174804688,\n",
              " 902.5942993164062,\n",
              " 910.12158203125,\n",
              " 907.2599487304688,\n",
              " 899.0521240234375,\n",
              " 911.22021484375,\n",
              " 894.6810302734375,\n",
              " 900.1322021484375,\n",
              " 902.376708984375,\n",
              " 900.5228271484375,\n",
              " 898.3779907226562,\n",
              " 900.792724609375,\n",
              " 903.5010986328125,\n",
              " 891.5111694335938,\n",
              " 898.3226318359375,\n",
              " 898.7910766601562,\n",
              " 894.3057861328125,\n",
              " 899.6717529296875,\n",
              " 893.67626953125,\n",
              " 892.4595336914062,\n",
              " 894.5687255859375,\n",
              " 901.7894897460938,\n",
              " 890.6022338867188,\n",
              " 887.8154907226562,\n",
              " 893.5546875,\n",
              " 892.4573364257812,\n",
              " 893.6116943359375,\n",
              " 892.5509033203125,\n",
              " 891.914794921875,\n",
              " 889.3749389648438,\n",
              " 885.87353515625,\n",
              " 887.0473022460938,\n",
              " 887.5326538085938,\n",
              " 891.836669921875,\n",
              " 886.3043212890625,\n",
              " 884.4480590820312,\n",
              " 879.7911987304688,\n",
              " 886.0265502929688,\n",
              " 883.2007446289062,\n",
              " 888.1198120117188,\n",
              " 879.7031860351562,\n",
              " 874.6835327148438,\n",
              " 884.9448852539062,\n",
              " 879.515869140625,\n",
              " 876.1094970703125,\n",
              " 881.1412353515625,\n",
              " 878.7012329101562,\n",
              " 883.8375854492188,\n",
              " 879.2421875,\n",
              " 878.2130737304688,\n",
              " 875.7510375976562,\n",
              " 876.3289794921875,\n",
              " 872.6407470703125,\n",
              " 873.5742797851562,\n",
              " 872.3260498046875,\n",
              " 870.6957397460938,\n",
              " 879.2804565429688,\n",
              " 870.6820068359375,\n",
              " 873.9993286132812,\n",
              " 868.0415649414062,\n",
              " 865.8834228515625,\n",
              " 867.756103515625,\n",
              " 865.1655883789062,\n",
              " 862.9043579101562,\n",
              " 868.9301147460938,\n",
              " 875.7813720703125,\n",
              " 860.5569458007812,\n",
              " 862.4149169921875,\n",
              " 862.6964111328125,\n",
              " 872.6755981445312,\n",
              " 865.1078491210938,\n",
              " 865.9071655273438,\n",
              " 864.468017578125,\n",
              " 856.4427490234375,\n",
              " 864.8855590820312,\n",
              " 865.1939697265625,\n",
              " 867.7223510742188,\n",
              " 864.1425170898438,\n",
              " 855.693603515625,\n",
              " 855.402099609375,\n",
              " 858.1136474609375,\n",
              " 867.8331909179688,\n",
              " 856.4086303710938,\n",
              " 850.9360961914062,\n",
              " 852.3795166015625,\n",
              " 848.2879638671875,\n",
              " 851.222900390625,\n",
              " 851.0307006835938,\n",
              " 853.6550903320312,\n",
              " 855.3283081054688,\n",
              " 849.2059936523438,\n",
              " 851.832275390625,\n",
              " 850.7078247070312,\n",
              " 847.747314453125,\n",
              " 851.0336303710938,\n",
              " 849.9952392578125,\n",
              " 855.0490112304688,\n",
              " 848.5097045898438,\n",
              " 845.8286743164062,\n",
              " 846.1142578125,\n",
              " 847.4105834960938,\n",
              " 851.728271484375,\n",
              " 852.4843139648438,\n",
              " 845.4817504882812,\n",
              " 842.2017822265625,\n",
              " 849.0796508789062,\n",
              " 833.9169311523438,\n",
              " 839.5454711914062,\n",
              " 842.4140014648438,\n",
              " 835.3206176757812,\n",
              " 832.9500732421875,\n",
              " 836.74072265625,\n",
              " 838.82080078125,\n",
              " 832.8976440429688,\n",
              " 832.9098510742188,\n",
              " 828.67626953125,\n",
              " 837.986083984375,\n",
              " 834.0289306640625,\n",
              " 834.5448608398438,\n",
              " 830.3242797851562,\n",
              " 833.9205932617188,\n",
              " 839.6557006835938,\n",
              " 830.2767333984375,\n",
              " 831.8394165039062,\n",
              " 834.80419921875,\n",
              " 831.6868896484375,\n",
              " 824.1444091796875,\n",
              " 832.27685546875,\n",
              " 824.2073974609375,\n",
              " 815.8212890625,\n",
              " 827.8820190429688,\n",
              " 826.6206665039062,\n",
              " 818.5617065429688,\n",
              " 823.1671142578125,\n",
              " 827.5911865234375,\n",
              " 819.562744140625,\n",
              " 820.6099853515625,\n",
              " 821.6348266601562,\n",
              " 820.7177124023438,\n",
              " 825.4248657226562,\n",
              " 813.6097412109375,\n",
              " 817.297119140625,\n",
              " 819.0736083984375,\n",
              " 828.5467529296875,\n",
              " 810.1158447265625,\n",
              " 821.8995361328125,\n",
              " 816.88720703125,\n",
              " 814.6635131835938,\n",
              " 808.45703125,\n",
              " 819.104248046875,\n",
              " 809.767822265625,\n",
              " 812.9717407226562,\n",
              " 811.9171142578125,\n",
              " 809.650634765625,\n",
              " 803.7239379882812,\n",
              " 805.0420532226562,\n",
              " 807.9817504882812,\n",
              " 811.82958984375,\n",
              " 804.1557006835938,\n",
              " 809.2905883789062,\n",
              " 803.3815307617188,\n",
              " 802.4033203125,\n",
              " 793.6353149414062,\n",
              " 808.76953125,\n",
              " 807.1302490234375,\n",
              " 800.0836181640625,\n",
              " 807.13916015625,\n",
              " 800.4051513671875,\n",
              " 801.6463623046875,\n",
              " 801.1610107421875,\n",
              " 797.634765625,\n",
              " 796.7221069335938,\n",
              " 793.6449584960938,\n",
              " 802.6898193359375,\n",
              " 800.4564819335938,\n",
              " 799.2007446289062,\n",
              " 790.469482421875,\n",
              " 799.6005249023438,\n",
              " 789.5733032226562,\n",
              " 785.5829467773438,\n",
              " 793.5397338867188,\n",
              " 797.8394165039062,\n",
              " 791.9435424804688,\n",
              " 790.5082397460938,\n",
              " 797.94189453125,\n",
              " 788.1090698242188,\n",
              " 784.6219482421875,\n",
              " 783.5306396484375,\n",
              " 788.9525146484375,\n",
              " 792.4052734375,\n",
              " 786.5536499023438,\n",
              " 789.3717651367188,\n",
              " 774.7970581054688,\n",
              " 783.9688720703125,\n",
              " 781.3184814453125,\n",
              " 783.4631958007812,\n",
              " 778.720703125,\n",
              " 781.880859375,\n",
              " 780.1646728515625,\n",
              " 779.3322143554688,\n",
              " 771.5829467773438,\n",
              " 783.7069091796875,\n",
              " 771.3109741210938,\n",
              " 771.9110107421875,\n",
              " 775.8373413085938,\n",
              " 775.9108276367188,\n",
              " 774.0194702148438,\n",
              " 773.880615234375,\n",
              " 770.5111694335938,\n",
              " 772.7474365234375,\n",
              " 778.67236328125,\n",
              " 771.1017456054688,\n",
              " 773.0808715820312,\n",
              " 770.1781616210938,\n",
              " 771.1760864257812,\n",
              " 768.4931030273438,\n",
              " 774.1133422851562,\n",
              " 771.0301513671875,\n",
              " 763.5858764648438,\n",
              " 772.3775024414062,\n",
              " 760.468994140625,\n",
              " 766.5198974609375,\n",
              " 768.2926635742188,\n",
              " 765.4349975585938,\n",
              " 767.0676879882812,\n",
              " 765.0393676757812,\n",
              " 765.4382934570312,\n",
              " 763.6328735351562,\n",
              " 757.1629638671875,\n",
              " 758.9612426757812,\n",
              " 749.3276977539062,\n",
              " 749.5638427734375,\n",
              " 759.019775390625,\n",
              " 753.793701171875,\n",
              " 749.5135498046875,\n",
              " 752.4657592773438,\n",
              " 756.5211791992188,\n",
              " 759.039794921875,\n",
              " 747.8309326171875,\n",
              " 755.4337768554688,\n",
              " 741.2825317382812,\n",
              " 755.9014282226562,\n",
              " 750.000244140625,\n",
              " 744.0521850585938,\n",
              " 750.7080078125,\n",
              " 751.5831909179688,\n",
              " 751.5608520507812,\n",
              " 749.7844848632812,\n",
              " 745.249755859375,\n",
              " 747.251953125,\n",
              " 741.2719116210938,\n",
              " 748.3802490234375,\n",
              " 739.8628540039062,\n",
              " 741.6307373046875,\n",
              " 745.4325561523438,\n",
              " 734.7861938476562,\n",
              " 736.9957885742188,\n",
              " 735.053955078125,\n",
              " 743.8645629882812,\n",
              " 742.1177978515625,\n",
              " 739.8198852539062,\n",
              " 733.2522583007812,\n",
              " 734.890625,\n",
              " 736.1134643554688,\n",
              " 730.39453125,\n",
              " 734.5848999023438,\n",
              " 730.80615234375,\n",
              " 729.5845336914062,\n",
              " 733.9852905273438,\n",
              " 724.1563720703125,\n",
              " 736.90625,\n",
              " 731.5144653320312,\n",
              " 735.0743408203125,\n",
              " 729.0819091796875,\n",
              " 726.9144287109375,\n",
              " 726.6951293945312,\n",
              " 724.2279052734375,\n",
              " 727.2220458984375,\n",
              " 723.7918701171875,\n",
              " 723.0426025390625,\n",
              " 717.2824096679688,\n",
              " 722.75146484375,\n",
              " 724.2765502929688,\n",
              " 722.467529296875,\n",
              " 720.4723510742188,\n",
              " 728.6610717773438,\n",
              " 721.5695190429688,\n",
              " 720.0816040039062,\n",
              " 718.313232421875,\n",
              " 715.3057861328125,\n",
              " 709.5203247070312,\n",
              " 708.468994140625,\n",
              " 715.02001953125,\n",
              " 712.921142578125,\n",
              " 717.0511474609375,\n",
              " 718.0411987304688,\n",
              " 712.57958984375,\n",
              " 711.3326416015625,\n",
              " 711.5281372070312,\n",
              " 705.1083374023438,\n",
              " 713.0059204101562,\n",
              " 704.9795532226562,\n",
              " 706.6083374023438,\n",
              " 709.0836791992188,\n",
              " 696.9570922851562,\n",
              " 703.3181762695312,\n",
              " 708.7254638671875,\n",
              " 706.9581298828125,\n",
              " 707.6166381835938,\n",
              " 708.7913818359375,\n",
              " 706.9942016601562,\n",
              " 692.8594970703125,\n",
              " 704.904541015625,\n",
              " 696.0286254882812,\n",
              " 703.9910278320312,\n",
              " 696.9298095703125,\n",
              " 700.7516479492188,\n",
              " 700.6013793945312,\n",
              " 693.947021484375,\n",
              " 693.7047729492188,\n",
              " 702.3868408203125,\n",
              " 687.0383911132812,\n",
              " 697.4662475585938,\n",
              " 689.02685546875,\n",
              " 691.3907470703125,\n",
              " 686.0595703125,\n",
              " 690.1268920898438,\n",
              " 687.5718383789062,\n",
              " 693.9491577148438,\n",
              " 692.057861328125,\n",
              " 687.4326782226562,\n",
              " 684.1407470703125,\n",
              " 686.9005126953125,\n",
              " 685.94580078125,\n",
              " 684.7172241210938,\n",
              " 687.141845703125,\n",
              " 691.841064453125,\n",
              " 675.0421142578125,\n",
              " 691.0769653320312,\n",
              " 672.0691528320312,\n",
              " 678.516845703125,\n",
              " 679.94140625,\n",
              " 680.5780639648438,\n",
              " 678.6394653320312,\n",
              " 677.6966552734375,\n",
              " 679.4065551757812,\n",
              " 681.1204833984375,\n",
              " 676.8596801757812,\n",
              " 673.0618896484375,\n",
              " 667.5660400390625,\n",
              " 674.9925537109375,\n",
              " 675.1135864257812,\n",
              " 676.6959838867188,\n",
              " 675.198486328125,\n",
              " 671.0101928710938,\n",
              " 672.7667236328125,\n",
              " 671.0491943359375,\n",
              " 670.5155639648438,\n",
              " 667.0396118164062,\n",
              " 668.3941040039062,\n",
              " 670.095703125,\n",
              " 663.5078125,\n",
              " 669.6370239257812,\n",
              " 668.187744140625,\n",
              " 660.8425903320312,\n",
              " 657.7374877929688,\n",
              " 650.84130859375,\n",
              " 660.5335083007812,\n",
              " 660.873779296875,\n",
              " 660.2933959960938,\n",
              " 658.8529663085938,\n",
              " 661.0281372070312,\n",
              " 658.9993286132812,\n",
              " 653.0645751953125,\n",
              " 660.5283813476562,\n",
              " 663.2559814453125,\n",
              " 658.7191162109375,\n",
              " 651.6227416992188,\n",
              " 657.2706909179688,\n",
              " 653.924072265625,\n",
              " 647.3998413085938,\n",
              " 650.1633911132812,\n",
              " 646.6427612304688,\n",
              " 649.5288696289062,\n",
              " 646.5391235351562,\n",
              " 657.5865478515625,\n",
              " 651.0763549804688,\n",
              " 650.3659057617188,\n",
              " 649.3339233398438,\n",
              " 643.5165405273438,\n",
              " 642.8689575195312,\n",
              " 635.9780883789062,\n",
              " 645.6648559570312,\n",
              " 644.8670654296875,\n",
              " 643.54150390625,\n",
              " 643.3719482421875,\n",
              " 644.04541015625,\n",
              " 635.4016723632812,\n",
              " 644.7442016601562,\n",
              " 638.4645385742188,\n",
              " 636.009765625,\n",
              " 635.8425903320312,\n",
              " 636.486083984375,\n",
              " 640.63671875,\n",
              " 633.289306640625,\n",
              " 630.6430053710938,\n",
              " 627.7172241210938,\n",
              " 633.7745971679688,\n",
              " 631.26220703125,\n",
              " 636.6826782226562,\n",
              " 624.597412109375,\n",
              " 630.611572265625,\n",
              " 633.4601440429688,\n",
              " 633.0391845703125,\n",
              " 630.368408203125,\n",
              " 622.3067016601562,\n",
              " 632.9541625976562,\n",
              " 626.4465942382812,\n",
              " 625.0210571289062,\n",
              " 623.3714599609375,\n",
              " 626.0400390625,\n",
              " 620.0806274414062,\n",
              " 622.6477661132812,\n",
              " 624.222900390625,\n",
              " 624.0090942382812,\n",
              " 627.515625,\n",
              " 619.275146484375,\n",
              " 618.9717407226562,\n",
              " 617.5701293945312,\n",
              " 618.8511352539062,\n",
              " 614.13623046875,\n",
              " 614.7219848632812,\n",
              " 607.5709228515625,\n",
              " 615.0272827148438,\n",
              " 611.998046875,\n",
              " 610.2515258789062,\n",
              " 614.7767944335938,\n",
              " 613.0787963867188,\n",
              " 609.8507690429688,\n",
              " 614.7041015625,\n",
              " 602.0706176757812,\n",
              " 604.0595703125,\n",
              " 609.1380615234375,\n",
              " 603.1602783203125,\n",
              " 603.4133911132812,\n",
              " 604.5062255859375,\n",
              " 607.1553955078125,\n",
              " 606.3543090820312,\n",
              " 604.9384765625,\n",
              " 601.2015991210938,\n",
              " 605.9630737304688,\n",
              " 608.5343017578125,\n",
              " 607.7728881835938,\n",
              " 599.7623901367188,\n",
              " 602.7538452148438,\n",
              " 593.418212890625,\n",
              " 596.5667724609375,\n",
              " 599.83056640625,\n",
              " 594.7656860351562,\n",
              " 600.9268188476562,\n",
              " 599.5535888671875,\n",
              " 595.8268432617188,\n",
              " 593.5850830078125,\n",
              " 590.1013793945312,\n",
              " 598.4054565429688,\n",
              " 591.7018432617188,\n",
              " 591.2676391601562,\n",
              " 588.0733642578125,\n",
              " 589.7462158203125,\n",
              " 581.0584106445312,\n",
              " 587.1004028320312,\n",
              " 587.2568969726562,\n",
              " 590.2921752929688,\n",
              " 583.6316528320312,\n",
              " 585.2842407226562,\n",
              " 583.7817993164062,\n",
              " 584.65966796875,\n",
              " 585.0686645507812,\n",
              " 581.345947265625,\n",
              " 580.8878784179688,\n",
              " 586.1522827148438,\n",
              " 577.2476196289062,\n",
              " 579.2180786132812,\n",
              " 581.2492065429688,\n",
              " 583.6663818359375,\n",
              " 578.8790283203125,\n",
              " 576.5797119140625,\n",
              " 569.858642578125,\n",
              " 577.9334106445312,\n",
              " 573.9595336914062,\n",
              " 578.1373901367188,\n",
              " 572.7359008789062,\n",
              " 575.5296020507812,\n",
              " 574.5224609375,\n",
              " 566.1810302734375,\n",
              " 571.3075561523438,\n",
              " 566.51220703125,\n",
              " 567.3112182617188,\n",
              " 571.5440673828125,\n",
              " 569.6722412109375,\n",
              " 568.5177001953125,\n",
              " 564.4725952148438,\n",
              " 566.4452514648438,\n",
              " 565.630615234375,\n",
              " 571.2012329101562,\n",
              " 566.001220703125,\n",
              " 564.786376953125,\n",
              " 558.9849853515625,\n",
              " 567.2573852539062,\n",
              " 560.2777709960938,\n",
              " 558.8340454101562,\n",
              " 562.9066162109375,\n",
              " 550.7205810546875,\n",
              " 555.9698486328125,\n",
              " 550.4547119140625,\n",
              " 560.688720703125,\n",
              " 559.0435180664062,\n",
              " 556.2660522460938,\n",
              " 555.07080078125,\n",
              " 545.2060546875,\n",
              " 550.576904296875,\n",
              " 552.3435668945312,\n",
              " 550.45849609375,\n",
              " 554.6578369140625,\n",
              " 553.4266967773438,\n",
              " 544.78466796875,\n",
              " 550.4942626953125,\n",
              " 553.975830078125,\n",
              " 550.166015625,\n",
              " 552.8477783203125,\n",
              " 541.9447631835938,\n",
              " 546.7050170898438,\n",
              " 546.7982177734375,\n",
              " 547.5527954101562,\n",
              " 550.3939208984375,\n",
              " 541.9288330078125,\n",
              " 541.9257202148438,\n",
              " 542.3994140625,\n",
              " 541.7225341796875,\n",
              " 544.7227172851562,\n",
              " 540.7986450195312,\n",
              " 532.4775390625,\n",
              " 532.9744873046875,\n",
              " 537.461181640625,\n",
              " 545.2455444335938,\n",
              " 536.3084106445312,\n",
              " 535.3756103515625,\n",
              " 538.9867553710938,\n",
              " 537.247802734375,\n",
              " 533.2342529296875,\n",
              " 529.3097534179688,\n",
              " 532.2896728515625,\n",
              " 537.4093627929688,\n",
              " 530.9617309570312,\n",
              " 529.0057373046875,\n",
              " 531.9873046875,\n",
              " 529.0077514648438,\n",
              " 529.0408325195312,\n",
              " 528.3182983398438,\n",
              " 529.4498291015625,\n",
              " 523.3633422851562,\n",
              " 529.2530517578125,\n",
              " 515.7037353515625,\n",
              " 525.243896484375,\n",
              " 525.0095825195312,\n",
              " 523.6203002929688,\n",
              " 519.0809326171875,\n",
              " 527.35302734375,\n",
              " 520.9202270507812,\n",
              " 525.4061889648438,\n",
              " 524.8548583984375,\n",
              " 516.1146240234375,\n",
              " 518.3075561523438,\n",
              " 522.6318359375,\n",
              " 518.187255859375,\n",
              " 515.478515625,\n",
              " 516.10205078125,\n",
              " 518.877685546875,\n",
              " 512.2573852539062,\n",
              " 507.4385681152344,\n",
              " 516.843994140625,\n",
              " 516.9229125976562,\n",
              " 510.1545715332031,\n",
              " 510.1416320800781,\n",
              " 502.7077331542969,\n",
              " 506.3727722167969,\n",
              " 514.5921630859375,\n",
              " 508.9517822265625,\n",
              " 509.1579284667969,\n",
              " 511.617919921875,\n",
              " 509.8625183105469,\n",
              " 508.37005615234375,\n",
              " 501.060791015625,\n",
              " 508.63800048828125,\n",
              " 505.3210754394531,\n",
              " 512.9947509765625,\n",
              " 499.8416442871094,\n",
              " 508.5842590332031,\n",
              " 496.6069030761719,\n",
              " 499.13079833984375,\n",
              " 499.4259948730469,\n",
              " 502.3489074707031,\n",
              " 499.3310241699219,\n",
              " 497.6163635253906,\n",
              " 498.6929626464844,\n",
              " 491.4010009765625,\n",
              " 506.8804931640625,\n",
              " 496.4232482910156,\n",
              " 499.31317138671875,\n",
              " 501.527587890625,\n",
              " 496.0445556640625,\n",
              " 492.5547180175781,\n",
              " 488.0417175292969,\n",
              " 485.1934509277344,\n",
              " 491.5229797363281,\n",
              " 488.75946044921875,\n",
              " 495.77935791015625,\n",
              " 494.7544860839844,\n",
              " 484.9579772949219,\n",
              " 483.3198547363281,\n",
              " 485.93011474609375,\n",
              " 486.95428466796875,\n",
              " 488.37713623046875,\n",
              " 486.6546936035156,\n",
              " 483.4459533691406,\n",
              " 489.5709228515625,\n",
              " 484.9231262207031,\n",
              " 479.210693359375,\n",
              " 490.449462890625,\n",
              " 481.5531005859375,\n",
              " 483.2947082519531,\n",
              " 479.4905090332031,\n",
              " 478.749267578125,\n",
              " 484.3530578613281,\n",
              " 476.4151916503906,\n",
              " 479.7045593261719,\n",
              " 480.2245788574219,\n",
              " 480.1153564453125,\n",
              " 474.46868896484375,\n",
              " 481.6325988769531,\n",
              " 468.8330993652344,\n",
              " 475.8069763183594,\n",
              " 472.0586853027344,\n",
              " 477.3956604003906,\n",
              " 474.6061706542969,\n",
              " 474.2062683105469,\n",
              " 472.90301513671875,\n",
              " 473.9452819824219,\n",
              " 471.6382751464844,\n",
              " 459.76904296875,\n",
              " 465.4403381347656,\n",
              " 466.15533447265625,\n",
              " 468.1330261230469,\n",
              " 470.92803955078125,\n",
              " 469.5567932128906,\n",
              " 468.67608642578125,\n",
              " 467.8281555175781,\n",
              " 462.6221008300781,\n",
              " 467.3231506347656,\n",
              " 467.2664794921875,\n",
              " 466.0341491699219,\n",
              " 465.9371643066406,\n",
              " 464.3575134277344,\n",
              " 460.8004150390625,\n",
              " 457.97210693359375,\n",
              " 462.3186340332031,\n",
              " 462.10638427734375,\n",
              " 453.3602294921875,\n",
              " 461.94775390625,\n",
              " 460.5592346191406,\n",
              " 451.39984130859375,\n",
              " 454.4748229980469,\n",
              " 462.6761779785156,\n",
              " 453.0597839355469,\n",
              " 453.39361572265625,\n",
              " 452.60528564453125,\n",
              " 448.8586120605469,\n",
              " 453.27557373046875,\n",
              " 452.62347412109375,\n",
              " 453.395263671875,\n",
              " 456.47625732421875,\n",
              " 447.29345703125,\n",
              " 448.2912902832031,\n",
              " 449.7601318359375,\n",
              " 449.2545166015625,\n",
              " 444.08453369140625,\n",
              " 443.1988220214844,\n",
              " 446.61029052734375,\n",
              " 445.5448303222656,\n",
              " 454.74542236328125,\n",
              " 446.470947265625,\n",
              " 444.8546142578125,\n",
              " 445.41033935546875,\n",
              " 449.5700378417969,\n",
              " 442.0650329589844,\n",
              " 438.8330078125,\n",
              " 438.0684814453125,\n",
              " 440.0564880371094,\n",
              " 442.01763916015625,\n",
              " 436.1307678222656,\n",
              " 439.4366149902344,\n",
              " 438.4935607910156,\n",
              " 441.1374816894531,\n",
              " 439.94610595703125,\n",
              " 435.2417907714844,\n",
              " 438.7590026855469,\n",
              " 439.25048828125,\n",
              " 435.1915283203125,\n",
              " 430.61248779296875,\n",
              " 438.3085021972656,\n",
              " 432.39697265625,\n",
              " 428.4889221191406,\n",
              " 440.0675354003906,\n",
              " 437.293701171875,\n",
              " 427.4990539550781,\n",
              " 431.2305908203125,\n",
              " 422.98602294921875,\n",
              " 422.2693786621094,\n",
              " 436.4427490234375,\n",
              " 425.7810363769531,\n",
              " 432.90655517578125,\n",
              " 428.069091796875,\n",
              " 428.0225524902344,\n",
              " 425.30029296875,\n",
              " 424.4852294921875,\n",
              " 432.49432373046875,\n",
              " 417.8854675292969,\n",
              " 422.22674560546875,\n",
              " 423.7898254394531,\n",
              " 423.34051513671875,\n",
              " 427.10406494140625,\n",
              " 423.7924499511719,\n",
              " 417.1728820800781,\n",
              " 415.988037109375,\n",
              " 420.4872131347656,\n",
              " 415.32989501953125,\n",
              " 420.5880432128906,\n",
              " 417.1189270019531,\n",
              " 418.5984802246094,\n",
              " 422.7733154296875,\n",
              " 412.935791015625,\n",
              " 417.9336853027344,\n",
              " 420.12060546875,\n",
              " 413.03839111328125,\n",
              " 415.3224182128906,\n",
              " 416.23199462890625,\n",
              " 408.11993408203125,\n",
              " 414.34820556640625,\n",
              " 407.3344421386719,\n",
              " 416.2672119140625,\n",
              " 412.58349609375,\n",
              " 414.4072265625,\n",
              " 408.0752868652344,\n",
              " 414.7471618652344,\n",
              " 401.75445556640625,\n",
              " 400.7185974121094,\n",
              " 404.521728515625,\n",
              " 407.1038818359375,\n",
              " 406.28656005859375,\n",
              " 407.0832824707031,\n",
              " 402.2472839355469,\n",
              " 404.9254150390625,\n",
              " 400.9327697753906,\n",
              " 403.0623779296875,\n",
              " 405.5231628417969,\n",
              " 407.444091796875,\n",
              " 404.31536865234375,\n",
              " 397.87359619140625,\n",
              " 397.5677795410156,\n",
              " 403.3662414550781,\n",
              " 398.3202819824219,\n",
              " 395.89923095703125,\n",
              " 404.2918701171875,\n",
              " 394.7037353515625,\n",
              " 400.6020202636719,\n",
              " 394.83612060546875,\n",
              " 398.254638671875,\n",
              " 398.71929931640625,\n",
              " 392.817626953125,\n",
              " 393.8514099121094,\n",
              " 395.3772888183594,\n",
              " 396.26116943359375,\n",
              " 393.3026428222656,\n",
              " 390.864990234375,\n",
              " 388.1787414550781,\n",
              " 391.5503845214844,\n",
              " 393.0287170410156,\n",
              " 395.3742980957031,\n",
              " 392.71893310546875,\n",
              " 390.302490234375,\n",
              " 390.4568176269531,\n",
              " 384.9385681152344,\n",
              " 384.8639221191406,\n",
              " 383.5841369628906,\n",
              " 389.8672790527344,\n",
              " 385.1561279296875,\n",
              " 386.3524475097656,\n",
              " 383.2056579589844,\n",
              " 386.75830078125,\n",
              " 383.7987365722656,\n",
              " 384.6228332519531,\n",
              " 381.0086975097656,\n",
              " 370.9898681640625,\n",
              " 379.8904113769531,\n",
              " 385.6391906738281,\n",
              " 383.1636657714844,\n",
              " 382.7064514160156,\n",
              " 380.7724914550781,\n",
              " 376.2948303222656,\n",
              " 382.7997131347656,\n",
              " 377.92828369140625,\n",
              " 377.5019836425781,\n",
              " 375.10333251953125,\n",
              " 377.7437438964844,\n",
              " 374.34136962890625,\n",
              " 381.37451171875,\n",
              " 378.57147216796875,\n",
              " 372.86224365234375,\n",
              " 375.9962158203125,\n",
              " 373.8627014160156,\n",
              " 370.7259826660156,\n",
              " 370.5444030761719,\n",
              " 372.1942138671875,\n",
              " 366.75372314453125,\n",
              " 371.42413330078125,\n",
              " 366.94427490234375,\n",
              " 371.8412780761719,\n",
              " 368.1227722167969,\n",
              " 371.6778869628906,\n",
              " 373.4699401855469,\n",
              " 366.77264404296875,\n",
              " 371.5080871582031,\n",
              " 366.6229248046875,\n",
              " 370.72906494140625,\n",
              " 363.5219421386719,\n",
              " 360.5442199707031,\n",
              " 369.30413818359375,\n",
              " 367.0314025878906,\n",
              " 361.2733154296875,\n",
              " 362.9078063964844,\n",
              " 367.5346374511719,\n",
              " 362.23590087890625,\n",
              " 360.72906494140625,\n",
              " 361.5759582519531,\n",
              " 360.0155944824219,\n",
              " 357.8518371582031,\n",
              " 361.3877258300781,\n",
              " 358.4468078613281,\n",
              " 359.019287109375,\n",
              " 362.2055358886719,\n",
              " 363.76776123046875,\n",
              " 353.0785217285156,\n",
              " 353.9973449707031,\n",
              " 359.5911865234375,\n",
              " 356.3716735839844,\n",
              " 350.7877197265625,\n",
              " 351.46490478515625,\n",
              " 356.34271240234375,\n",
              " 351.6340637207031,\n",
              " 351.2499084472656,\n",
              " 345.9378662109375,\n",
              " 352.4849548339844,\n",
              " 354.35992431640625,\n",
              " 351.197509765625,\n",
              " 349.1334533691406,\n",
              " 347.7048034667969,\n",
              " 355.61724853515625,\n",
              " 352.8043212890625,\n",
              " 354.8854675292969,\n",
              " 350.14031982421875,\n",
              " 348.3238830566406,\n",
              " 347.95697021484375,\n",
              " 349.49920654296875,\n",
              " 346.3475646972656,\n",
              " 345.9979553222656,\n",
              " 341.43548583984375,\n",
              " 338.04522705078125,\n",
              " 344.15673828125,\n",
              " 343.9718322753906,\n",
              " 340.5856628417969,\n",
              " 343.30389404296875,\n",
              " 339.0673522949219,\n",
              " 342.2384338378906,\n",
              " 344.3404541015625,\n",
              " 336.7064208984375,\n",
              " 341.1476745605469,\n",
              " 341.62054443359375,\n",
              " 338.7964172363281,\n",
              " 339.6097717285156,\n",
              " 334.9303894042969,\n",
              " 337.9395446777344,\n",
              " 341.6734619140625,\n",
              " 333.3995666503906,\n",
              " 339.6589050292969,\n",
              " 338.4335632324219,\n",
              " 335.67437744140625,\n",
              " 338.20458984375,\n",
              " 335.49957275390625,\n",
              " 333.4458923339844,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# AE\n",
        "# ae = AE(dv_set.dataset[0][0].shape[0], 60).to(device)\n",
        "_ = ae_train([tr_set, dv_set], ae, ae_config, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\nFalse\nFalse\nFalse\n"
          ]
        }
      ],
      "source": [
        "for p in model.enc.parameters():\n",
        "    print(p.requires_grad)\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= 5.8673, tt_loss = 6.5381, tr_loss = 1.5411, dv_loss = 1.4664)\n",
            "Saving model (epoch =   54, ae_loss = 5.5150, tt_loss = 6.1389, tr_loss = 1.5114, dv_loss = 1.4057)\n",
            "Saving model (epoch =   62, ae_loss = 4.2836, tt_loss = 4.7756, tr_loss = 1.4184, dv_loss = 1.3889)\n",
            "Saving model (epoch =   64, ae_loss = 4.0502, tt_loss = 4.5051, tr_loss = 1.4078, dv_loss = 1.3765)\n",
            "Saving model (epoch =   65, ae_loss = 3.9345, tt_loss = 4.3763, tr_loss = 1.4075, dv_loss = 1.3425)\n",
            "Saving model (epoch =   66, ae_loss = 3.8147, tt_loss = 4.2549, tr_loss = 1.4096, dv_loss = 1.3045)\n",
            "Saving model (epoch =   67, ae_loss = 3.7083, tt_loss = 4.1494, tr_loss = 1.3982, dv_loss = 1.2752)\n",
            "Saving model (epoch =   74, ae_loss = 3.0840, tt_loss = 3.4728, tr_loss = 1.3787, dv_loss = 1.2630)\n",
            "Saving model (epoch =   75, ae_loss = 2.9888, tt_loss = 3.3321, tr_loss = 1.4032, dv_loss = 1.2484)\n",
            "Saving model (epoch =   76, ae_loss = 2.9141, tt_loss = 3.2719, tr_loss = 1.3644, dv_loss = 1.2343)\n",
            "Saving model (epoch =   82, ae_loss = 2.5141, tt_loss = 2.8265, tr_loss = 1.3397, dv_loss = 1.2196)\n",
            "Saving model (epoch =   83, ae_loss = 2.4454, tt_loss = 2.7496, tr_loss = 1.3468, dv_loss = 1.2195)\n",
            "Saving model (epoch =   85, ae_loss = 2.3372, tt_loss = 2.6336, tr_loss = 1.3507, dv_loss = 1.2165)\n",
            "Saving model (epoch =   87, ae_loss = 2.2185, tt_loss = 2.5130, tr_loss = 1.3377, dv_loss = 1.2116)\n",
            "Saving model (epoch =   91, ae_loss = 2.0315, tt_loss = 2.2912, tr_loss = 1.3733, dv_loss = 1.2074)\n",
            "Saving model (epoch =   92, ae_loss = 1.9839, tt_loss = 2.2544, tr_loss = 1.3441, dv_loss = 1.1996)\n",
            "Saving model (epoch =  100, ae_loss = 1.6860, tt_loss = 1.9166, tr_loss = 1.3207, dv_loss = 1.1968)\n",
            "Saving model (epoch =  101, ae_loss = 1.6460, tt_loss = 1.8830, tr_loss = 1.2954, dv_loss = 1.1951)\n",
            "Saving model (epoch =  103, ae_loss = 1.5895, tt_loss = 1.8139, tr_loss = 1.2909, dv_loss = 1.1938)\n",
            "Saving model (epoch =  104, ae_loss = 1.5606, tt_loss = 1.7725, tr_loss = 1.2972, dv_loss = 1.1841)\n",
            "Saving model (epoch =  110, ae_loss = 1.3885, tt_loss = 1.5935, tr_loss = 1.2800, dv_loss = 1.1783)\n",
            "Saving model (epoch =  111, ae_loss = 1.3692, tt_loss = 1.5722, tr_loss = 1.3080, dv_loss = 1.1727)\n",
            "Saving model (epoch =  119, ae_loss = 1.2052, tt_loss = 1.3796, tr_loss = 1.2801, dv_loss = 1.1671)\n",
            "Saving model (epoch =  125, ae_loss = 1.0966, tt_loss = 1.2589, tr_loss = 1.3190, dv_loss = 1.1602)\n",
            "Saving model (epoch =  126, ae_loss = 1.0839, tt_loss = 1.2497, tr_loss = 1.2576, dv_loss = 1.1560)\n",
            "Saving model (epoch =  130, ae_loss = 1.0235, tt_loss = 1.1749, tr_loss = 1.2615, dv_loss = 1.1504)\n",
            "Saving model (epoch =  138, ae_loss = 0.9210, tt_loss = 1.0562, tr_loss = 1.2695, dv_loss = 1.1446)\n",
            "Saving model (epoch =  144, ae_loss = 0.8506, tt_loss = 0.9785, tr_loss = 1.2346, dv_loss = 1.1368)\n",
            "Saving model (epoch =  145, ae_loss = 0.8406, tt_loss = 0.9666, tr_loss = 1.2325, dv_loss = 1.1366)\n",
            "Saving model (epoch =  150, ae_loss = 0.7958, tt_loss = 0.9105, tr_loss = 1.2256, dv_loss = 1.1343)\n",
            "Saving model (epoch =  152, ae_loss = 0.7824, tt_loss = 0.8967, tr_loss = 1.2739, dv_loss = 1.1282)\n",
            "Saving model (epoch =  155, ae_loss = 0.7593, tt_loss = 0.8669, tr_loss = 1.2182, dv_loss = 1.1273)\n",
            "Saving model (epoch =  159, ae_loss = 0.7307, tt_loss = 0.8310, tr_loss = 1.2332, dv_loss = 1.1258)\n",
            "Saving model (epoch =  164, ae_loss = 0.6969, tt_loss = 0.7924, tr_loss = 1.2424, dv_loss = 1.1190)\n",
            "Saving model (epoch =  171, ae_loss = 0.6594, tt_loss = 0.7438, tr_loss = 1.2120, dv_loss = 1.1151)\n",
            "Saving model (epoch =  181, ae_loss = 0.6075, tt_loss = 0.6850, tr_loss = 1.2208, dv_loss = 1.1082)\n",
            "Saving model (epoch =  192, ae_loss = 0.5617, tt_loss = 0.6331, tr_loss = 1.1938, dv_loss = 1.1035)\n",
            "Saving model (epoch =  195, ae_loss = 0.5527, tt_loss = 0.6220, tr_loss = 1.2144, dv_loss = 1.1003)\n",
            "Saving model (epoch =  196, ae_loss = 0.5477, tt_loss = 0.6158, tr_loss = 1.1805, dv_loss = 1.0994)\n",
            "Saving model (epoch =  200, ae_loss = 0.5330, tt_loss = 0.6019, tr_loss = 1.1769, dv_loss = 1.0976)\n",
            "Saving model (epoch =  203, ae_loss = 0.5249, tt_loss = 0.5912, tr_loss = 1.2021, dv_loss = 1.0958)\n",
            "Saving model (epoch =  205, ae_loss = 0.5179, tt_loss = 0.5834, tr_loss = 1.1813, dv_loss = 1.0944)\n",
            "Saving model (epoch =  208, ae_loss = 0.5072, tt_loss = 0.5719, tr_loss = 1.1813, dv_loss = 1.0877)\n",
            "Saving model (epoch =  211, ae_loss = 0.4993, tt_loss = 0.5619, tr_loss = 1.1728, dv_loss = 1.0853)\n",
            "Saving model (epoch =  220, ae_loss = 0.4750, tt_loss = 0.5319, tr_loss = 1.1983, dv_loss = 1.0830)\n",
            "Saving model (epoch =  226, ae_loss = 0.4574, tt_loss = 0.5156, tr_loss = 1.1599, dv_loss = 1.0740)\n",
            "Saving model (epoch =  229, ae_loss = 0.4483, tt_loss = 0.5044, tr_loss = 1.1526, dv_loss = 1.0719)\n",
            "Saving model (epoch =  232, ae_loss = 0.4402, tt_loss = 0.4945, tr_loss = 1.1746, dv_loss = 1.0670)\n",
            "Saving model (epoch =  233, ae_loss = 0.4357, tt_loss = 0.4911, tr_loss = 1.1537, dv_loss = 1.0641)\n",
            "Saving model (epoch =  235, ae_loss = 0.4315, tt_loss = 0.4869, tr_loss = 1.1660, dv_loss = 1.0614)\n",
            "Saving model (epoch =  242, ae_loss = 0.4128, tt_loss = 0.4687, tr_loss = 1.1625, dv_loss = 1.0600)\n",
            "Saving model (epoch =  245, ae_loss = 0.4052, tt_loss = 0.4630, tr_loss = 1.1454, dv_loss = 1.0580)\n",
            "Saving model (epoch =  249, ae_loss = 0.3975, tt_loss = 0.4516, tr_loss = 1.1518, dv_loss = 1.0531)\n",
            "Saving model (epoch =  250, ae_loss = 0.3943, tt_loss = 0.4517, tr_loss = 1.1325, dv_loss = 1.0512)\n",
            "Saving model (epoch =  256, ae_loss = 0.3804, tt_loss = 0.4370, tr_loss = 1.1305, dv_loss = 1.0470)\n",
            "Saving model (epoch =  257, ae_loss = 0.3787, tt_loss = 0.4344, tr_loss = 1.1328, dv_loss = 1.0457)\n",
            "Saving model (epoch =  258, ae_loss = 0.3773, tt_loss = 0.4318, tr_loss = 1.1268, dv_loss = 1.0441)\n",
            "Saving model (epoch =  265, ae_loss = 0.3624, tt_loss = 0.4145, tr_loss = 1.1386, dv_loss = 1.0423)\n",
            "Saving model (epoch =  266, ae_loss = 0.3608, tt_loss = 0.4123, tr_loss = 1.1294, dv_loss = 1.0401)\n",
            "Saving model (epoch =  268, ae_loss = 0.3584, tt_loss = 0.4077, tr_loss = 1.1321, dv_loss = 1.0339)\n",
            "Saving model (epoch =  276, ae_loss = 0.3434, tt_loss = 0.3933, tr_loss = 1.1341, dv_loss = 1.0293)\n",
            "Saving model (epoch =  281, ae_loss = 0.3363, tt_loss = 0.3872, tr_loss = 1.1147, dv_loss = 1.0241)\n",
            "Saving model (epoch =  286, ae_loss = 0.3305, tt_loss = 0.3792, tr_loss = 1.1112, dv_loss = 1.0220)\n",
            "Saving model (epoch =  287, ae_loss = 0.3279, tt_loss = 0.3788, tr_loss = 1.1070, dv_loss = 1.0213)\n",
            "Saving model (epoch =  289, ae_loss = 0.3262, tt_loss = 0.3748, tr_loss = 1.1110, dv_loss = 1.0188)\n",
            "Saving model (epoch =  298, ae_loss = 0.3200, tt_loss = 0.3627, tr_loss = 1.1695, dv_loss = 1.0157)\n",
            "Saving model (epoch =  300, ae_loss = 0.3156, tt_loss = 0.3598, tr_loss = 1.1102, dv_loss = 1.0126)\n",
            "Saving model (epoch =  302, ae_loss = 0.3129, tt_loss = 0.3589, tr_loss = 1.0986, dv_loss = 1.0092)\n",
            "Saving model (epoch =  304, ae_loss = 0.3173, tt_loss = 0.3562, tr_loss = 1.1552, dv_loss = 1.0074)\n",
            "Saving model (epoch =  315, ae_loss = 0.3023, tt_loss = 0.3451, tr_loss = 1.1127, dv_loss = 1.0010)\n",
            "Saving model (epoch =  320, ae_loss = 0.2989, tt_loss = 0.3412, tr_loss = 1.0907, dv_loss = 0.9983)\n",
            "Saving model (epoch =  322, ae_loss = 0.2951, tt_loss = 0.3371, tr_loss = 1.0801, dv_loss = 0.9972)\n",
            "Saving model (epoch =  327, ae_loss = 0.2913, tt_loss = 0.3333, tr_loss = 1.0884, dv_loss = 0.9925)\n",
            "Saving model (epoch =  331, ae_loss = 0.2871, tt_loss = 0.3293, tr_loss = 1.1095, dv_loss = 0.9891)\n",
            "Saving model (epoch =  335, ae_loss = 0.2820, tt_loss = 0.3270, tr_loss = 1.0725, dv_loss = 0.9874)\n",
            "Saving model (epoch =  338, ae_loss = 0.2814, tt_loss = 0.3261, tr_loss = 1.0820, dv_loss = 0.9850)\n",
            "Saving model (epoch =  344, ae_loss = 0.2772, tt_loss = 0.3189, tr_loss = 1.0791, dv_loss = 0.9800)\n",
            "Saving model (epoch =  355, ae_loss = 0.2688, tt_loss = 0.3109, tr_loss = 1.0776, dv_loss = 0.9731)\n",
            "Saving model (epoch =  358, ae_loss = 0.2683, tt_loss = 0.3095, tr_loss = 1.0760, dv_loss = 0.9694)\n",
            "Saving model (epoch =  363, ae_loss = 0.2646, tt_loss = 0.3058, tr_loss = 1.0612, dv_loss = 0.9668)\n",
            "Saving model (epoch =  368, ae_loss = 0.2649, tt_loss = 0.3030, tr_loss = 1.0691, dv_loss = 0.9634)\n",
            "Saving model (epoch =  377, ae_loss = 0.2604, tt_loss = 0.2982, tr_loss = 1.0809, dv_loss = 0.9611)\n",
            "Saving model (epoch =  387, ae_loss = 0.2547, tt_loss = 0.2930, tr_loss = 1.0718, dv_loss = 0.9586)\n",
            "Saving model (epoch =  388, ae_loss = 0.2524, tt_loss = 0.2930, tr_loss = 1.0391, dv_loss = 0.9572)\n",
            "Saving model (epoch =  389, ae_loss = 0.2518, tt_loss = 0.2923, tr_loss = 1.0399, dv_loss = 0.9505)\n",
            "Saving model (epoch =  397, ae_loss = 0.2527, tt_loss = 0.2897, tr_loss = 1.0877, dv_loss = 0.9445)\n",
            "Saving model (epoch =  409, ae_loss = 0.2481, tt_loss = 0.2831, tr_loss = 1.0716, dv_loss = 0.9414)\n",
            "Saving model (epoch =  413, ae_loss = 0.2446, tt_loss = 0.2845, tr_loss = 1.0284, dv_loss = 0.9382)\n",
            "Saving model (epoch =  414, ae_loss = 0.2466, tt_loss = 0.2837, tr_loss = 1.0314, dv_loss = 0.9343)\n",
            "Saving model (epoch =  423, ae_loss = 0.2415, tt_loss = 0.2789, tr_loss = 1.0193, dv_loss = 0.9302)\n",
            "Saving model (epoch =  429, ae_loss = 0.2392, tt_loss = 0.2764, tr_loss = 1.0214, dv_loss = 0.9296)\n",
            "Saving model (epoch =  432, ae_loss = 0.2397, tt_loss = 0.2751, tr_loss = 1.0271, dv_loss = 0.9271)\n",
            "Saving model (epoch =  434, ae_loss = 0.2385, tt_loss = 0.2773, tr_loss = 1.0413, dv_loss = 0.9240)\n",
            "Saving model (epoch =  442, ae_loss = 0.2403, tt_loss = 0.2717, tr_loss = 1.0106, dv_loss = 0.9220)\n",
            "Saving model (epoch =  447, ae_loss = 0.2359, tt_loss = 0.2711, tr_loss = 1.0166, dv_loss = 0.9195)\n",
            "Saving model (epoch =  452, ae_loss = 0.2341, tt_loss = 0.2702, tr_loss = 1.0251, dv_loss = 0.9160)\n",
            "Saving model (epoch =  454, ae_loss = 0.2400, tt_loss = 0.2698, tr_loss = 1.0499, dv_loss = 0.9160)\n",
            "Saving model (epoch =  458, ae_loss = 0.2336, tt_loss = 0.2669, tr_loss = 1.0088, dv_loss = 0.9156)\n",
            "Saving model (epoch =  461, ae_loss = 0.2329, tt_loss = 0.2669, tr_loss = 1.0423, dv_loss = 0.9153)\n",
            "Saving model (epoch =  465, ae_loss = 0.2313, tt_loss = 0.2640, tr_loss = 0.9997, dv_loss = 0.9145)\n",
            "Saving model (epoch =  468, ae_loss = 0.2345, tt_loss = 0.2644, tr_loss = 1.0219, dv_loss = 0.9079)\n",
            "Saving model (epoch =  473, ae_loss = 0.2315, tt_loss = 0.2629, tr_loss = 1.0148, dv_loss = 0.9058)\n",
            "Saving model (epoch =  477, ae_loss = 0.2291, tt_loss = 0.2624, tr_loss = 1.0013, dv_loss = 0.9032)\n",
            "Saving model (epoch =  482, ae_loss = 0.2341, tt_loss = 0.2610, tr_loss = 1.0502, dv_loss = 0.9028)\n",
            "Saving model (epoch =  495, ae_loss = 0.2258, tt_loss = 0.2564, tr_loss = 1.0042, dv_loss = 0.8978)\n",
            "Saving model (epoch =  496, ae_loss = 0.2257, tt_loss = 0.2605, tr_loss = 1.0050, dv_loss = 0.8937)\n",
            "Saving model (epoch =  505, ae_loss = 0.2256, tt_loss = 0.2545, tr_loss = 1.0216, dv_loss = 0.8920)\n",
            "Saving model (epoch =  510, ae_loss = 0.2242, tt_loss = 0.2550, tr_loss = 0.9913, dv_loss = 0.8897)\n",
            "Saving model (epoch =  523, ae_loss = 0.2229, tt_loss = 0.2529, tr_loss = 0.9986, dv_loss = 0.8865)\n",
            "Saving model (epoch =  532, ae_loss = 0.2209, tt_loss = 0.2505, tr_loss = 0.9778, dv_loss = 0.8812)\n",
            "Saving model (epoch =  536, ae_loss = 0.2234, tt_loss = 0.2496, tr_loss = 1.0125, dv_loss = 0.8799)\n",
            "Saving model (epoch =  542, ae_loss = 0.2184, tt_loss = 0.2471, tr_loss = 0.9750, dv_loss = 0.8785)\n",
            "Saving model (epoch =  552, ae_loss = 0.2214, tt_loss = 0.2470, tr_loss = 0.9877, dv_loss = 0.8775)\n",
            "Saving model (epoch =  554, ae_loss = 0.2180, tt_loss = 0.2473, tr_loss = 0.9765, dv_loss = 0.8765)\n",
            "Saving model (epoch =  559, ae_loss = 0.2175, tt_loss = 0.2453, tr_loss = 0.9760, dv_loss = 0.8743)\n",
            "Saving model (epoch =  561, ae_loss = 0.2169, tt_loss = 0.2505, tr_loss = 0.9791, dv_loss = 0.8737)\n",
            "Saving model (epoch =  566, ae_loss = 0.2171, tt_loss = 0.2452, tr_loss = 0.9779, dv_loss = 0.8701)\n",
            "Saving model (epoch =  573, ae_loss = 0.2157, tt_loss = 0.2451, tr_loss = 0.9802, dv_loss = 0.8664)\n",
            "Saving model (epoch =  582, ae_loss = 0.2248, tt_loss = 0.2465, tr_loss = 1.0715, dv_loss = 0.8647)\n",
            "Saving model (epoch =  585, ae_loss = 0.2180, tt_loss = 0.2414, tr_loss = 1.0089, dv_loss = 0.8636)\n",
            "Saving model (epoch =  589, ae_loss = 0.2204, tt_loss = 0.2418, tr_loss = 1.0249, dv_loss = 0.8633)\n",
            "Saving model (epoch =  596, ae_loss = 0.2132, tt_loss = 0.2409, tr_loss = 0.9623, dv_loss = 0.8615)\n",
            "Saving model (epoch =  602, ae_loss = 0.2126, tt_loss = 0.2395, tr_loss = 0.9612, dv_loss = 0.8614)\n",
            "Saving model (epoch =  617, ae_loss = 0.2108, tt_loss = 0.2378, tr_loss = 0.9497, dv_loss = 0.8583)\n",
            "Saving model (epoch =  624, ae_loss = 0.2111, tt_loss = 0.2373, tr_loss = 0.9686, dv_loss = 0.8582)\n",
            "Saving model (epoch =  634, ae_loss = 0.2120, tt_loss = 0.2357, tr_loss = 0.9823, dv_loss = 0.8561)\n",
            "Saving model (epoch =  635, ae_loss = 0.2102, tt_loss = 0.2371, tr_loss = 0.9559, dv_loss = 0.8558)\n",
            "Saving model (epoch =  646, ae_loss = 0.2108, tt_loss = 0.2389, tr_loss = 0.9552, dv_loss = 0.8498)\n",
            "Saving model (epoch =  655, ae_loss = 0.2082, tt_loss = 0.2334, tr_loss = 0.9405, dv_loss = 0.8457)\n",
            "Saving model (epoch =  673, ae_loss = 0.2082, tt_loss = 0.2347, tr_loss = 0.9710, dv_loss = 0.8437)\n",
            "Saving model (epoch =  691, ae_loss = 0.2075, tt_loss = 0.2294, tr_loss = 0.9769, dv_loss = 0.8432)\n",
            "Saving model (epoch =  692, ae_loss = 0.2051, tt_loss = 0.2349, tr_loss = 0.9480, dv_loss = 0.8420)\n",
            "Saving model (epoch =  704, ae_loss = 0.2063, tt_loss = 0.2289, tr_loss = 0.9618, dv_loss = 0.8395)\n",
            "Saving model (epoch =  710, ae_loss = 0.2052, tt_loss = 0.2293, tr_loss = 0.9547, dv_loss = 0.8373)\n",
            "Saving model (epoch =  715, ae_loss = 0.2047, tt_loss = 0.2299, tr_loss = 0.9482, dv_loss = 0.8366)\n",
            "Saving model (epoch =  719, ae_loss = 0.2044, tt_loss = 0.2283, tr_loss = 0.9361, dv_loss = 0.8362)\n",
            "Saving model (epoch =  720, ae_loss = 0.2055, tt_loss = 0.2293, tr_loss = 0.9462, dv_loss = 0.8343)\n",
            "Saving model (epoch =  729, ae_loss = 0.2066, tt_loss = 0.2266, tr_loss = 1.0059, dv_loss = 0.8326)\n",
            "Saving model (epoch =  745, ae_loss = 0.2036, tt_loss = 0.2294, tr_loss = 0.9395, dv_loss = 0.8307)\n",
            "Saving model (epoch =  750, ae_loss = 0.2039, tt_loss = 0.2304, tr_loss = 0.9481, dv_loss = 0.8298)\n",
            "Saving model (epoch =  776, ae_loss = 0.2050, tt_loss = 0.2300, tr_loss = 0.9400, dv_loss = 0.8292)\n",
            "Saving model (epoch =  779, ae_loss = 0.2087, tt_loss = 0.2244, tr_loss = 0.9798, dv_loss = 0.8288)\n",
            "Saving model (epoch =  785, ae_loss = 0.2033, tt_loss = 0.2243, tr_loss = 0.9319, dv_loss = 0.8283)\n",
            "Saving model (epoch =  794, ae_loss = 0.2012, tt_loss = 0.2224, tr_loss = 0.9277, dv_loss = 0.8280)\n",
            "Saving model (epoch =  799, ae_loss = 0.2006, tt_loss = 0.2267, tr_loss = 0.9198, dv_loss = 0.8259)\n",
            "Saving model (epoch =  807, ae_loss = 0.2002, tt_loss = 0.2216, tr_loss = 0.9387, dv_loss = 0.8248)\n",
            "Saving model (epoch =  822, ae_loss = 0.1990, tt_loss = 0.2218, tr_loss = 0.9234, dv_loss = 0.8244)\n",
            "Saving model (epoch =  828, ae_loss = 0.2002, tt_loss = 0.2224, tr_loss = 0.9386, dv_loss = 0.8208)\n",
            "Saving model (epoch =  851, ae_loss = 0.1996, tt_loss = 0.2196, tr_loss = 0.9258, dv_loss = 0.8203)\n",
            "Saving model (epoch =  867, ae_loss = 0.1984, tt_loss = 0.2223, tr_loss = 0.9175, dv_loss = 0.8193)\n",
            "Saving model (epoch =  870, ae_loss = 0.1985, tt_loss = 0.2195, tr_loss = 0.9313, dv_loss = 0.8165)\n",
            "Saving model (epoch =  905, ae_loss = 0.1964, tt_loss = 0.2193, tr_loss = 0.9141, dv_loss = 0.8155)\n",
            "Saving model (epoch =  919, ae_loss = 0.1957, tt_loss = 0.2190, tr_loss = 0.9096, dv_loss = 0.8154)\n",
            "Saving model (epoch =  935, ae_loss = 0.1964, tt_loss = 0.2197, tr_loss = 0.9320, dv_loss = 0.8143)\n",
            "Saving model (epoch =  944, ae_loss = 0.1953, tt_loss = 0.2170, tr_loss = 0.9071, dv_loss = 0.8130)\n",
            "Saving model (epoch =  960, ae_loss = 0.1962, tt_loss = 0.2204, tr_loss = 0.9615, dv_loss = 0.8123)\n",
            "Saving model (epoch =  964, ae_loss = 0.1964, tt_loss = 0.2172, tr_loss = 0.9425, dv_loss = 0.8089)\n",
            "Saving model (epoch = 1016, ae_loss = 0.2033, tt_loss = 0.2141, tr_loss = 0.8998, dv_loss = 0.8081)\n",
            "Saving model (epoch = 1023, ae_loss = 0.1990, tt_loss = 0.2142, tr_loss = 0.9155, dv_loss = 0.8078)\n",
            "Saving model (epoch = 1033, ae_loss = 0.2004, tt_loss = 0.2130, tr_loss = 0.9161, dv_loss = 0.8073)\n",
            "Saving model (epoch = 1034, ae_loss = 0.1999, tt_loss = 0.2123, tr_loss = 0.9146, dv_loss = 0.8072)\n",
            "Saving model (epoch = 1082, ae_loss = 0.2146, tt_loss = 0.2189, tr_loss = 0.9631, dv_loss = 0.8057)\n",
            "Saving model (epoch = 1089, ae_loss = 0.2066, tt_loss = 0.2166, tr_loss = 0.9162, dv_loss = 0.8054)\n",
            "Saving model (epoch = 1090, ae_loss = 0.2083, tt_loss = 0.2144, tr_loss = 0.9086, dv_loss = 0.8045)\n",
            "Saving model (epoch = 1094, ae_loss = 0.2087, tt_loss = 0.2182, tr_loss = 0.9065, dv_loss = 0.8027)\n",
            "Saving model (epoch = 1121, ae_loss = 0.2156, tt_loss = 0.2310, tr_loss = 0.9315, dv_loss = 0.8015)\n",
            "Saving model (epoch = 1140, ae_loss = 0.2128, tt_loss = 0.2225, tr_loss = 0.8932, dv_loss = 0.7995)\n",
            "Saving model (epoch = 1188, ae_loss = 0.2153, tt_loss = 0.2158, tr_loss = 0.9159, dv_loss = 0.7990)\n",
            "Saving model (epoch = 1219, ae_loss = 0.2141, tt_loss = 0.2152, tr_loss = 0.9128, dv_loss = 0.7986)\n",
            "Saving model (epoch = 1228, ae_loss = 0.2115, tt_loss = 0.2143, tr_loss = 0.8894, dv_loss = 0.7975)\n",
            "Saving model (epoch = 1236, ae_loss = 0.2197, tt_loss = 0.2193, tr_loss = 0.8954, dv_loss = 0.7973)\n",
            "Saving model (epoch = 1256, ae_loss = 0.2166, tt_loss = 0.2182, tr_loss = 0.9228, dv_loss = 0.7954)\n",
            "Saving model (epoch = 1317, ae_loss = 0.2156, tt_loss = 0.2165, tr_loss = 0.8727, dv_loss = 0.7934)\n",
            "Saving model (epoch = 1377, ae_loss = 0.2195, tt_loss = 0.2160, tr_loss = 0.8967, dv_loss = 0.7931)\n",
            "Saving model (epoch = 1390, ae_loss = 0.2175, tt_loss = 0.2167, tr_loss = 0.8883, dv_loss = 0.7921)\n",
            "Saving model (epoch = 1403, ae_loss = 0.2174, tt_loss = 0.2157, tr_loss = 0.8768, dv_loss = 0.7919)\n",
            "Saving model (epoch = 1420, ae_loss = 0.2261, tt_loss = 0.2182, tr_loss = 0.8976, dv_loss = 0.7912)\n",
            "Saving model (epoch = 1458, ae_loss = 0.2226, tt_loss = 0.2153, tr_loss = 0.8828, dv_loss = 0.7907)\n",
            "Saving model (epoch = 1467, ae_loss = 0.2267, tt_loss = 0.2249, tr_loss = 0.9845, dv_loss = 0.7896)\n",
            "Saving model (epoch = 1554, ae_loss = 0.2286, tt_loss = 0.2215, tr_loss = 0.8938, dv_loss = 0.7894)\n",
            "Saving model (epoch = 1555, ae_loss = 0.2292, tt_loss = 0.2200, tr_loss = 0.9018, dv_loss = 0.7893)\n",
            "Saving model (epoch = 1565, ae_loss = 0.2247, tt_loss = 0.2158, tr_loss = 0.8747, dv_loss = 0.7891)\n",
            "Saving model (epoch = 1571, ae_loss = 0.2240, tt_loss = 0.2161, tr_loss = 0.8709, dv_loss = 0.7880)\n",
            "Saving model (epoch = 1619, ae_loss = 0.2272, tt_loss = 0.2167, tr_loss = 0.8770, dv_loss = 0.7866)\n",
            "Saving model (epoch = 1661, ae_loss = 0.2249, tt_loss = 0.2201, tr_loss = 0.8686, dv_loss = 0.7864)\n",
            "Saving model (epoch = 1697, ae_loss = 0.2270, tt_loss = 0.2179, tr_loss = 0.8908, dv_loss = 0.7863)\n",
            "Saving model (epoch = 1764, ae_loss = 0.2308, tt_loss = 0.2197, tr_loss = 0.8714, dv_loss = 0.7863)\n",
            "Saving model (epoch = 1788, ae_loss = 0.2299, tt_loss = 0.2190, tr_loss = 0.8947, dv_loss = 0.7860)\n",
            "Saving model (epoch = 1795, ae_loss = 0.2301, tt_loss = 0.2191, tr_loss = 0.8716, dv_loss = 0.7840)\n",
            "Saving model (epoch = 1909, ae_loss = 0.2527, tt_loss = 0.2467, tr_loss = 1.0524, dv_loss = 0.7840)\n",
            "Saving model (epoch = 2054, ae_loss = 0.2397, tt_loss = 0.2247, tr_loss = 0.8819, dv_loss = 0.7839)\n",
            "Saving model (epoch = 2057, ae_loss = 0.2383, tt_loss = 0.2215, tr_loss = 0.9353, dv_loss = 0.7837)\n",
            "Saving model (epoch = 2062, ae_loss = 0.2341, tt_loss = 0.2235, tr_loss = 0.8620, dv_loss = 0.7833)\n",
            "Saving model (epoch = 2145, ae_loss = 0.2387, tt_loss = 0.2238, tr_loss = 0.8630, dv_loss = 0.7818)\n",
            "Saving model (epoch = 2215, ae_loss = 0.2335, tt_loss = 0.2203, tr_loss = 0.8597, dv_loss = 0.7804)\n",
            "Saving model (epoch = 2401, ae_loss = 0.2450, tt_loss = 0.2252, tr_loss = 0.8557, dv_loss = 0.7791)\n",
            "Saving model (epoch = 2595, ae_loss = 0.2722, tt_loss = 0.2315, tr_loss = 0.8676, dv_loss = 0.7783)\n",
            "Saving model (epoch = 2677, ae_loss = 0.2476, tt_loss = 0.2337, tr_loss = 0.8643, dv_loss = 0.7768)\n",
            "Saving model (epoch = 2770, ae_loss = 0.2650, tt_loss = 0.2315, tr_loss = 0.8539, dv_loss = 0.7764)\n",
            "Saving model (epoch = 2932, ae_loss = 0.2636, tt_loss = 0.2376, tr_loss = 0.8465, dv_loss = 0.7764)\n",
            "Finished training after 3233 epochs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7277908325,\n",
              "   1.2134183049201965,\n",
              "   1.1565412878990173,\n",
              "   1.1591220498085022,\n",
              "   1.1503956317901611,\n",
              "   1.1680160164833069,\n",
              "   1.1683529615402222,\n",
              "   1.219775915145874,\n",
              "   1.1580893993377686,\n",
              "   1.1654012799263,\n",
              "   1.161908209323883,\n",
              "   1.1565817594528198,\n",
              "   1.1445513367652893,\n",
              "   1.1584385931491852,\n",
              "   1.201051950454712,\n",
              "   1.152254194021225,\n",
              "   1.1704266369342804,\n",
              "   1.2087026834487915,\n",
              "   1.1367771625518799,\n",
              "   1.1365850567817688,\n",
              "   1.1508129835128784,\n",
              "   1.1418454051017761,\n",
              "   1.1563995480537415,\n",
              "   1.1423433423042297,\n",
              "   1.134347140789032,\n",
              "   1.1365264058113098,\n",
              "   1.128211498260498,\n",
              "   1.2627192735671997,\n",
              "   1.1339498162269592,\n",
              "   1.1272773742675781,\n",
              "   1.2552493810653687,\n",
              "   1.1355313658714294,\n",
              "   1.249148428440094,\n",
              "   1.1257686018943787,\n",
              "   1.1567411422729492,\n",
              "   1.2284507751464844,\n",
              "   1.126044750213623,\n",
              "   1.180282175540924,\n",
              "   1.118960440158844,\n",
              "   1.1400628685951233,\n",
              "   1.1551653146743774,\n",
              "   1.1629112362861633,\n",
              "   1.2054556012153625,\n",
              "   1.1317315697669983,\n",
              "   1.1219221353530884,\n",
              "   1.1150943636894226,\n",
              "   1.1370155811309814,\n",
              "   1.1282158493995667,\n",
              "   1.1976128816604614,\n",
              "   1.1238325834274292,\n",
              "   1.149198591709137,\n",
              "   1.243927001953125,\n",
              "   1.1418530344963074,\n",
              "   1.1248565316200256,\n",
              "   1.1297152042388916,\n",
              "   1.108199656009674,\n",
              "   1.110725462436676,\n",
              "   1.1570881307125092,\n",
              "   1.1156365871429443,\n",
              "   1.1233295798301697,\n",
              "   1.11020165681839,\n",
              "   1.1094610095024109,\n",
              "   1.1305848956108093,\n",
              "   1.1241894364356995,\n",
              "   1.1270375847816467,\n",
              "   1.1151285767555237,\n",
              "   1.1035293936729431,\n",
              "   1.6741774678230286,\n",
              "   1.1579166650772095,\n",
              "   1.1002985835075378,\n",
              "   1.0993651151657104,\n",
              "   1.1254552006721497,\n",
              "   1.1241546869277954,\n",
              "   1.1126863956451416,\n",
              "   1.0975653529167175,\n",
              "   1.1595491766929626,\n",
              "   1.1615989208221436,\n",
              "   1.0957729816436768,\n",
              "   1.1882309913635254,\n",
              "   1.0944136381149292,\n",
              "   1.1318917274475098,\n",
              "   1.1332588195800781,\n",
              "   1.0877169370651245,\n",
              "   1.2031458616256714,\n",
              "   1.0927485227584839,\n",
              "   1.085323989391327,\n",
              "   1.090003490447998,\n",
              "   1.0853752493858337,\n",
              "   1.0893093049526215,\n",
              "   1.1145064234733582,\n",
              "   1.1259496212005615,\n",
              "   1.0955997109413147,\n",
              "   1.089449942111969,\n",
              "   1.0876508951187134,\n",
              "   1.0829508602619171,\n",
              "   1.0897480845451355,\n",
              "   1.097870647907257,\n",
              "   1.1309944689273834,\n",
              "   1.2084795236587524,\n",
              "   1.097982108592987,\n",
              "   1.0740185976028442,\n",
              "   1.075371265411377,\n",
              "   1.0741530656814575,\n",
              "   1.0718939304351807,\n",
              "   1.086726725101471,\n",
              "   1.0842975974082947,\n",
              "   1.067007601261139,\n",
              "   1.064107358455658,\n",
              "   1.0656619668006897,\n",
              "   1.0614149272441864,\n",
              "   1.078304946422577,\n",
              "   1.0879138112068176,\n",
              "   1.1465469002723694,\n",
              "   1.0718070268630981,\n",
              "   1.0939049422740936,\n",
              "   1.0644907653331757,\n",
              "   1.0599843859672546,\n",
              "   1.065770298242569,\n",
              "   1.0745810866355896,\n",
              "   1.0579688251018524,\n",
              "   1.1066813468933105,\n",
              "   1.1233870387077332,\n",
              "   1.0586539506912231,\n",
              "   1.0531177520751953,\n",
              "   1.0511820316314697,\n",
              "   1.0646236538887024,\n",
              "   1.0663525462150574,\n",
              "   1.0814962983131409,\n",
              "   1.149109661579132,\n",
              "   1.051327109336853,\n",
              "   1.0470050871372223,\n",
              "   1.0457350611686707,\n",
              "   1.0440555214881897,\n",
              "   1.1371106505393982,\n",
              "   1.046600192785263,\n",
              "   1.0468435287475586,\n",
              "   1.0858331322669983,\n",
              "   1.0490275025367737,\n",
              "   1.0639318227767944,\n",
              "   1.0422832071781158,\n",
              "   1.0401406288146973,\n",
              "   1.1232749819755554,\n",
              "   1.033904105424881,\n",
              "   1.0462799072265625,\n",
              "   1.1391922235488892,\n",
              "   1.0581468045711517,\n",
              "   1.0426008701324463,\n",
              "   1.0351221859455109,\n",
              "   1.046314001083374,\n",
              "   1.0389156937599182,\n",
              "   1.0292689800262451,\n",
              "   1.0313579440116882,\n",
              "   1.0462965965270996,\n",
              "   1.0470741391181946,\n",
              "   1.0678094625473022,\n",
              "   1.024064838886261,\n",
              "   1.0527625679969788,\n",
              "   1.0550160706043243,\n",
              "   1.0408345758914948,\n",
              "   1.0312142074108124,\n",
              "   1.0219578742980957,\n",
              "   1.0212666690349579,\n",
              "   1.0213667154312134,\n",
              "   1.0188491642475128,\n",
              "   1.0626064836978912,\n",
              "   1.0418742299079895,\n",
              "   1.0318166613578796,\n",
              "   1.0236463844776154,\n",
              "   1.0345535576343536,\n",
              "   1.035458117723465,\n",
              "   1.068551778793335,\n",
              "   1.054992288351059,\n",
              "   1.0157350897789001,\n",
              "   1.0360284447669983,\n",
              "   1.0125740766525269,\n",
              "   1.076067566871643,\n",
              "   1.009170800447464,\n",
              "   1.2177072763442993,\n",
              "   1.007378339767456,\n",
              "   1.0540987849235535,\n",
              "   1.0622116029262543,\n",
              "   1.0085661709308624,\n",
              "   1.0394746661186218,\n",
              "   1.0105744302272797,\n",
              "   1.0111986100673676,\n",
              "   1.0198039412498474,\n",
              "   1.0208095908164978,\n",
              "   1.05459463596344,\n",
              "   1.0697947144508362,\n",
              "   1.0010193288326263,\n",
              "   1.026738315820694,\n",
              "   1.0280379056930542,\n",
              "   1.0025059282779694,\n",
              "   1.127025544643402,\n",
              "   0.9982667863368988,\n",
              "   1.0264227390289307,\n",
              "   0.9971995055675507,\n",
              "   1.0139498710632324,\n",
              "   1.0145381093025208,\n",
              "   1.0520015060901642,\n",
              "   1.0623237192630768,\n",
              "   0.9925461411476135,\n",
              "   1.0181754231452942,\n",
              "   1.018429696559906,\n",
              "   1.041060447692871,\n",
              "   0.9891084730625153,\n",
              "   0.9999127089977264,\n",
              "   1.0348461866378784,\n",
              "   0.9903388023376465,\n",
              "   0.9874302744865417,\n",
              "   0.9926870465278625,\n",
              "   0.9875653386116028,\n",
              "   0.9849779903888702,\n",
              "   0.9910907745361328,\n",
              "   1.058873027563095,\n",
              "   0.9964341223239899,\n",
              "   0.9950470328330994,\n",
              "   1.0764693021774292,\n",
              "   0.9799746870994568,\n",
              "   1.0717783272266388,\n",
              "   1.0053938925266266,\n",
              "   1.033538967370987,\n",
              "   0.9949703514575958,\n",
              "   1.0002915263175964,\n",
              "   0.9971796274185181,\n",
              "   0.9864665269851685,\n",
              "   0.9933935105800629,\n",
              "   0.9994908273220062,\n",
              "   1.0113630890846252,\n",
              "   0.9730965793132782,\n",
              "   1.042449027299881,\n",
              "   1.0243311524391174,\n",
              "   0.9693747460842133,\n",
              "   1.0624578595161438,\n",
              "   0.9713210761547089,\n",
              "   0.9791221916675568,\n",
              "   0.9748100340366364,\n",
              "   0.9667768478393555,\n",
              "   0.9722402393817902,\n",
              "   0.9788067936897278,\n",
              "   1.0089465379714966,\n",
              "   1.0521049201488495,\n",
              "   0.9633569121360779,\n",
              "   0.9950807094573975,\n",
              "   0.9733279943466187,\n",
              "   0.9643701016902924,\n",
              "   0.9699323773384094,\n",
              "   0.9656293392181396,\n",
              "   1.1145234107971191,\n",
              "   0.9647288918495178,\n",
              "   0.9676799178123474,\n",
              "   0.9610637426376343,\n",
              "   0.9725618660449982,\n",
              "   0.977369636297226,\n",
              "   0.969382643699646,\n",
              "   0.964944988489151,\n",
              "   0.9615203142166138,\n",
              "   0.9611211717128754,\n",
              "   0.96741583943367,\n",
              "   0.9625250101089478,\n",
              "   0.9947162866592407,\n",
              "   0.9585566222667694,\n",
              "   0.9572052955627441,\n",
              "   0.9504950046539307,\n",
              "   0.9729289412498474,\n",
              "   0.9553009271621704,\n",
              "   0.9557360410690308,\n",
              "   0.9589448571205139,\n",
              "   0.9998719692230225,\n",
              "   0.9868359863758087,\n",
              "   0.9507785737514496,\n",
              "   0.9444704353809357,\n",
              "   0.9594027400016785,\n",
              "   1.0663167238235474,\n",
              "   1.0230916440486908,\n",
              "   0.962265282869339,\n",
              "   1.0855973958969116,\n",
              "   0.9475443363189697,\n",
              "   0.9762268960475922,\n",
              "   1.0006395280361176,\n",
              "   0.9641497433185577,\n",
              "   0.9624587893486023,\n",
              "   0.9533984959125519,\n",
              "   0.9414470791816711,\n",
              "   0.949923574924469,\n",
              "   0.9482628703117371,\n",
              "   0.9523946940898895,\n",
              "   0.9381605982780457,\n",
              "   0.9343487322330475,\n",
              "   0.9632836580276489,\n",
              "   0.9528465867042542,\n",
              "   0.9604768753051758,\n",
              "   0.9548020958900452,\n",
              "   0.9587506949901581,\n",
              "   0.9377669095993042,\n",
              "   0.9749462902545929,\n",
              "   0.9805106520652771,\n",
              "   0.9301799535751343,\n",
              "   0.9341380894184113,\n",
              "   0.951064795255661,\n",
              "   0.989194244146347,\n",
              "   0.9417015314102173,\n",
              "   0.9749050140380859,\n",
              "   0.9295884966850281,\n",
              "   0.9388575851917267,\n",
              "   0.967422217130661,\n",
              "   0.9271157383918762,\n",
              "   0.9458155333995819,\n",
              "   0.9240213632583618,\n",
              "   0.9317231774330139,\n",
              "   0.9297813475131989,\n",
              "   0.9242691397666931,\n",
              "   0.9403150975704193,\n",
              "   0.9725410640239716,\n",
              "   0.9433950185775757,\n",
              "   1.0184927582740784,\n",
              "   0.9220251142978668,\n",
              "   0.9814863502979279,\n",
              "   0.9348280727863312,\n",
              "   0.9370029270648956,\n",
              "   0.9555603265762329,\n",
              "   0.919491708278656,\n",
              "   0.9284400641918182,\n",
              "   0.9283583164215088,\n",
              "   0.9785110950469971,\n",
              "   0.9277644753456116,\n",
              "   0.9160252511501312,\n",
              "   1.0419237315654755,\n",
              "   0.9160085916519165,\n",
              "   0.9278105199337006,\n",
              "   0.9347971677780151,\n",
              "   0.9485487341880798,\n",
              "   0.9156444370746613,\n",
              "   1.0643024444580078,\n",
              "   0.9197591543197632,\n",
              "   0.9153402149677277,\n",
              "   0.9258918166160583,\n",
              "   0.9642741084098816,\n",
              "   0.9164535701274872,\n",
              "   0.9144928157329559,\n",
              "   0.9216280877590179,\n",
              "   1.0553365647792816,\n",
              "   0.9079491794109344,\n",
              "   0.9163907766342163,\n",
              "   0.9090652167797089,\n",
              "   0.9523327648639679,\n",
              "   0.9832633137702942,\n",
              "   0.9058481454849243,\n",
              "   0.910046249628067,\n",
              "   0.912134200334549,\n",
              "   0.9943433403968811,\n",
              "   0.9032469093799591,\n",
              "   0.9491295516490936,\n",
              "   0.9040254354476929,\n",
              "   0.9181154370307922,\n",
              "   1.0238453149795532,\n",
              "   0.9028347730636597,\n",
              "   0.9059968888759613,\n",
              "   0.9692135155200958,\n",
              "   0.9096992015838623,\n",
              "   0.9092378318309784,\n",
              "   0.9353261888027191,\n",
              "   0.9231463670730591,\n",
              "   0.9138407409191132,\n",
              "   0.9087121188640594,\n",
              "   0.9106937050819397,\n",
              "   0.9808182120323181,\n",
              "   0.9120449721813202,\n",
              "   0.9075861871242523,\n",
              "   0.8977683782577515,\n",
              "   0.8936887681484222,\n",
              "   0.9270841181278229,\n",
              "   0.8937246203422546,\n",
              "   0.9205597639083862,\n",
              "   0.9319165945053101,\n",
              "   0.9074927270412445,\n",
              "   0.9238694608211517,\n",
              "   0.904306948184967,\n",
              "   0.9167342483997345,\n",
              "   0.8920330703258514,\n",
              "   0.892332524061203,\n",
              "   0.908260703086853,\n",
              "   0.9360846281051636,\n",
              "   0.9249968826770782,\n",
              "   0.8896576464176178,\n",
              "   0.9044544398784637,\n",
              "   0.908302515745163,\n",
              "   0.9103160798549652,\n",
              "   0.9344175159931183,\n",
              "   0.8939371109008789,\n",
              "   0.9354499280452728,\n",
              "   0.8953676521778107,\n",
              "   0.9138783812522888,\n",
              "   0.8971849381923676,\n",
              "   0.9669270813465118,\n",
              "   0.914597362279892,\n",
              "   0.9137133657932281,\n",
              "   0.8865362703800201,\n",
              "   0.9120219051837921,\n",
              "   0.903459370136261,\n",
              "   0.8871983885765076,\n",
              "   0.9931473135948181,\n",
              "   0.9160346686840057,\n",
              "   0.9016759693622589,\n",
              "   0.9177422225475311,\n",
              "   0.9276357591152191,\n",
              "   0.8811628222465515,\n",
              "   0.8825426399707794,\n",
              "   0.890631377696991,\n",
              "   0.8978403508663177,\n",
              "   0.8798846304416656,\n",
              "   0.9349896311759949,\n",
              "   0.8986505568027496,\n",
              "   0.8984300792217255,\n",
              "   0.91139155626297,\n",
              "   0.9119942486286163,\n",
              "   0.8784866333007812,\n",
              "   0.8857274651527405,\n",
              "   0.915277361869812,\n",
              "   0.8967501819133759,\n",
              "   0.8908207416534424,\n",
              "   0.8918049037456512,\n",
              "   0.9025925993919373,\n",
              "   0.8813751637935638,\n",
              "   0.9000226855278015,\n",
              "   0.9963178336620331,\n",
              "   0.877532035112381,\n",
              "   0.917050838470459,\n",
              "   0.8765324056148529,\n",
              "   0.9044062495231628,\n",
              "   0.9078643321990967,\n",
              "   0.8848457038402557,\n",
              "   0.9162789881229401,\n",
              "   0.8743347525596619,\n",
              "   0.8882644176483154,\n",
              "   0.8736771941184998,\n",
              "   0.8758105635643005,\n",
              "   0.8955941498279572,\n",
              "   0.8828085362911224,\n",
              "   0.9236308336257935,\n",
              "   0.8701130449771881,\n",
              "   0.9378296136856079,\n",
              "   0.8750011622905731,\n",
              "   0.9008659422397614,\n",
              "   0.8899129033088684,\n",
              "   0.8893510401248932,\n",
              "   0.8726111054420471,\n",
              "   0.8663888573646545,\n",
              "   0.9549018144607544,\n",
              "   0.8752086460590363,\n",
              "   0.8808284103870392,\n",
              "   0.8676111996173859,\n",
              "   0.8850182592868805,\n",
              "   0.8756830096244812,\n",
              "   0.9045651853084564,\n",
              "   0.8797760307788849,\n",
              "   0.8646804392337799,\n",
              "   0.8836953639984131,\n",
              "   0.8668237924575806,\n",
              "   0.8635802567005157,\n",
              "   0.8711888194084167,\n",
              "   0.8765208423137665,\n",
              "   0.9161504805088043,\n",
              "   0.8632667064666748,\n",
              "   0.8769374191761017,\n",
              "   0.8684814870357513,\n",
              "   0.8715404272079468,\n",
              "   0.9063768982887268,\n",
              "   0.8882482051849365,\n",
              "   0.8705894649028778,\n",
              "   0.8614673018455505,\n",
              "   0.8880421817302704,\n",
              "   0.8615310192108154,\n",
              "   0.8808781504631042,\n",
              "   0.8718625903129578,\n",
              "   0.8950794339179993,\n",
              "   0.8613838255405426,\n",
              "   0.9065673053264618,\n",
              "   0.869682788848877,\n",
              "   0.8850297629833221,\n",
              "   0.8647533655166626,\n",
              "   0.8662612438201904,\n",
              "   0.8682138919830322,\n",
              "   0.9148454964160919,\n",
              "   0.8721708357334137,\n",
              "   0.8696468770503998,\n",
              "   0.8656661212444305,\n",
              "   0.8729320168495178,\n",
              "   0.894049882888794,\n",
              "   0.8639590442180634,\n",
              "   0.8721391558647156,\n",
              "   0.8582550883293152,\n",
              "   0.8610870540142059,\n",
              "   0.8751294910907745,\n",
              "   0.862800806760788,\n",
              "   0.8779631555080414,\n",
              "   0.9625798165798187,\n",
              "   0.8679647743701935,\n",
              "   0.8581861257553101,\n",
              "   0.8585619628429413,\n",
              "   0.8657689988613129,\n",
              "   0.8705786466598511,\n",
              "   0.8639421761035919,\n",
              "   0.8789456188678741,\n",
              "   0.87428879737854,\n",
              "   0.865751177072525,\n",
              "   0.8679856956005096,\n",
              "   0.8822379112243652,\n",
              "   0.8561123609542847,\n",
              "   0.8558408319950104,\n",
              "   0.8918100297451019,\n",
              "   0.870336651802063,\n",
              "   0.8735912144184113,\n",
              "   0.8618680238723755,\n",
              "   0.8578804433345795,\n",
              "   0.9325954616069794,\n",
              "   0.8788744211196899,\n",
              "   0.8568744957447052,\n",
              "   0.8957935571670532,\n",
              "   0.8567506968975067,\n",
              "   0.8497709333896637,\n",
              "   0.8671782910823822,\n",
              "   0.9122336804866791,\n",
              "   0.8979278802871704,\n",
              "   0.9062187373638153,\n",
              "   0.8585882484912872,\n",
              "   0.8614005446434021,\n",
              "   0.8517408668994904,\n",
              "   0.8589715659618378,\n",
              "   0.8457450270652771,\n",
              "   0.8511861860752106,\n",
              "   0.8473389744758606,\n",
              "   0.9208363592624664,\n",
              "   0.8470076620578766,\n",
              "   0.8706324100494385,\n",
              "   0.8577669262886047,\n",
              "   0.9118613004684448,\n",
              "   0.8471550941467285,\n",
              "   0.8906241357326508,\n",
              "   0.889257401227951,\n",
              "   0.8640926480293274,\n",
              "   0.9024929702281952,\n",
              "   0.9232405424118042,\n",
              "   0.855506956577301,\n",
              "   0.8480016887187958,\n",
              "   0.8768677711486816,\n",
              "   0.8651162087917328,\n",
              "   0.843714565038681,\n",
              "   0.870429664850235,\n",
              "   0.8722617626190186,\n",
              "   0.8518107831478119,\n",
              "   0.892594575881958,\n",
              "   0.8870274126529694,\n",
              "   0.8462350368499756,\n",
              "   0.863716185092926,\n",
              "   0.8541825413703918,\n",
              "   0.8739117681980133,\n",
              "   0.845431387424469,\n",
              "   0.8602179288864136,\n",
              "   0.876950204372406,\n",
              "   0.8703335225582123,\n",
              "   0.8485528528690338,\n",
              "   0.8535005152225494,\n",
              "   0.913293331861496,\n",
              "   0.8550817370414734,\n",
              "   0.8432376980781555,\n",
              "   0.8420266211032867,\n",
              "   0.8458029925823212,\n",
              "   0.8451061844825745,\n",
              "   0.8688989877700806,\n",
              "   0.8525671362876892,\n",
              "   0.8481650352478027,\n",
              "   0.875681459903717,\n",
              "   0.8512634932994843,\n",
              "   0.8518147468566895,\n",
              "   0.8994961380958557,\n",
              "   0.8808230757713318,\n",
              "   0.8597887456417084,\n",
              "   0.8395143449306488,\n",
              "   0.8911426067352295,\n",
              "   0.8417510390281677,\n",
              "   0.8533312976360321,\n",
              "   0.873139500617981,\n",
              "   0.8449477851390839,\n",
              "   0.8372808396816254,\n",
              "   0.849259078502655,\n",
              "   0.8557201325893402,\n",
              "   0.8751016855239868,\n",
              "   0.8540201783180237,\n",
              "   0.8365562558174133,\n",
              "   0.8409644663333893,\n",
              "   0.8663311898708344,\n",
              "   0.8882728219032288,\n",
              "   0.8362426459789276,\n",
              "   0.8343324959278107,\n",
              "   0.8575112521648407,\n",
              "   0.8552305698394775,\n",
              "   0.8675611019134521,\n",
              "   0.8690192699432373,\n",
              "   0.8358012139797211,\n",
              "   0.8445729911327362,\n",
              "   0.9008162021636963,\n",
              "   0.8390204906463623,\n",
              "   0.8325730860233307,\n",
              "   0.8610871434211731,\n",
              "   0.8652822375297546,\n",
              "   0.8501926362514496,\n",
              "   0.8496474921703339,\n",
              "   0.8764729201793671,\n",
              "   0.8429608643054962,\n",
              "   0.8482231795787811,\n",
              "   0.8504965603351593,\n",
              "   0.8489586710929871,\n",
              "   0.8376547694206238,\n",
              "   0.8573472201824188,\n",
              "   0.9105426669120789,\n",
              "   0.8382275998592377,\n",
              "   0.8364825546741486,\n",
              "   0.8544116020202637,\n",
              "   0.8306666612625122,\n",
              "   0.8677213191986084,\n",
              "   0.8469457030296326,\n",
              "   0.8456262350082397,\n",
              "   0.8446915745735168,\n",
              "   0.8297523260116577,\n",
              "   0.8768519759178162,\n",
              "   0.9095338582992554,\n",
              "   0.8370647430419922,\n",
              "   0.8635378479957581,\n",
              "   0.8341895639896393,\n",
              "   0.8411122262477875,\n",
              "   0.8385226726531982,\n",
              "   0.8684604167938232,\n",
              "   0.8780740797519684,\n",
              "   0.8339014053344727,\n",
              "   0.8710507750511169,\n",
              "   0.834655374288559,\n",
              "   0.851648360490799,\n",
              "   0.8329973518848419,\n",
              "   0.8807662725448608,\n",
              "   0.8315342664718628,\n",
              "   0.8380308151245117,\n",
              "   0.8472647070884705,\n",
              "   0.8438476920127869,\n",
              "   0.857489138841629,\n",
              "   0.8761570453643799,\n",
              "   0.8508825898170471,\n",
              "   0.8301691114902496,\n",
              "   0.8621982932090759,\n",
              "   0.8975647389888763,\n",
              "   0.8291983604431152,\n",
              "   0.8666727542877197,\n",
              "   0.9478893876075745,\n",
              "   0.8287782371044159,\n",
              "   0.8446413576602936,\n",
              "   0.8769128620624542,\n",
              "   0.834099143743515,\n",
              "   0.8427218496799469,\n",
              "   0.8431419134140015,\n",
              "   0.8283473253250122,\n",
              "   0.916246086359024,\n",
              "   0.867220938205719,\n",
              "   0.8383901417255402,\n",
              "   0.8726910352706909,\n",
              "   0.8504530191421509,\n",
              "   0.8319124281406403,\n",
              "   0.8333232700824738,\n",
              "   0.8963823020458221,\n",
              "   0.8279984891414642,\n",
              "   0.8834902942180634,\n",
              "   0.836594969034195,\n",
              "   0.8498449325561523,\n",
              "   0.8676308691501617,\n",
              "   0.8258528411388397,\n",
              "   0.8325013518333435,\n",
              "   0.8276589810848236,\n",
              "   0.8748761117458344,\n",
              "   0.8344667255878448,\n",
              "   0.8399213254451752,\n",
              "   0.908502072095871,\n",
              "   0.8608183264732361,\n",
              "   0.8248246312141418,\n",
              "   0.8936169147491455,\n",
              "   0.8459299206733704,\n",
              "   0.8668255507946014,\n",
              "   0.8948351144790649,\n",
              "   0.8275106251239777,\n",
              "   0.8304338157176971,\n",
              "   0.8283112943172455,\n",
              "   0.8485014736652374,\n",
              "   0.8404598534107208,\n",
              "   0.8249419927597046,\n",
              "   0.8357341885566711,\n",
              "   0.8464608192443848,\n",
              "   0.8501711189746857,\n",
              "   0.8345947563648224,\n",
              "   0.824425220489502,\n",
              "   0.8823866546154022,\n",
              "   0.8508282005786896,\n",
              "   0.8330174684524536,\n",
              "   0.8611763715744019,\n",
              "   0.8472930490970612,\n",
              "   0.8208082914352417,\n",
              "   0.8430127203464508,\n",
              "   0.8321574628353119,\n",
              "   0.8613613247871399,\n",
              "   0.8216745555400848,\n",
              "   0.8407703936100006,\n",
              "   0.8217909038066864,\n",
              "   0.826351672410965,\n",
              "   0.8857003450393677,\n",
              "   0.8782795965671539,\n",
              "   0.8342896699905396,\n",
              "   0.8285800814628601,\n",
              "   0.8488875925540924,\n",
              "   0.8316742479801178,\n",
              "   0.8433917760848999,\n",
              "   0.8385938704013824,\n",
              "   0.828935295343399,\n",
              "   0.8259933292865753,\n",
              "   0.8705556392669678,\n",
              "   0.8211869597434998,\n",
              "   0.8399709165096283,\n",
              "   0.8275659084320068,\n",
              "   0.8595298826694489,\n",
              "   0.8203094899654388,\n",
              "   0.8390071988105774,\n",
              "   0.8239130973815918,\n",
              "   0.8526745438575745,\n",
              "   0.8307501673698425,\n",
              "   0.8412224650382996,\n",
              "   0.9441035389900208,\n",
              "   0.8249614238739014,\n",
              "   0.8290010392665863,\n",
              "   0.862930417060852,\n",
              "   0.8260957300662994,\n",
              "   0.8213999271392822,\n",
              "   0.8311622440814972,\n",
              "   0.8488636612892151,\n",
              "   0.8370067775249481,\n",
              "   0.8396836221218109,\n",
              "   0.8192808628082275,\n",
              "   0.8265437185764313,\n",
              "   0.8423132598400116,\n",
              "   0.8165141940116882,\n",
              "   0.8286799192428589,\n",
              "   0.8267703652381897,\n",
              "   0.8613573312759399,\n",
              "   0.8310534060001373,\n",
              "   0.8442652523517609,\n",
              "   0.8274329602718353,\n",
              "   0.8289484977722168,\n",
              "   0.8219626843929291,\n",
              "   0.9201827347278595,\n",
              "   0.8168612718582153,\n",
              "   0.8488906621932983,\n",
              "   0.8826866149902344,\n",
              "   0.8174928724765778,\n",
              "   0.8264227509498596,\n",
              "   0.855158120393753,\n",
              "   0.8226819038391113,\n",
              "   0.8239741027355194,\n",
              "   0.8467530012130737,\n",
              "   0.8219249248504639,\n",
              "   0.8563691675662994,\n",
              "   0.8223221004009247,\n",
              "   0.8180891871452332,\n",
              "   0.8270919620990753,\n",
              "   0.8706457316875458,\n",
              "   0.8165713548660278,\n",
              "   0.820009171962738,\n",
              "   0.8864668011665344,\n",
              "   0.8347272276878357,\n",
              "   0.8241733610630035,\n",
              "   0.8217480480670929,\n",
              "   0.8400746285915375,\n",
              "   0.819381058216095,\n",
              "   0.8702437877655029,\n",
              "   0.8456923365592957,\n",
              "   0.8155340850353241,\n",
              "   0.8170499801635742,\n",
              "   0.8157680332660675,\n",
              "   0.8394420742988586,\n",
              "   0.8206271231174469,\n",
              "   0.878201425075531,\n",
              "   0.820423424243927,\n",
              "   0.836298942565918,\n",
              "   0.8494850695133209,\n",
              "   0.823825865983963,\n",
              "   0.8540322184562683,\n",
              "   0.8157981932163239,\n",
              "   0.904815673828125,\n",
              "   0.8480753600597382,\n",
              "   0.8154384195804596,\n",
              "   0.8569469153881073,\n",
              "   0.8239641487598419,\n",
              "   0.8418524861335754,\n",
              "   0.8295014500617981,\n",
              "   0.8315439820289612,\n",
              "   0.8491670191287994,\n",
              "   0.8213172256946564,\n",
              "   0.8391446173191071,\n",
              "   0.8360628485679626,\n",
              "   0.8337027430534363,\n",
              "   0.8331383168697357,\n",
              "   0.838096559047699,\n",
              "   0.8260428011417389,\n",
              "   0.837427020072937,\n",
              "   0.836320161819458,\n",
              "   0.8143441081047058,\n",
              "   0.8286118805408478,\n",
              "   0.8631396889686584,\n",
              "   0.8604079782962799,\n",
              "   0.8162622153759003,\n",
              "   0.8346713483333588,\n",
              "   0.8431044518947601,\n",
              "   0.8183850646018982,\n",
              "   0.8423052430152893,\n",
              "   0.8129952251911163,\n",
              "   0.8367450535297394,\n",
              "   0.8281488418579102,\n",
              "   0.8636552393436432,\n",
              "   0.8206640183925629,\n",
              "   0.8307966887950897,\n",
              "   0.8370248079299927,\n",
              "   0.8260648548603058,\n",
              "   0.8247562646865845,\n",
              "   0.8389716148376465,\n",
              "   0.8347176611423492,\n",
              "   0.8191591501235962,\n",
              "   0.8662858605384827,\n",
              "   0.8552708327770233,\n",
              "   0.8135521709918976,\n",
              "   0.8165722191333771,\n",
              "   0.8122588694095612,\n",
              "   0.8813782036304474,\n",
              "   0.8214436173439026,\n",
              "   0.8155041038990021,\n",
              "   0.808888703584671,\n",
              "   0.9003795385360718,\n",
              "   0.827966719865799,\n",
              "   0.8128054738044739,\n",
              "   0.8617609441280365,\n",
              "   0.8245907127857208,\n",
              "   0.8159518539905548,\n",
              "   0.8352843523025513,\n",
              "   0.8844355642795563,\n",
              "   0.8450435101985931,\n",
              "   0.8316094279289246,\n",
              "   0.812743216753006,\n",
              "   0.8855067789554596,\n",
              "   0.8148677349090576,\n",
              "   0.8277295529842377,\n",
              "   0.8326423466205597,\n",
              "   0.8324410319328308,\n",
              "   0.8146730065345764,\n",
              "   0.8242801129817963,\n",
              "   0.8138927221298218,\n",
              "   0.8149007558822632,\n",
              "   0.8466622829437256,\n",
              "   0.8560759723186493,\n",
              "   0.8144483268260956,\n",
              "   0.836624026298523,\n",
              "   0.8200425207614899,\n",
              "   0.8410806953907013,\n",
              "   0.8138216137886047,\n",
              "   0.8117157816886902,\n",
              "   0.8161446154117584,\n",
              "   0.8244889676570892,\n",
              "   0.8329100906848907,\n",
              "   0.841867208480835,\n",
              "   0.8287526071071625,\n",
              "   0.8197686970233917,\n",
              "   0.8478281497955322,\n",
              "   0.8139793574810028,\n",
              "   ...]})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "_, model_loss_record = train(tr_set, dv_set, tt_set, model, ae, config, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX2B_zgSOPTJ"
      },
      "source": [
        "# **Start Training!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hsNO9nnXQBvP",
        "outputId": "48658096-2d4a-4b6b-f9a7-500142ae6ba7"
      },
      "source": [
        "plot_learning_curve(model_loss_record, title='deep model')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_loss_record' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-b16e30382e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_loss_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deep model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_loss_record' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "3iZTVn5WQFpX",
        "outputId": "de1a49b4-26c7-4352-b326-a5553f44ed86"
      },
      "source": [
        "# del model\n",
        "# del ae\n",
        "# ae = AE(tr_set.dataset.dim, config['feat_dim']).to(device)\n",
        "ae = create_ae()\n",
        "ckpt = torch.load(config['ae_save_path'], map_location='cpu')  # Load your best model\n",
        "ae.load_state_dict(ckpt)\n",
        "\n",
        "model = NeuralNet(tr_set.dataset.dim, ae).to(device)\n",
        "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 360x360 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"331.674375pt\" version=\"1.1\" viewBox=\"0 0 333.165625 331.674375\" width=\"333.165625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-12T02:18:43.350870</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 331.674375 \nL 333.165625 331.674375 \nL 333.165625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 294.118125 \nL 319.603125 294.118125 \nL 319.603125 22.318125 \nL 40.603125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"maea81207da\" style=\"stroke:#ff0000;stroke-opacity:0.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p353d5cb6a0)\">\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"176.445284\" xlink:href=\"#maea81207da\" y=\"176.364466\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"247.237733\" xlink:href=\"#maea81207da\" y=\"93.232377\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"211.850028\" xlink:href=\"#maea81207da\" y=\"127.412399\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"140.12584\" xlink:href=\"#maea81207da\" y=\"194.117057\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"211.155254\" xlink:href=\"#maea81207da\" y=\"124.39845\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"100.858218\" xlink:href=\"#maea81207da\" y=\"237.212161\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"150.337319\" xlink:href=\"#maea81207da\" y=\"177.734929\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"236.084544\" xlink:href=\"#maea81207da\" y=\"96.89417\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"71.579528\" xlink:href=\"#maea81207da\" y=\"268.398299\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"296.79843\" xlink:href=\"#maea81207da\" y=\"47.899842\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"240.408885\" xlink:href=\"#maea81207da\" y=\"98.282439\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"86.30771\" xlink:href=\"#maea81207da\" y=\"248.747687\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"205.098402\" xlink:href=\"#maea81207da\" y=\"133.000526\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"182.121937\" xlink:href=\"#maea81207da\" y=\"168.64219\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"257.854842\" xlink:href=\"#maea81207da\" y=\"82.605204\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"329.97428\" xlink:href=\"#maea81207da\" y=\"14.144834\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"171.80442\" xlink:href=\"#maea81207da\" y=\"160.327041\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"186.735634\" xlink:href=\"#maea81207da\" y=\"157.430914\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"110.968873\" xlink:href=\"#maea81207da\" y=\"232.388736\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"147.795508\" xlink:href=\"#maea81207da\" y=\"195.937284\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"126.489383\" xlink:href=\"#maea81207da\" y=\"211.519812\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"237.272161\" xlink:href=\"#maea81207da\" y=\"97.905025\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"190.227145\" xlink:href=\"#maea81207da\" y=\"148.459525\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"213.647441\" xlink:href=\"#maea81207da\" y=\"120.15273\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"209.749138\" xlink:href=\"#maea81207da\" y=\"142.521761\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"119.594303\" xlink:href=\"#maea81207da\" y=\"218.359505\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"217.360414\" xlink:href=\"#maea81207da\" y=\"125.045735\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"188.318192\" xlink:href=\"#maea81207da\" y=\"154.255461\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"215.88864\" xlink:href=\"#maea81207da\" y=\"118.105806\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"235.990178\" xlink:href=\"#maea81207da\" y=\"104.652934\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"243.488493\" xlink:href=\"#maea81207da\" y=\"87.733559\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"296.505536\" xlink:href=\"#maea81207da\" y=\"58.398\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"214.869542\" xlink:href=\"#maea81207da\" y=\"113.9253\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"118.703261\" xlink:href=\"#maea81207da\" y=\"210.121823\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"226.378577\" xlink:href=\"#maea81207da\" y=\"112.140324\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"195.217807\" xlink:href=\"#maea81207da\" y=\"134.792011\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"92.903014\" xlink:href=\"#maea81207da\" y=\"243.709874\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"110.641351\" xlink:href=\"#maea81207da\" y=\"227.44812\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"118.936606\" xlink:href=\"#maea81207da\" y=\"218.619369\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"121.632801\" xlink:href=\"#maea81207da\" y=\"222.599577\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"124.57656\" xlink:href=\"#maea81207da\" y=\"203.652321\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"206.298491\" xlink:href=\"#maea81207da\" y=\"138.952192\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"124.160606\" xlink:href=\"#maea81207da\" y=\"199.035262\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"103.277071\" xlink:href=\"#maea81207da\" y=\"232.50147\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"155.783651\" xlink:href=\"#maea81207da\" y=\"175.890836\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"131.814249\" xlink:href=\"#maea81207da\" y=\"199.135492\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"202.416636\" xlink:href=\"#maea81207da\" y=\"135.102973\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"141.516059\" xlink:href=\"#maea81207da\" y=\"190.667402\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"176.497547\" xlink:href=\"#maea81207da\" y=\"159.584172\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"258.79773\" xlink:href=\"#maea81207da\" y=\"85.536235\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"245.054058\" xlink:href=\"#maea81207da\" y=\"112.175435\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"76.941359\" xlink:href=\"#maea81207da\" y=\"266.858667\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"149.986879\" xlink:href=\"#maea81207da\" y=\"182.419154\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"96.811287\" xlink:href=\"#maea81207da\" y=\"239.776239\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"189.043126\" xlink:href=\"#maea81207da\" y=\"160.09717\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"177.691106\" xlink:href=\"#maea81207da\" y=\"172.50208\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"141.41037\" xlink:href=\"#maea81207da\" y=\"196.311229\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"215.885012\" xlink:href=\"#maea81207da\" y=\"116.846346\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"111.391766\" xlink:href=\"#maea81207da\" y=\"220.78597\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"145.071458\" xlink:href=\"#maea81207da\" y=\"191.720798\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"247.861619\" xlink:href=\"#maea81207da\" y=\"97.598865\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"237.698033\" xlink:href=\"#maea81207da\" y=\"103.59445\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"145.748884\" xlink:href=\"#maea81207da\" y=\"184.40995\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"71.791995\" xlink:href=\"#maea81207da\" y=\"267.369835\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"227.137707\" xlink:href=\"#maea81207da\" y=\"94.715139\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"256.03327\" xlink:href=\"#maea81207da\" y=\"75.268714\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"164.610376\" xlink:href=\"#maea81207da\" y=\"185.155418\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"198.891231\" xlink:href=\"#maea81207da\" y=\"147.979518\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"322.050351\" xlink:href=\"#maea81207da\" y=\"33.415261\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"111.329609\" xlink:href=\"#maea81207da\" y=\"225.362734\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"135.971023\" xlink:href=\"#maea81207da\" y=\"200.207077\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"171.910533\" xlink:href=\"#maea81207da\" y=\"167.252449\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"221.29441\" xlink:href=\"#maea81207da\" y=\"114.477768\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"138.200272\" xlink:href=\"#maea81207da\" y=\"183.43143\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"170.569559\" xlink:href=\"#maea81207da\" y=\"157.884706\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"140.572921\" xlink:href=\"#maea81207da\" y=\"202.826277\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"132.984005\" xlink:href=\"#maea81207da\" y=\"193.158589\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"152.587982\" xlink:href=\"#maea81207da\" y=\"183.852931\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"233.506971\" xlink:href=\"#maea81207da\" y=\"105.401223\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"105.83518\" xlink:href=\"#maea81207da\" y=\"229.260231\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"271.209652\" xlink:href=\"#maea81207da\" y=\"73.589926\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"69.397494\" xlink:href=\"#maea81207da\" y=\"262.449392\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"253.566678\" xlink:href=\"#maea81207da\" y=\"78.768032\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"64.927269\" xlink:href=\"#maea81207da\" y=\"257.113993\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"245.048736\" xlink:href=\"#maea81207da\" y=\"93.07694\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"203.381295\" xlink:href=\"#maea81207da\" y=\"133.064591\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"243.525653\" xlink:href=\"#maea81207da\" y=\"84.289441\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"84.612041\" xlink:href=\"#maea81207da\" y=\"246.318655\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"257.951309\" xlink:href=\"#maea81207da\" y=\"96.678939\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"192.727404\" xlink:href=\"#maea81207da\" y=\"132.067404\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"120.181936\" xlink:href=\"#maea81207da\" y=\"215.90309\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"103.962797\" xlink:href=\"#maea81207da\" y=\"239.198623\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"240.34031\" xlink:href=\"#maea81207da\" y=\"112.50681\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"262.576466\" xlink:href=\"#maea81207da\" y=\"66.142654\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"151.894025\" xlink:href=\"#maea81207da\" y=\"180.856354\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"130.850687\" xlink:href=\"#maea81207da\" y=\"196.487182\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"189.937123\" xlink:href=\"#maea81207da\" y=\"149.576155\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"122.537873\" xlink:href=\"#maea81207da\" y=\"220.312819\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"92.983218\" xlink:href=\"#maea81207da\" y=\"246.683252\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"70.367534\" xlink:href=\"#maea81207da\" y=\"258.216591\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"209.084827\" xlink:href=\"#maea81207da\" y=\"130.374477\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"128.953216\" xlink:href=\"#maea81207da\" y=\"215.136834\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"117.187759\" xlink:href=\"#maea81207da\" y=\"221.352202\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"126.488\" xlink:href=\"#maea81207da\" y=\"215.577143\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"153.888417\" xlink:href=\"#maea81207da\" y=\"178.670171\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"324.734747\" xlink:href=\"#maea81207da\" y=\"30.006166\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"254.770381\" xlink:href=\"#maea81207da\" y=\"89.457974\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"120.422915\" xlink:href=\"#maea81207da\" y=\"213.740237\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"230.455407\" xlink:href=\"#maea81207da\" y=\"116.091725\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"97.49199\" xlink:href=\"#maea81207da\" y=\"238.637035\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"149.521225\" xlink:href=\"#maea81207da\" y=\"174.998355\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"138.623074\" xlink:href=\"#maea81207da\" y=\"200.883906\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"248.251631\" xlink:href=\"#maea81207da\" y=\"101.36552\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"168.006507\" xlink:href=\"#maea81207da\" y=\"173.876011\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"125.052449\" xlink:href=\"#maea81207da\" y=\"213.193137\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"160.501427\" xlink:href=\"#maea81207da\" y=\"172.375429\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"230.657715\" xlink:href=\"#maea81207da\" y=\"112.742925\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"108.040943\" xlink:href=\"#maea81207da\" y=\"228.010507\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"89.969872\" xlink:href=\"#maea81207da\" y=\"242.70529\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"134.816363\" xlink:href=\"#maea81207da\" y=\"196.178127\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"80.206169\" xlink:href=\"#maea81207da\" y=\"249.231317\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"290.199358\" xlink:href=\"#maea81207da\" y=\"44.146648\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"144.191384\" xlink:href=\"#maea81207da\" y=\"196.421135\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"239.85888\" xlink:href=\"#maea81207da\" y=\"94.344529\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"64.346046\" xlink:href=\"#maea81207da\" y=\"271.718311\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"207.049624\" xlink:href=\"#maea81207da\" y=\"127.985603\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"119.610562\" xlink:href=\"#maea81207da\" y=\"219.970214\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"213.404496\" xlink:href=\"#maea81207da\" y=\"121.513899\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"223.311335\" xlink:href=\"#maea81207da\" y=\"113.796137\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"150.981508\" xlink:href=\"#maea81207da\" y=\"174.035564\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"63.350332\" xlink:href=\"#maea81207da\" y=\"272.93168\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"198.280937\" xlink:href=\"#maea81207da\" y=\"137.202887\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"92.242698\" xlink:href=\"#maea81207da\" y=\"247.271446\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"113.194062\" xlink:href=\"#maea81207da\" y=\"204.697094\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"75.689079\" xlink:href=\"#maea81207da\" y=\"255.005754\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"258.717303\" xlink:href=\"#maea81207da\" y=\"83.656943\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"181.299976\" xlink:href=\"#maea81207da\" y=\"163.212976\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"237.954387\" xlink:href=\"#maea81207da\" y=\"99.088637\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"207.998802\" xlink:href=\"#maea81207da\" y=\"129.812995\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"242.365429\" xlink:href=\"#maea81207da\" y=\"100.77691\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"269.09895\" xlink:href=\"#maea81207da\" y=\"74.178005\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"64.698677\" xlink:href=\"#maea81207da\" y=\"269.041514\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"65.361257\" xlink:href=\"#maea81207da\" y=\"270.860628\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"178.3821\" xlink:href=\"#maea81207da\" y=\"170.150623\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"86.446247\" xlink:href=\"#maea81207da\" y=\"243.826559\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"166.800287\" xlink:href=\"#maea81207da\" y=\"166.189546\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"136.940331\" xlink:href=\"#maea81207da\" y=\"198.132008\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"194.356827\" xlink:href=\"#maea81207da\" y=\"147.568377\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"211.461694\" xlink:href=\"#maea81207da\" y=\"126.164928\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"176.399915\" xlink:href=\"#maea81207da\" y=\"155.500281\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"206.668789\" xlink:href=\"#maea81207da\" y=\"129.826191\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"124.993527\" xlink:href=\"#maea81207da\" y=\"213.484843\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"162.929877\" xlink:href=\"#maea81207da\" y=\"173.736289\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"143.169595\" xlink:href=\"#maea81207da\" y=\"187.395975\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"146.669239\" xlink:href=\"#maea81207da\" y=\"189.735805\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"154.13214\" xlink:href=\"#maea81207da\" y=\"189.73998\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"233.82989\" xlink:href=\"#maea81207da\" y=\"98.644065\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"229.476991\" xlink:href=\"#maea81207da\" y=\"111.698756\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"121.569231\" xlink:href=\"#maea81207da\" y=\"205.253332\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"109.31356\" xlink:href=\"#maea81207da\" y=\"237.805804\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"84.332923\" xlink:href=\"#maea81207da\" y=\"254.748791\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"181.613824\" xlink:href=\"#maea81207da\" y=\"167.669451\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"168.559498\" xlink:href=\"#maea81207da\" y=\"166.685754\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"125.840772\" xlink:href=\"#maea81207da\" y=\"218.065693\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"168.061506\" xlink:href=\"#maea81207da\" y=\"172.263336\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"106.477993\" xlink:href=\"#maea81207da\" y=\"235.80796\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"89.151358\" xlink:href=\"#maea81207da\" y=\"248.67678\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"250.324386\" xlink:href=\"#maea81207da\" y=\"87.819657\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"77.195775\" xlink:href=\"#maea81207da\" y=\"253.538025\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"150.996384\" xlink:href=\"#maea81207da\" y=\"182.162007\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"242.577957\" xlink:href=\"#maea81207da\" y=\"100.395608\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"116.881614\" xlink:href=\"#maea81207da\" y=\"219.509051\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"244.771912\" xlink:href=\"#maea81207da\" y=\"98.611605\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"191.307183\" xlink:href=\"#maea81207da\" y=\"151.754067\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"202.003494\" xlink:href=\"#maea81207da\" y=\"142.283112\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"138.797278\" xlink:href=\"#maea81207da\" y=\"203.80278\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"182.984413\" xlink:href=\"#maea81207da\" y=\"160.512522\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"107.839655\" xlink:href=\"#maea81207da\" y=\"217.559949\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"130.122125\" xlink:href=\"#maea81207da\" y=\"209.016622\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"159.341854\" xlink:href=\"#maea81207da\" y=\"176.460174\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"87.916063\" xlink:href=\"#maea81207da\" y=\"241.354312\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"100.785459\" xlink:href=\"#maea81207da\" y=\"233.107032\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"165.470774\" xlink:href=\"#maea81207da\" y=\"172.372049\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"84.690835\" xlink:href=\"#maea81207da\" y=\"253.150939\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"216.394364\" xlink:href=\"#maea81207da\" y=\"124.993835\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"207.309955\" xlink:href=\"#maea81207da\" y=\"125.690075\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"220.623341\" xlink:href=\"#maea81207da\" y=\"115.223346\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"316.859316\" xlink:href=\"#maea81207da\" y=\"22.578924\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"168.343901\" xlink:href=\"#maea81207da\" y=\"170.97012\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"135.119409\" xlink:href=\"#maea81207da\" y=\"199.550528\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"117.652809\" xlink:href=\"#maea81207da\" y=\"217.768001\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"201.508957\" xlink:href=\"#maea81207da\" y=\"140.447061\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"236.620716\" xlink:href=\"#maea81207da\" y=\"97.854892\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"247.476007\" xlink:href=\"#maea81207da\" y=\"91.619863\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"249.642191\" xlink:href=\"#maea81207da\" y=\"90.923092\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"170.569438\" xlink:href=\"#maea81207da\" y=\"158.744307\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"171.025062\" xlink:href=\"#maea81207da\" y=\"176.436013\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"118.858303\" xlink:href=\"#maea81207da\" y=\"208.328387\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"226.335702\" xlink:href=\"#maea81207da\" y=\"118.007601\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"185.80103\" xlink:href=\"#maea81207da\" y=\"153.463519\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"137.371176\" xlink:href=\"#maea81207da\" y=\"202.897957\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"214.129158\" xlink:href=\"#maea81207da\" y=\"111.779612\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"118.338217\" xlink:href=\"#maea81207da\" y=\"224.33207\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"70.495982\" xlink:href=\"#maea81207da\" y=\"256.07478\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"173.406646\" xlink:href=\"#maea81207da\" y=\"164.106038\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"125.15423\" xlink:href=\"#maea81207da\" y=\"215.109801\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"149.453292\" xlink:href=\"#maea81207da\" y=\"175.969696\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"218.102975\" xlink:href=\"#maea81207da\" y=\"114.497621\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"250.536007\" xlink:href=\"#maea81207da\" y=\"93.481099\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"215.420907\" xlink:href=\"#maea81207da\" y=\"135.778536\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"222.687994\" xlink:href=\"#maea81207da\" y=\"111.757991\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"104.487415\" xlink:href=\"#maea81207da\" y=\"224.762762\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"197.180006\" xlink:href=\"#maea81207da\" y=\"143.792296\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"238.714651\" xlink:href=\"#maea81207da\" y=\"96.405002\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"180.712343\" xlink:href=\"#maea81207da\" y=\"147.872712\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"204.13462\" xlink:href=\"#maea81207da\" y=\"132.46579\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"291.50426\" xlink:href=\"#maea81207da\" y=\"63.974933\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"103.879146\" xlink:href=\"#maea81207da\" y=\"234.392055\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"183.952624\" xlink:href=\"#maea81207da\" y=\"161.261606\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"106.834315\" xlink:href=\"#maea81207da\" y=\"230.459874\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"150.391434\" xlink:href=\"#maea81207da\" y=\"196.362614\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"153.580768\" xlink:href=\"#maea81207da\" y=\"172.386982\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"70.495982\" xlink:href=\"#maea81207da\" y=\"264.412426\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"196.944393\" xlink:href=\"#maea81207da\" y=\"137.917272\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"311.293024\" xlink:href=\"#maea81207da\" y=\"37.679715\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"123.019219\" xlink:href=\"#maea81207da\" y=\"217.78873\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"148.571911\" xlink:href=\"#maea81207da\" y=\"188.584373\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"95.2762\" xlink:href=\"#maea81207da\" y=\"255.625572\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"236.903859\" xlink:href=\"#maea81207da\" y=\"107.859139\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"241.658289\" xlink:href=\"#maea81207da\" y=\"97.049548\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"120.779758\" xlink:href=\"#maea81207da\" y=\"215.485138\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"237.261488\" xlink:href=\"#maea81207da\" y=\"102.761595\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"117.283183\" xlink:href=\"#maea81207da\" y=\"221.097441\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"118.052292\" xlink:href=\"#maea81207da\" y=\"224.892631\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"133.734979\" xlink:href=\"#maea81207da\" y=\"203.976612\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"199.797777\" xlink:href=\"#maea81207da\" y=\"149.886646\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"202.353579\" xlink:href=\"#maea81207da\" y=\"134.056536\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"98.025414\" xlink:href=\"#maea81207da\" y=\"240.324447\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"150.751935\" xlink:href=\"#maea81207da\" y=\"193.96478\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"149.795841\" xlink:href=\"#maea81207da\" y=\"186.235574\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"150.209029\" xlink:href=\"#maea81207da\" y=\"180.093389\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"101.635841\" xlink:href=\"#maea81207da\" y=\"229.75923\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"87.870459\" xlink:href=\"#maea81207da\" y=\"239.076847\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"131.132568\" xlink:href=\"#maea81207da\" y=\"204.159486\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"118.674696\" xlink:href=\"#maea81207da\" y=\"219.133191\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"169.44261\" xlink:href=\"#maea81207da\" y=\"158.219763\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"155.129597\" xlink:href=\"#maea81207da\" y=\"167.920265\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"198.169034\" xlink:href=\"#maea81207da\" y=\"152.899387\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"244.366465\" xlink:href=\"#maea81207da\" y=\"84.062251\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"89.083127\" xlink:href=\"#maea81207da\" y=\"247.422972\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"182.315416\" xlink:href=\"#maea81207da\" y=\"153.303016\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"317.177245\" xlink:href=\"#maea81207da\" y=\"38.473394\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"141.822371\" xlink:href=\"#maea81207da\" y=\"194.344248\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"144.75504\" xlink:href=\"#maea81207da\" y=\"186.355399\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"206.968396\" xlink:href=\"#maea81207da\" y=\"146.842298\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"149.440722\" xlink:href=\"#maea81207da\" y=\"192.426288\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"166.03102\" xlink:href=\"#maea81207da\" y=\"163.741585\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"239.360851\" xlink:href=\"#maea81207da\" y=\"109.262165\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"268.739688\" xlink:href=\"#maea81207da\" y=\"61.820291\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"201.539722\" xlink:href=\"#maea81207da\" y=\"132.698282\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"247.049017\" xlink:href=\"#maea81207da\" y=\"108.218291\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"212.886889\" xlink:href=\"#maea81207da\" y=\"131.91341\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"96.350319\" xlink:href=\"#maea81207da\" y=\"243.00338\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"204.176829\" xlink:href=\"#maea81207da\" y=\"138.06346\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"205.711145\" xlink:href=\"#maea81207da\" y=\"126.621753\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"172.721442\" xlink:href=\"#maea81207da\" y=\"157.295212\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"98.666859\" xlink:href=\"#maea81207da\" y=\"241.313111\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"243.829855\" xlink:href=\"#maea81207da\" y=\"91.451907\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"229.193909\" xlink:href=\"#maea81207da\" y=\"105.720344\"/>\n     <use style=\"fill:#ff0000;fill-opacity:0.5;stroke:#ff0000;stroke-opacity:0.5;\" x=\"247.913111\" xlink:href=\"#maea81207da\" y=\"100.553814\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mbe0729088c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.188352\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(39.007102 308.716563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"81.819034\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(78.637784 308.716563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.449716\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(115.087216 308.716563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.080398\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(154.717898 308.716563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.71108\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(194.34858 308.716563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.341761\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(233.979261 308.716563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"279.972443\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(273.609943 308.716563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"319.603125\" xlink:href=\"#mbe0729088c\" y=\"294.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 35 -->\n      <g transform=\"translate(313.240625 308.716563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- ground truth value -->\n     <g transform=\"translate(133.265625 322.394687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.339844\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"163.521484\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"226.900391\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"290.279297\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"353.755859\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"385.542969\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"424.751953\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"465.865234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"529.244141\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"568.453125\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"631.832031\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"663.619141\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"722.798828\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"784.078125\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"811.861328\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"875.240234\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5a7963bf10\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"292.573807\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 296.373026)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"253.965852\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 257.765071)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"215.357898\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 219.157116)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"176.749943\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 180.549162)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"138.141989\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 141.941207)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"99.534034\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 103.333253)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"60.92608\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 64.725298)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m5a7963bf10\" y=\"22.318125\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 26.117344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- predicted value -->\n     <g transform=\"translate(14.798438 197.182188)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"163.863281\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"227.339844\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"255.123047\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"310.103516\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"349.3125\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"410.835938\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"474.3125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"506.099609\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"565.279297\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"626.558594\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"654.341797\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"717.720703\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p353d5cb6a0)\" d=\"M 40.603125 294.118125 \nL 319.603125 22.318125 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 294.118125 \nL 40.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 319.603125 294.118125 \nL 319.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 294.118125 \nL 319.603125 294.118125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 22.318125 \nL 319.603125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_19\">\n    <!-- Ground Truth v.s. Prediction -->\n    <g transform=\"translate(97.283438 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-71\"/>\n     <use x=\"77.490234\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"116.353516\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"177.535156\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"240.914062\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"304.292969\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"367.769531\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"399.556641\" xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"445.890625\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"487.003906\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"550.382812\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"589.591797\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"652.970703\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"684.757812\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"736.1875\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"767.974609\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"820.074219\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"851.861328\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"883.648438\" xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"942.201172\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"981.064453\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1042.587891\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1106.064453\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1133.847656\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1188.828125\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1228.037109\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1255.820312\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1317.001953\" xlink:href=\"#DejaVuSans-110\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p353d5cb6a0\">\n   <rect height=\"271.8\" width=\"279\" x=\"40.603125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQikz3IPiyPf"
      },
      "source": [
        "# **Testing**\n",
        "The predictions of your model on testing set will be stored at `pred.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "err: 0.2558\nerr: 0.2368\nerr: 0.2414\nerr: 0.2320\nerr: 0.2179\nerr: 0.2336\nerr: 0.2330\n0.23579423768179758\n"
          ]
        }
      ],
      "source": [
        "ae_test(tt_set, ae, device)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8cTuQjQQOon",
        "outputId": "e78af8f9-7faa-4543-9dad-a07b4bff2677"
      },
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    print('Saving results to {}'.format(file))\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])\n",
        "\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "# preds = test(ae_tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to pred.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfrVxqJanGpE"
      },
      "source": [
        "# **Hints**\n",
        "\n",
        "## **Simple Baseline**\n",
        "* Run sample code\n",
        "\n",
        "## **Medium Baseline**\n",
        "* Feature selection: 40 states + 2 `tested_positive` (`TODO` in dataset)\n",
        "\n",
        "## **Strong Baseline**\n",
        "* Feature selection (what other features are useful?)\n",
        "* DNN architecture (layers? dimension? activation function?)\n",
        "* Training (mini-batch? optimizer? learning rate?)\n",
        "* L2 regularization\n",
        "* There are some mistakes in the sample code, can you find them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tmCwXgpot3t"
      },
      "source": [
        "# **Reference**\n",
        "This code is completely written by Heng-Jui Chang @ NTUEE.  \n",
        "Copying or reusing this code is required to specify the original author. \n",
        "\n",
        "E.g.  \n",
        "Source: Heng-Jui Chang @ NTUEE (https://github.com/ga642381/ML2021-Spring/blob/main/HW01/HW01.ipynb)\n"
      ]
    }
  ]
}