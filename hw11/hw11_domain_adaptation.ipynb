{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw11_domain_adaptation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python385jvsc74a57bd008f5944c08cf889f43480de463c4b71999d47c1cfd2d200a929d500fe2e3b01b",
      "display_name": "Python 3.8.5 64-bit ('torch': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cFq_TgWlQ_"
      },
      "source": [
        "# Homework 11 - Transfer Learning (Domain Adversarial Training)\n",
        "\n",
        "> Author: Arvin Liu (r09922071@ntu.edu.tw)\n",
        "\n",
        "若有任何問題，歡迎來信至助教信箱 ntu-ml-2021spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNiZCGrIYKdR"
      },
      "source": [
        "# Readme\n",
        "\n",
        "\n",
        "這份作業的任務是Transfer Learning中的Domain Adversarial Training。\n",
        "\n",
        "<img src=\"https://i.imgur.com/iMVIxCH.png\" width=\"500px\">\n",
        "\n",
        "> 也就是左下角的那一塊。\n",
        "\n",
        "## Scenario and Why Domain Adversarial Training\n",
        "你現在有Source Data + label，其中Source Data和Target Data可能有點關係，所以你想要訓練一個model做在Source Data上並Predict在Target Data上。\n",
        "\n",
        "但這樣有什麼樣的問題? 相信大家學過Anomaly Detection就會知道，如果有data是在Source Data沒有出現過的(或稱Abnormal的)，那麼model大部分都會因為不熟悉這個data而可能亂做一發。 \n",
        "\n",
        "以下我們將model拆成Feature Extractor(上半部)和Classifier(下半部)來作例子:\n",
        "<img src=\"https://i.imgur.com/IL0PxCY.png\" width=\"500px\">\n",
        "\n",
        "整個Model在學習Source Data的時候，Feature Extrator因為看過很多次Source Data，所以所抽取出來的Feature可能就頗具意義，例如像圖上的藍色Distribution，已經將圖片分成各個Cluster，所以這個時候Classifier就可以依照這個Cluster去預測結果。\n",
        "\n",
        "但是在做Target Data的時候，Feature Extractor會沒看過這樣的Data，導致輸出的Target Feature可能不屬於在Source Feature Distribution上，這樣的Feature給Classifier預測結果顯然就不會做得好。\n",
        "\n",
        "## Domain Adversarial Training of Nerural Networks (DaNN)\n",
        "基於如此，是不是只要讓Soucre Data和Target Data經過Feature Extractor都在同個Distribution上，就會做得好了呢? 這就是DaNN的主要核心。\n",
        "\n",
        "<img src=\"https://i.imgur.com/vrOE5a6.png\" width=\"500px\">\n",
        "\n",
        "我們追加一個Domain Classifier，在學習的過程中，讓Domain Classifier去判斷經過Feature Extractor後的Feature是源自於哪個domain，讓Feature Extractor學習如何產生Feature以**騙過**Domain Classifier。 持久下來，通常Feature Extractor都會打贏Domain Classifier。(因為Domain Classifier的Input來自於Feature Extractor，而且對Feature Extractor來說Domain&Classification的任務並沒有衝突。)\n",
        "\n",
        "如此一來，我們就可以確信不管是哪一個Domain，Feature Extractor都會把它產生在同一個Feature Distribution上。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-qnUkspmap3"
      },
      "source": [
        "# Data Introduce\n",
        "\n",
        "這次的任務是Source Data: 真實照片，Target Data: 手畫塗鴉。\n",
        "\n",
        "我們必須讓model看過真實照片以及標籤，嘗試去預測手畫塗鴉的標籤為何。\n",
        "\n",
        "資料位於[這裡](https://drive.google.com/open?id=12-07DSquGdzN3JBHBChN4nMo3i8BqTiL)，以下的code分別為下載和觀看這次的資料大概長甚麼樣子。\n",
        "\n",
        "特別注意一點: **這次的source和target data的圖片都是平衡的，你們可以使用這個資訊做其他事情。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "0_uO-ZSDoR6i",
        "outputId": "546b0695-ef92-4317-b9d8-3786946b5136"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def no_axis_show(img, title='', cmap=None):\n",
        "  # imshow, and set the interpolation mode to be \"nearest\"。\n",
        "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
        "  # do not show the axes in the images.\n",
        "  fig.axes.get_xaxis().set_visible(False)\n",
        "  fig.axes.get_yaxis().set_visible(False)\n",
        "  plt.title(title)\n",
        "\n",
        "# titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "# plt.figure(figsize=(18, 18))\n",
        "# for i in range(10):\n",
        "#   plt.subplot(1, 10, i+1)\n",
        "#   fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "# plt.figure(figsize=(18, 18))\n",
        "# for i in range(10):\n",
        "#   plt.subplot(1, 10, i+1)\n",
        "#   fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "3eMs7DbVt4Ee",
        "outputId": "6b400470-daec-4cd8-986b-041ae6ddf459"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moXQw9To5TqZ"
      },
      "source": [
        "# Special Domain Knowledge\n",
        "\n",
        "因為大家塗鴉的時候通常只會畫輪廓，我們可以根據這點將source data做點邊緣偵測處理，讓source data更像target data一點。\n",
        "\n",
        "## Canny Edge Detection\n",
        "算法這邊不贅述，只教大家怎麼用。若有興趣歡迎參考wiki或[這裡](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19)。\n",
        "\n",
        "cv2.Canny使用非常方便，只需要兩個參數: low_threshold, high_threshold。\n",
        "\n",
        "```cv2.Canny(image, low_threshold, high_threshold)```\n",
        "\n",
        "簡單來說就是當邊緣值超過high_threshold，我們就確定它是edge。如果只有超過low_threshold，那就先判斷一下再決定是不是edge。\n",
        "\n",
        "以下我們直接拿source data做做看。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mn2MkDLV7E2-",
        "outputId": "8700d78d-77cf-499c-b58e-e4f9f500bd38"
      },
      "source": [
        "import cv2\n",
        "# import matplotlib.pyplot as plt\n",
        "# titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "# plt.figure(figsize=(18, 18))\n",
        "\n",
        "# original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
        "# plt.subplot(1, 5, 1)\n",
        "# no_axis_show(original_img, title='original')\n",
        "\n",
        "# gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "# plt.subplot(1, 5, 2)\n",
        "# no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "# gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "# plt.subplot(1, 5, 2)\n",
        "# no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "# canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
        "# plt.subplot(1, 5, 3)\n",
        "# no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
        "\n",
        "# canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
        "# plt.subplot(1, 5, 4)\n",
        "# no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
        "\n",
        "# canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
        "# plt.subplot(1, 5, 5)\n",
        "# no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8THSdt_hmwYh"
      },
      "source": [
        "# Data Process\n",
        "\n",
        "在這裡我故意將data用成可以使用torchvision.ImageFolder的形式，所以只要使用該函式便可以做出一個datasets。\n",
        "\n",
        "transform的部分請參考以下註解。\n",
        "<!-- \n",
        "#### 一些細節\n",
        "\n",
        "在一般的版本上，對灰階圖片使用RandomRotation使用```transforms.RandomRotation(15)```即可。但在colab上需要加上```fill=(0,)```才可運行。\n",
        "在n98上執行需要把```fill=(0,)```拿掉才可運行。 -->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZHIBGknmi8Z"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "source_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n",
        "    transforms.Grayscale(),\n",
        "    # cv2 do not support skimage.Image, so we transform it to np.array, \n",
        "    # and then adopt cv2.Canny algorithm.\n",
        "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), np.random.randint(170, 200), np.random.randint(250, 300))),\n",
        "    # Transform np.array back to the skimage.Image.\n",
        "    transforms.ToPILImage(),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(30, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.03, 0.07), ratio=(0.3, 3), value=1-1e-6),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.03, 0.07), ratio=(0.3, 3), value=1e-6),\n",
        "])\n",
        "target_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale.\n",
        "    transforms.Grayscale(),\n",
        "    # Resize: size of source data is 32x32, thus we need to \n",
        "    #  enlarge the size of target data from 28x28 to 32x32。\n",
        "    transforms.Resize((32, 32)),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(30, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.03, 0.07), ratio=(0.3, 3), value=1-1e-6),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.03, 0.07), ratio=(0.3, 3), value=1e-6),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale.\n",
        "    transforms.Grayscale(),\n",
        "    # Resize: size of source data is 32x32, thus we need to \n",
        "    #  enlarge the size of target data from 28x28 to 32x32。\n",
        "    transforms.Resize((32, 32)),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    # transforms.RandomRotation(15, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
        "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
        "\n",
        "batch_size = 100\n",
        "source_dataloader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "target_dataloader = DataLoader(target_dataset, batch_size=batch_size*4, shuffle=True, num_workers=4, pin_memory=True)\n",
        "target_dataloader_nshuf = DataLoader(\n",
        "    target_dataset, batch_size=100, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_dataloader = DataLoader(target_dataset, batch_size=1000, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_class = len(source_dataset.class_to_idx.values())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdwDEMrOycs5"
      },
      "source": [
        "# Model\n",
        "\n",
        "Feature Extractor: 典型的VGG-like疊法。\n",
        "\n",
        "Label Predictor / Domain Classifier: MLP到尾。\n",
        "\n",
        "相信作業寫到這邊大家對以下的Layer都很熟悉，因此不再贅述。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uw2eP09z-pD"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x).squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 5, stride=2, padding=2, output_padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # nn.AvgPool2d(2),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 256, 5, stride=2, padding=2, output_padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 5, stride=2, padding=2, output_padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 5, stride=2, padding=2, output_padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 1, 5, stride=2, padding=2, output_padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv(x.view(-1, 512, 1, 1))\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DomainAdapt(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.F = FeatureExtractor()\n",
        "        self.C = LabelPredictor()\n",
        "        self.D = DomainClassifier()\n",
        "        self.AE = Decoder()\n",
        "    \n",
        "    def setOptim(self, F, C, D, AE):\n",
        "        self.optim_F = F\n",
        "        self.optim_C = C\n",
        "        self.optim_D = D\n",
        "        self.optim_AE = AE\n",
        "    \n",
        "    def setLR(self, F, C, D, AE):\n",
        "        self.lr_F = F\n",
        "        self.lr_C = C\n",
        "        self.lr_D = D\n",
        "        self.lr_AE = AE\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.C(self.F(x))\n",
        "\n",
        "    def recon(self, x):\n",
        "        return self.AE(self.F(x))\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optim_F.zero_grad()\n",
        "        self.optim_C.zero_grad()\n",
        "        self.optim_D.zero_grad()\n",
        "        self.optim_AE.zero_grad()\n",
        "\n",
        "    def stepFC(self):\n",
        "        self.optim_F.step()\n",
        "        self.optim_C.step()\n",
        "\n",
        "    def stepF(self):\n",
        "        self.optim_F.step()\n",
        "    \n",
        "    def stepC(self):\n",
        "        self.optim_C.step()\n",
        "    \n",
        "    def stepD(self):\n",
        "        self.optim_D.step()\n",
        "\n",
        "    def stepAE(self):\n",
        "        self.optim_AE.step()\n",
        "\n",
        "    def stepLR(self):\n",
        "        self.lr_F.step()\n",
        "        self.lr_C.step()\n",
        "        self.lr_D.step()\n",
        "        self.lr_AE.step()\n",
        "\n",
        "    def load(self, isD=True):\n",
        "        self.F.load_state_dict(torch.load('extractor_model.bin'))\n",
        "        self.C.load_state_dict(torch.load('predictor_model.bin'))\n",
        "        self.AE.load_state_dict(torch.load('ae_model.bin'))\n",
        "        if isD: self.D.load_state_dict(torch.load('discriminator_model.bin'))\n",
        "        return self\n",
        "    \n",
        "    def save(self, isD=True):\n",
        "        torch.save(self.F.state_dict(), f'extractor_model.bin')\n",
        "        torch.save(self.C.state_dict(), f'predictor_model.bin')\n",
        "        torch.save(self.AE.state_dict(), f'ae_model.bin')\n",
        "        if isD: torch.save(self.D.state_dict(), f'discriminator_model.bin')\n",
        "        \n",
        "    # def cuda(self):\n",
        "    #     self.F.cuda()\n",
        "    #     self.C.cuda()\n",
        "    #     self.D.cuda()\n",
        "    #     return self\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxdBIPhF0Icb"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "這裡我們選用Adam來當Optimizer。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epoch = 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrxKelBy0PJ7"
      },
      "source": [
        "model = DomainAdapt().load().cuda()\n",
        "# model = DomainAdapt().cuda()\n",
        "\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "class_sep_crit = nn.CrossEntropyLoss(reduction='none')\n",
        "# ae_crit = nn.L1Loss()\n",
        "ae_crit = nn.MSELoss()\n",
        "bal_crit = nn.MSELoss()\n",
        "uniform_dist = torch.full((n_class, ), 1./n_class).float().cuda()\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "optimizer_F = optim.AdamW(model.F.parameters(), lr=lr, weight_decay=1e-4)\n",
        "lr_F = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_F, T_max=n_epoch, eta_min=1e-6, verbose=False)\n",
        "\n",
        "optimizer_C = optim.AdamW(model.C.parameters(), lr=lr, weight_decay=1e-3)\n",
        "lr_C = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_C, T_max=n_epoch, eta_min=1e-6, verbose=False)\n",
        "\n",
        "optimizer_D = optim.AdamW(model.D.parameters(), lr=lr, weight_decay=1e-3)\n",
        "lr_D = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_D, T_max=n_epoch, eta_min=1e-6, verbose=False)\n",
        "\n",
        "optimizer_AE = optim.AdamW(model.AE.parameters(), lr=lr, weight_decay=1e-4)\n",
        "lr_AE = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_AE, T_max=n_epoch, eta_min=1e-6, verbose=False)\n",
        "\n",
        "model.setOptim(optimizer_F, optimizer_C, optimizer_D, optimizer_AE)\n",
        "model.setLR(lr_F, lr_C, lr_D, lr_AE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuAE4cqJ0itR"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "\n",
        "## 如何實作DaNN?\n",
        "\n",
        "理論上，在原始paper中是加上Gradient Reversal Layer，並將Feature Extractor / Label Predictor / Domain Classifier 一起train，但其實我們也可以交換的train Domain Classfier & Feature Extractor(就像在train GAN的Generator & Discriminator一樣)，這也是可行的。\n",
        "\n",
        "在code實現中，我們採取後者的方式，畢竟GAN是之前的作業，應該會比較熟悉:)。\n",
        "\n",
        "## 小提醒\n",
        "* 原文中的lambda(控制Domain Adversarial Loss的係數)是有Adaptive的版本，如果有興趣可以參考[原文](https://arxiv.org/pdf/1505.07818.pdf)。\n",
        "* 因為我們完全沒有target的label，所以結果如何，只好丟kaggle看看囉:)?"
      ]
    },
    {
      "source": [
        "## Semi-supervised"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SubsetCustomLabel():\n",
        "    def __init__(self, dataset, labels, indices):\n",
        "        # super().__init__(dataset, indices)\n",
        "        self.dataset = dataset\n",
        "        self.labels = labels\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[self.indices[idx]][0], self.labels[idx]\n",
        "\n",
        "    def set_transform(self, transform):\n",
        "        self.dataset.transform = transform\n",
        "\n",
        "\n",
        "# def SubsetCustomLabel(dataset, labels, indices):\n",
        "#     subset = Subset(dataset, indices)\n",
        "#     subset.labels = labels\n",
        "\n",
        "#     subset.__len__ = __len__\n",
        "#     subset.__getitem__ = __getitem__\n",
        "#     subset.set_transform = set_transform\n",
        "#     return subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pseudo_labels(model, dataloader, threshold=0.5, prob_thre=0.3):\n",
        "    model.eval()\n",
        "    # Define softmax function.\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    idx = []\n",
        "    targets = []\n",
        "    count = torch.zeros(n_class, dtype=torch.float32)\n",
        "\n",
        "    # Iterate over the dataset by batches.\n",
        "    c = 0\n",
        "    for i, (img, ans) in enumerate(tqdm(dataloader, desc='PseudoLabels', leave=False)):\n",
        "        with torch.no_grad():\n",
        "            logits = model(img.cuda())\n",
        "\n",
        "        # Obtain the probability distributions by applying softmax on logits.\n",
        "        probs = softmax(logits)\n",
        "        # Filter the data and construct a new dataset.\n",
        "        # probs1 = probs.max(dim=1).values\n",
        "        top2 = probs.topk(2, dim=1).values\n",
        "        ratio = top2[:, 1] / top2[:, 0]\n",
        "        \n",
        "        select = (ratio < threshold) & (top2[:, 0] >= prob_thre)\n",
        "        c = probs[select].sum(dim=0).cpu()\n",
        "        count += c\n",
        "        if not (select.any()):\n",
        "            continue\n",
        "        probs_arg = probs[select].argmax(dim=1)\n",
        "        targets.append(probs_arg)\n",
        "        idx += (torch.where(select)[0] + batch_size*i).tolist()\n",
        "\n",
        "    # custom subset\n",
        "    if len(targets) == 0:\n",
        "        return None, None\n",
        "    targets = torch.cat(targets, dim=0).cpu().tolist()\n",
        "    new = SubsetCustomLabel(dataloader.dataset, targets, idx)\n",
        "    model.train()\n",
        "    return new, count\n",
        "\n",
        "def get_semi_set(model, threshold=0.5, prob_thre=0.):\n",
        "    target_dataset.transform = test_transform\n",
        "    # source_dataset.transform = tfm_weak\n",
        "    \n",
        "    pseudo_dataset, count = get_pseudo_labels(\n",
        "        model, target_dataloader_nshuf, threshold=threshold, prob_thre=prob_thre)\n",
        "    if pseudo_dataset is None:\n",
        "        return [], None\n",
        "    # pseudo_loader = DataLoader(pseudo_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "   \n",
        "    # target_dataset.transform = tfm_strong\n",
        "    # train_set.transform = tfm_strong\n",
        "    target_dataset.transform = target_transform\n",
        "    return pseudo_dataset, count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_cls_epoch(dataloader, weight):\n",
        "    '''\n",
        "      Args:\n",
        "        source_dataloader: source data的dataloader\n",
        "        target_dataloader: target data的dataloader\n",
        "        lamb: control the balance of domain adaptatoin and classification.\n",
        "    '''\n",
        "\n",
        "    # D loss: Domain Classifier的loss\n",
        "    # F loss: Feature Extrator & Label Predictor的loss\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "    class_count = torch.zeros((n_class)).cuda()\n",
        "\n",
        "    for i, (data, ans) in enumerate((dataloader)):\n",
        "        ans = ans.cuda()\n",
        "        class_logits = model(data.cuda())\n",
        "        predict_label = torch.argmax(class_logits, dim=1)\n",
        "        \n",
        "        class_count += predict_label.bincount(minlength=n_class).float()\n",
        "        loss_balance = bal_crit(F.softmax(class_logits*10, 1).mean(0), uniform_dist)\n",
        "\n",
        "\n",
        "        loss_cls = class_sep_crit(class_logits, ans)\n",
        "        loss_cls = (weight[ans]*loss_cls).mean()\n",
        "\n",
        "\n",
        "        loss = loss_cls + loss_balance*3\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        model.stepFC()\n",
        "        model.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(predict_label == ans).item()\n",
        "        total_num += data.shape[0]\n",
        "\n",
        "        # print(i, end='\\r')\n",
        "    class_count = class_count.cpu()*n_class / total_num\n",
        "    return running_loss / (i+1), total_hit / total_num, class_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uniform_noise(x, p=0.025):\n",
        "    out = x.clone()\n",
        "    noise = torch.rand_like(out)\n",
        "    out[noise <= p] = 1e-6\n",
        "    out[noise >= (1-p)] = 1-1e-6\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for e in range(3):\n",
        "#     l = 0.0\n",
        "#     for i, (target_data, _) in enumerate(target_dataloader):\n",
        "#         target_data = target_data.cuda()\n",
        "#         out = model.AE(model.F(uniform_noise(target_data)))\n",
        "#         loss = ae_crit(out, target_data)\n",
        "#         loss.backward()\n",
        "#         model.stepF()\n",
        "#         model.stepAE()\n",
        "#         model.zero_grad()\n",
        "#         l += loss.item()\n",
        "#     print(e, l/(i+1))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRAFFKvX0p9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57fdb3d3-88f1-4cc1-94e5-fd6c40377263",
        "tags": []
      },
      "source": [
        "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
        "    '''\n",
        "      Args:\n",
        "        source_dataloader: source data的dataloader\n",
        "        target_dataloader: target data的dataloader\n",
        "        lamb: control the balance of domain adaptatoin and classification.\n",
        "    '''\n",
        "\n",
        "    # D loss: Domain Classifier的loss\n",
        "    # F loss: Feature Extrator & Label Predictor的loss\n",
        "\n",
        "    running_D_loss, running_F_loss = 0.0, 0.0\n",
        "    running_AE_loss = 0.0\n",
        "    # running_b_loss = 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "    # confusion = torch.zeros((n_class, n_class), dtype=float)\n",
        "    class_count = torch.zeros((n_class)).cuda()\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "        target_data = target_data.cuda()\n",
        "        \n",
        "        # Mixed the source data and target data, or it'll mislead the running params\n",
        "        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n",
        "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
        "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
        "        # set domain label of source data to be 1.\n",
        "        domain_label[:source_data.shape[0]] = 1\n",
        "\n",
        "        # ======================= Step 1 : train domain classifier\n",
        "        feature = model.F(mixed_data)\n",
        "        # feature = model.F(uniform_noise(mixed_data, 0.02))\n",
        "        # We don't need to train feature extractor in step 1.\n",
        "        # Thus we detach the feature neuron to avoid backpropgation.\n",
        "\n",
        "        domain_logits = model.D(feature.detach())\n",
        "        loss = domain_criterion(domain_logits, domain_label)\n",
        "        running_D_loss += loss.item()\n",
        "        loss.backward()\n",
        "        model.stepD()\n",
        "\n",
        "        # ======================= Step 2 : train feature extractor and label classifier\n",
        "        class_logits = model.C(feature[:source_data.shape[0]])\n",
        "        domain_logits = model.D(feature)\n",
        "        predict_label = torch.argmax(class_logits, dim=1)\n",
        "        # domain_label = torch.full([source_data.shape[0] + target_data.shape[0], 1], 0.5).cuda()\n",
        "\n",
        "        loss_ae = ae_crit(model.recon(uniform_noise(mixed_data, 0.02)), mixed_data)\n",
        "        # loss_ae = ae_crit(model.AE(feature), mixed_data)\n",
        "        running_AE_loss += loss_ae.item()\n",
        "\n",
        "        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n",
        "\n",
        "        class_count += predict_label.bincount(minlength=n_class).float()\n",
        "        loss_balance = bal_crit(F.softmax(class_logits*10, 1).mean(0), uniform_dist)\n",
        "\n",
        "        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n",
        "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label) + loss_balance*3 + loss_ae*3\n",
        "        running_F_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        model.stepFC()\n",
        "        model.stepAE()\n",
        "        model.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(predict_label == source_label).item()\n",
        "        total_num += source_data.shape[0]\n",
        "\n",
        "        # print(i, end='\\r')\n",
        "    class_count = class_count.cpu()*n_class / total_num\n",
        "    return running_D_loss / (i+1), running_F_loss / (i+1),  running_AE_loss / (i+1),total_hit / total_num, class_count\n",
        "\n",
        "\n",
        "\n",
        "# train 200 epochs\n",
        "weight = torch.ones(n_class).cuda()\n",
        "# class_criterion = crosss\n",
        "train_acc = 0.0\n",
        "semi_flg = False\n",
        "for epoch in (range(n_epoch)):\n",
        "    # You should chooose lamnda cleverly.\n",
        "    # if True:\n",
        "    if (train_acc >= 0.85 or semi_flg):\n",
        "        pseudo_dataset, count = get_semi_set(model, threshold=0.5, prob_thre=0.3)\n",
        "        print(f\"Get pseudo label: {len(pseudo_dataset)}\")\n",
        "        print(\", \".join([f\"{int(c):4d}\" for c in count.tolist()]))\n",
        "        weight = (count.mean()/count).cuda()\n",
        "    else:\n",
        "        pseudo_dataset = None\n",
        "    \n",
        "    if pseudo_dataset:\n",
        "        semi_flg = True\n",
        "        # choice = np.random.choice(len(pseudo_dataset), len(source_dataset)*4, replace=False).tolist()\n",
        "        # dataloader = DataLoader(ConcatDataset([Subset(pseudo_dataset, choice), source_dataset]),\n",
        "        # dataloader = DataLoader(ConcatDataset([pseudo_dataset, source_dataset]),\n",
        "        dataloader = DataLoader(pseudo_dataset,\n",
        "            batch_size=batch_size*5, shuffle=True, num_workers=4, pin_memory=True)\n",
        "        train_loss, train_acc, class_count = train_cls_epoch(dataloader, weight)\n",
        "        print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_loss, train_acc))\n",
        "    \n",
        "    else:\n",
        "        train_D_loss, train_F_loss, train_AE_loss, train_acc, class_count = train_epoch(\n",
        "            source_dataloader, target_dataloader, lamb=1.)\n",
        "        # weight = 1+(1-class_count)**2\n",
        "        # class_criterion = nn.CrossEntropyLoss(weight=weight.cuda())\n",
        "\n",
        "        print('epoch {:>3d}: D loss: {:6.4f}, F loss: {:6.4f}, AE loss: {:6.4f} acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_AE_loss, train_acc))\n",
        "    \n",
        "    model.save()\n",
        "    model.stepLR()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA2UlEQVR4nO1VSRLEIAjUVP7/5Z6DFcIgSyRymKrhlChLQwO29uvSi/wCGB9Hnffeq9Df8FtFBtx7ifAAAA71gk7ewzmt4O1iiR8meFMMAFiO1IRImW4JmfQzNFYrAyY+FMnPUgz1F8Cp6S8LL8WoTJDBm+YRtpsHbSZWDzA3Rlq8DLYMvUmyxYczayogPYCzb614j1Z0ootoviwThYOl0i9sJx/IBsPccg693yQXPaFBm5pbxdAXooPmtXri3deREfgyEd+6/fcenW+DQROPVAg/JjLdrH9Jywf0Ntek7VkD0QAAAABJRU5ErkJggg==\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,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\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABC0lEQVR4nLVV2Q7EIAjEpv//y+yDyZRyq10eGoPMcEiBqCHMjK88WKWy7ApgEHtb+JBGKsAyHDe/B2UTB2lCLW2SAhLRrWBjDCIaYwCGs5RpNvXzrBhwCENTFViSFqRTfde4U+Euu2oqF3JFYL+ClZn1oR3MiJrsnZgeB0h2j13yOA5Avcqu6MLWXCItgVJ/bbNTWkwUPOyiEzl8xa6cZtCs7UIuLmP+DH3ycHjIaWE3HTPfFrMkyOCkG7t4lcor6RxfDld3oD5dVI9v7zGjKa1meKp9h29vl5aSD4g0OXVREgXeaA/pwPmTP5wkzPyvYYe1+NpotPFuladbskP7CbVWlV3xjUS/wjbhDzttn+AqTRSTAAAAAElFTkSuQmCC\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,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\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAAuklEQVR4nO1Vyw6DMAzDFf//y92hWoicNKQjEofVBxSocR5u4Tg2/gi9d7nq4CFACQBQcFGBQRtBPaTFquYuUfd5QZpAggwLcFqtMX0atBD0qjZslqDpG+GRw6Ri/Y8QTJNy2JQ2tjilrlmztGtduezetWXOzh0tBU04HuhKM3UBWD56tx7mPWhu76sVRTnsWv8iKX3PfO17UEJoSYn4MK558HMOn5OfcsaSpQ0SqZRw/Ncqf2cbGzl8AJdxB15/l0WIAAAAAElFTkSuQmCC\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAFSUlEQVR4nO1WzU8TWxSfe+eztExKWmigwmhLqaFoLEEMkWiiboxuNMaYuHKhCxMXbv0b9A8wujFxozGy0gUkRjcsJjJOiApCkaktbfphOzBfnc/7FtfHIz54vveieZt3FpNZnHt+93fO75xzCeIXGADgV4T9j2xPLhBC/IMQ2v7+BAAAAACApmmO40iSpGla13XHcTzP+3cYfwAAACCEAIDu7u5MJjM8PJzJZDY2NkRRVBSl0+n4vo8QCoLgHwFQ29EZhqFpuqenJ5lMnjx5Mp/PDw0NybJsGIZhGM1m0/O8IAi2YYIg+DucvjFgGCYejw8PDx84cMD3fY7jTNOkKMo0Tc/zCoVCuVxGCAEAWJZlWdbzvFar5bqu7/s/YAAAIEkyHo+fOHFiYmJifX19bm6u0WhYlkWSJMuy0Wi00+kQBIEQYllWEISpqSmO4yRJKhaL6+vrf10eCgDAcdyZM2euXr26tbU1OztbqVRM00QIQQh938e1oWna8zyCIEKhkCAI2Wz22rVrpVLp7t27kiR9/fp1r9pAAADP8xMTE4cPH97Y2FAUxXEcXBWKosLhcCqVyufz6XRaEIS+vj6KokRRnJmZmZ+fT6VS9+/fv379eiQS2Zb17gwoiqrVapVKJQgClmVJkuzq6hodHT116tT09DTDMNVq9dOnT6urq5FIZGVlRRTF2dnZ8+fP37lz59atW5IkvXnzxrKsXQAQQp1OZ2tra2lpybKsRCIRjUZ7enry+fzZs2fHxsZs2y6VSr7v0zSdzWaTyaSqqp8/f1ZV9cWLF9ls9saNGxcvXlxYWLBtGyH0XT0ohNDm5ubz588FQVBVlaKo8fHx6enpY8eO+b6/srIiy/LTp081TQuFQtlsNp1Oj4yMFItF0zQhhI1Gg+O4vr6+RCKhaZpt27swcBynUCg0Gg2EUCwWYxgmkUiQJPnhw4d79+5JkqRpGkKIYRhd1/fv39/b2zs5Oanren9//+DgYL1eVxSl3W7vKieKIAjP8wzDCIKA47h2u10qldrtdrlcfvnypSRJm5ub+Jht26urq0+ePBkaGqrVarZtcxwXi8UURcGXwDLbBQC3Au6GgYGB3t5eXddx6nFasSvmWq1WTdPUNI0kyWq1uri42Gg0ZFnG2tsdAEIYiURGRkZGR0ej0Wiz2azX6yRJhsPh71aH7/uGYZimifvDtm2e5/v7+/Gk2lOm4XD40KFDFy5cUFVVluXl5eX3799ns1kAQDQadV3Xdd3tAzjReGw4jmMYBsMwJEnu1WgUy7KDg4Pnzp1Lp9MPHz58/fq1YRgEQSiKsm/fvlwut7a2VqlUbNvGk3znQPV93/f9ycnJVqulKMrufcDz/Pj4OITw8ePHr1690jQNk202myRJ5nK5TCYjiuLa2hqeoxh+u3IDAwMHDx5kGGZ5eblQKGCfnQBAEIRcLtdoNHDv7EwlRVGpVOrmzZs8zzuO8/btW1EUC4VCp9PBUXieHxsbu337di6Xm5ube/ToUbFY1DTNdd0gCDBdEIvFWJZttVqO4/w5j7jvLl26dPr06aWlpQcPHiwuLuq67rouXiHd3d25XO7KlStHjx6FECqK8uzZs/n5+VqthgNSlmWpqrrX9vA87927d81mU5bler3+5csXvD6J31XbbrdFUSyXy8ePH798+fKRI0d4ng+HwzMzM1gaAEL4wy0IIQyFQnidua678ypYx3h7J5PJqampeDy+sLDw8eNHnPBvwvhrABzoh254Bnd1dbmua1mWbdtBEPzkJxiEEEKIdtjPjf+//WMDAMBf99TGz5HfAITkOFkWpbB6AAAAAElFTkSuQmCC\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA4ElEQVR4nO2VSxLEIAhExZr7X5ksTFmEr6CTVVhN0Dw6LTitffFuICLL9BdqnIEO7jE6Bf2XfpLr4JwavywXAFKCzAJSFAA4ShExUdvywS+g5pUvmFrYO1lzuDQpMNse+v6CG/UCNa4EUkhnNVT319GltTU0FW46YZ12SFcfxw9g2dmL9zLwDe05cdNS2sTjMR49ef5SrHWPPvIr82ntsWrMZG/2iIZ05914dX2wfZe8kF+gNpjvUkwP1RWuWK5aTVrOJEIirIOpX2K+6bJSBb3J8v70ZRez+2MrqPydk7wAFGd15vK3md4AAAAASUVORK5CYII=\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAF80lEQVR4nJ1WzW8SWxS/985loGVggCHANK2VNtiEujC6aNwQk5p0YWJiYqJxYbQrly79S1y5cutGu7CJSVsNjaJNsRFplUItpSO1fA0DzAfz9RbXh0jxvb53FhMynDm/8/k7B0IIbdsmTwAAQsi2bfL7NAIh7D0BAJZlDSggYrSnbVnW6a0DAOy/BSEEIewh/QKgKIpYPPnfKQVC6HA4GIbx+Xw9X3uCf0EhdDLAU4rH45mdnW2327Isq6r6Ww56cf2/CBBCTqfz8uXLL1++fPz48dTU1EAQGABAAP9T6ntCUZTf779+/frc3NxQLwdT9j8AfD5fPB7HGH/58qVerw/kGfca9DTS3ycIIYwxy7Iej6fT6ezv7yeTSVVVBz7B/2AdQkhRFOk/t9s9MjKCEKIoipTRsizTNBmGURRla2trd3d3e3vbMIxBgD+Zxhj7/f5YLDYxMcFx3J07d5xOZ7Vaff/+fSqV2tvbq9frhmFIkqQoSi6Xs21bluWTfTgEACFE0/TZs2cfPHiQSCRmZmZomsYYq6qayWSOj48LhcLBwYGu6wAARVFomhYEYWRkRFXVIQAURZmm2T9rFEWFQqH79+9fu3YtHA63Wi1RFPP5/KtXr7LZ7ObmpiRJPUOmaQIARFEMBAIEchAA45+d2vsGQqjreqFQWF1dPTw8JF5Xq9VKpdJutw3DGCibZVkIIZZlvV4vUfgt2xzHmaZpWZYsy6ZpQghHR0eDwSDP881mUxAETdNIiISmTjYFRVHnzp27evWqKIrLy8u1Wq1fB0cikVAo1Gw29/b22u22ZVmWZamqWigUJEnSNO1fm5j4FA6HvV7v9PS0qqqdTqf3FUYIxeNxnufX1tZ2dnYajQbGWJZlTdM0TRswBIYNPOm3QCBgGMbCwoLb7d7Z2Tk+PiblwYIg/PjxY2FhwefzhcPhdDqtaVqlUhmoGISQTJau6/2tQlEUy7IsyxYKBUVRpqenHz58uLGx8fTp03K5rOs6kiTp7du3KysrTqdzZmbmxo0bFy5c4DiOoqgBN8fGxi5duhQMBjHGZPoIEV28eHFsbOzDhw+rq6svXrxgWfbWrVu3b98eHx93OBzYNM1KpbK0tJTP54PBYDQaPX/+vKIorVar2WySMBFC4+Pji4uL0Wj048ePr1+/brfbrVaLpulYLHbv3j3btpPJpCAIR0dHT548efTo0d27dwVBWFpawgAAwzAqlcru7m6xWPz69SsZ3UQikUqlJEkyDIPn+cXFxStXrng8HlmWAQCNRsMwDJ/PF4vFbt68mcvlejVbX19fX1+fm5vjeX50dPQnF8myXCqVMMaED+LxeDQaZVlWURRFUebn5+fn591u98bGxsrKSr1eN01TlmWXyxWJRDqdTqvV0jSN1KbRaKytreXz+Ww2C3pUQVoTQqgoiiRJzWaz0+kkEonZ2VmXyxWLxb5///78+fPl5eXt7W1CmRBChmEmJydFUazX691ulxCzYRifPn3a3NxsNputVusXF5EhsiwLQliv1z9//hwKhaampmiaTqVSb968SSaTpVKp2+0STyGEpmlmMplSqWTbttPpRAgZhtHtdg8ODgAApN+GkJ1t26ZptlqtcrksCMLW1lY6nc7lctVqtdvt9uaAqAEAvF4vx3E8zx8eHtq2res6oQaiNnyjQQhpmp6cnIQQFovFb9++1Wo1XddPTpnD4aAoyul0MgxDehcAQKjl56E1FIBMkNfrNU2z2WwO5Tiiw/O8y+VSVdUwDEIz/Wq2bf9x4bhcromJiUAgoCgKScVJAIZhzpw5s7+/n81mi8Vij/b7ZTgAAIDQbzgc9vv9NE2DE4eTw+Hw+/21Wu3Zs2fpdLpcLhM/TgVAVqbT6eQ4juM4suP6a4AxJpnJZDKqqh4dHZ1c9z81/3RVkJNS0zSO4xiGIfu218cAAF3Xa7VarVYzTbO/qv8SATlMyOmwsrISiUTevXsnimI/iRKkgYwPtQ5JKgzD6F2P5A3GmGEYj8dDUZQoioSRQF///Wk3DFhHCP0FJ+bbU191o8IAAAAASUVORK5CYII=\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA9klEQVR4nM1WSRKEMAiUlP//MnOIhUjYzUwNB8ulgWaNxxEJIiIi3dDjfENXSyDj4IJCDN4pPnGSDilu+oth8WJss7VWOJSRNE2pAIAScTODavMg4nx81Vp+cnnvd2oQ6ghAiH/UgKMtzWoNdHa+iVIQQ8VtHB+lTasZSCWtND5rmzlaI2mUi0hgXANSSNIXN37BrggmKFlbwmdoDQ7KTBkArDwcZmPdZY71xqfzfcvT+gtMWSz4YeCoWCvyHjR/XlSCpJIao/wRuAbkONBPtOBX5xmrWGJC9xSaoXULOXNYWArV/SF++rY56J+dfbWqj1+4+Wv5AO4MJWvSZ9dpAAAAAElFTkSuQmCC\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,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\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA+klEQVR4nMWVyxLCMAhFi+P//zIunCIFchrSqiw6aRruI4Bu2x9DVe25HI+bxKxGaUJVwz64lBGoiKiqiPj896ZffICkgBrqtTh1kw+cEGTc8BXgIJeE5Iv2VjJloDnvolAGvx+K5MtAVvJVlLeRX0uLNQGjz4ggdC+EtUPNZ+WDCSawHSqyFc0WeYbD4alRKF+hKfk8ObAuzBAjpdn0E0xYp3s4/3MUdPhEUp3XMMxlI0zNAbQjFyZQxuH2xj1r6bp3Odk4B7QsoTcSjjSjOLRpC9pi6r9sDb2XMrpQRmlwtIvWJcg5usf9NJO4C4dX4rvov+O4FFcEvgAmhM+dvTPC+AAAAABJRU5ErkJggg==\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAGkElEQVR4nLVWW08TWxSevWd3bgWmpKWtFqSCoFhNpAnxlniDBB+MJCbqg3/AP+SPMNEX9MGYGB4QEo1UkZZLqW0BpfRCpzPDzLSzZ/ach31OAQ9gzsNZD5PJZO/1zbfWty4M838aAAAc+ZW+eJ538COE0PM8AED7Se0PGL+5ZlmWZVmEEL3pui7DMBzHSZIUiUQghD6fj+d5hmF0Xd/a2tJ13XVdQsifAQAACCGe58PhcF9fnyiKhmFQmIGBgVgsFggETp8+3d3djRBqtVqtVmt+fj6VSm1ubtZqtVar5TjOvwntA0AIJUkaGRl5+vTpxYsXW62W67rFYtFxHL/fPzAwEA6Hm82mKIoQQkKIz+drtVqVSiWXy2Wz2WKx+O3bN1VVMcaUEwVDBxn4/f7x8fEnT56YprmwsJDP579+/YoQ6ujoMAwjGAzW6/VyuaxpmqIooVBIEIR4PJ5MJicmJjRNm5mZ+fjxY7FYLJVKpmk6jkMIQQcZdHZ2jo6OSpL0+vXrFy9e7O7uYoxZlu3u7i6VSsFgcGtrq1Ao2LaNMaYkeJ5PJBIPHjy4evXq1NTU7du3c7nc9PT07OxsvV4HAKCD8QqHw7IsI4RmZ2d//vxp2zbDMCzLdnZ2CoIQDod//fpl27au64QQmn/P8xqNxvr6ejKZPHv27JUrV8bGxuLxuKqq8/PzzWZzH8B1XdM0q9VqpVLRNI1yAgBIkpRIJO7du9fV1aUoyvb2NuXuOI7rujTQiqLMzc19+vTp/fv3z58/v3PnzrNnz9bX17e3tw8xoPebzWYgEPD7/a7rIoSGhoaSySRCaGNjo1AoaJpm27bjOAAAn88niiIhxLIswzAIIYZhTE9Pj42NXbt2LRqN7uzsHEoyjYllWTQgkUiEegcAZLPZfD6/srLSaDRc1wUAcBwnimJPTw/GuF6vG4aBMcYY5/N5wzBkWQ4EAr/nAELYaDQwxpcuXcIYh0Kh4eHhVqtVq9WWl5eXlpYURaEShBAihEKhkCzLGOOdnR1aBxDCer2+trZGy4VhGNj27nkeIYQqx+fzOY5jWdbm5mYul1tYWFhfX6f/7nkehJDn+UgkMj4+/vjx476+PgAALWZCiK7rP378sG2b47hDdUBvdnR0NBqNtbW1lZUVlmUDgYAgCIqiqKpq2zbtQizLCoIQiUT6+/uj0Wiz2Ww2m7SyaI/SdT2Xy5XL5d/rgGVZQkipVJqZmSmXywghy7IkSXIcByHEcRwhRBTF3t7enp6eRCIxNjbmOI5pmhjjdpOgfgAAlO6hJAeDwWg0ms1maSYghDQNPM9vb2/7/f7R0dHh4eHBwUFd1z3PO3fuXK1Wi8VigiB4nue6LoQwEAgMDw8Hg0HK+FAd7O3tmaZZr9c5jpNlub+//9GjR/F4PBwOY4w5jhsZGaGyKZVK5XK5Wq1yHHfjxo2dnZ1Go1GtVru7u+/fv//w4cPV1VXbtg+pCEIoCIJlWRzHXbhwYXBw8ObNm5cvXxZFsVKpbG5uViqVTCYjCIIkSdls1ufzJZPJM2fO3Lp1K5FIVKtVjDHP80NDQyzLFgoFRVH2GdCKvX79+sTERKFQYBhmamqqt7e3VCqlUqnl5eVarabrum3bCCG/36+qqizLoih2dXXFYjFN08rlsiRJq6uri4uLmqZ9+PBBVdV9FUEI+/v7Jycnh4aGFhcXS6XS27dvEULpdJqGwnEc2iHoOGIYpqura3d3l2EY0zTfvHnz6tUrRVFs23Zdl2VZy7IOdVMIYV9f3/nz58vl8rt37zKZzNramiRJ9Xp9b2/PsixCSLvF0x7VbDa/fPly9+5dQsjc3Fw+n3cch/mXIRofWZYnJyeDweDLly9TqZRhGKqqIoRoR6OCawuR1pTjOJlMJpPJxGKxjY2NI8fZPkA0Gj116pRpmrlczrIs6rE9mI4buYZhpNNpWmjHTX9Ea1hVVUKIJEnUXfvJHN4tDprneYZhfP/+nf7QkWeYdi8yTTOdThuGEYvF6NQ97sJBc11X0zSqrj8AWJaVTqc/f/7c2dkZiUQ4juM4jmXZkwEo9WKxSBv4kWf+DpFt20tLSzzPC4Lg8/kkScIYt3vkCVEyTXNra+u4DDPttQUAwPN8MBiUZVlV1b29PYSQYRhUy8dtcCzLQgghhFRsJwEwDENnCELoYOOlQjphcaM1cYLSfl8d6emD7ydr6chF9liAI+8fhPmvAAAAeFz223aC6zbdNhL4x5h/Vum/AIvrazM0AxZ9AAAAAElFTkSuQmCC\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABLUlEQVR4nK1WSw4FIQjTife/Mm+hMQgtoHmsHIUW+TmtIREREdGLP4sB1WRbvBrcwbvaHmIZSmh4WGk9iG5Q2A67Gb6NZ2XKRhO4BYF8WNKgU4fYQdWe3+wL3A8C2ntnHCKiT4e33ErM2X2qF1BhEWjCudjHgRhobaL5ht6dTBCdBQQqb48TR1kHFYfHVPv2h081zCS7YuKiaQ1PVvTaox9V5F2rRpPL6oPY/hm97RzE6A/xmbI6uaj6QNB7LxGYELFx7a1EZKQJ8PPAT5GJhaHYBK4+IClUOuLT/fj0IslpOcGurBKkwhJpCWCi3ubPMeyKUm86+qJdGbNr6bKmBMGMY30AcUZzIZ6f9VfTry2B4feDIfgBMDpX5bAsdVXEPehnVEuTfFudOr1z/QMBwfP80B8GpwAAAABJRU5ErkJggg==\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,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\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABB0lEQVR4nMVWSRKEIAxMLP7/5cwBhUyWJiNUTS6iwe7OhhL9y0TkCM6FOfZpEAEzb6KnBKfyQ0SBxo7OzJrmSDQ3upe/E1CQoi7WgL7muA5ipSaPAZrXrK1fjtXQ2V2DisDDqQt7KeTAA59mRkR0O4WtNVGYx35jLSMw75tF3eYcgEj78yw/gzvccGnoUKBPjuHDNSi1KfCC7M8IwJuZt34OlorsueubSwQ+lKxaOl19vSYI649TpOsfC9RufEKYFgqOyB10D+oJ0Ee/YgZdp6vf7hIsO6rhU6zI4WdtAi5rWOEABWveobXsf+nS/yL6jh3Pmt72gyZRlokIb8nNB+KouLyUsfgA9XgfodmAct4AAAAASUVORK5CYII=\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,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\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAA8ElEQVR4nN1WyxLEIAgDx///ZffQDsNGHkrrYZdTVUwCBVqif7AxxjnwdhSdiDoRMXPhJijzQHoBegYVMnn44lvP0uU5xoArsi9L8eFrbQYo+7MC8L+xzCzpy7OuWWxqcKszs06chvPkm8Ih+5GsRdXiZgb6QvXPECZHKxNAPoHDjqDwSlMCjKDW1YHVU+SZm6VCcqB+zH6uE0CHBg3r7sYt5p3Cfo+hzXEUF8JVvoaPF2Y6OzcsaZM1Ajh9v0yBGwmeDylm1i9gOwLv66Qd9LLpAyE3B9lTWx/oaRXZDluS00KyO3Gdo/a5PvsP+bP2ASFITC+qtBpcAAAAAElFTkSuQmCC\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,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\"/></td></tr></table>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<table class=\"show_images\" style=\"border-spacing:0;\"><tr><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAABLUlEQVR4nLVUyw7DMAgL0/7/l9khKnPBJkxaObW8DA6w1lruvi5x9/hFPTpU5VlSGML0/spt6939nbRmtm3xsb9TLrQqPQdMH6rq6hCBNOTccm9SPu5uyRBsUFqOABXv+wYYn6hEU7xQrlTAv5JfotLMsJWaaIc0zd2mCHOlqmuK4UK8qypyUTaOJiJ0ItVc0XE8ZO7XuBn54bbr7dC55jvxamwhzVA6CE1FANTT0Um1S1QfvINa6WRgaBMdQBNWRVVA9mCxq4u7/fOlmk7bXYkL0dzqM0UYGQMzv6ycoiTqRo0A6IGsFKuqz3jNIeq3Nx0rZeWLlqaIFjHkSr7BhPcNs62VVdlBhal5+xCMGo3pAqKxp8okKWh4CSae/NnmAD9JN0X/lccBnpKg6AMTDZ1X0qK64AAAAABJRU5ErkJggg==\"/></td><td><img width=\"32\" height=\"32\" style=\"image-rendering:pixelated; object-fit:cover;\" src=\"data:image/png;base64,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\"/></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import mediapy\n",
        "model.eval()\n",
        "imgs = range(10)\n",
        "for i in imgs:\n",
        "    # ae_crit(model.AE(model.F(target_dataset[i][0].unsqueeze(0).cuda())).cpu(), target_dataset[1][0].unsqueeze(0))\n",
        "    orig = source_dataset[i][0].unsqueeze(0).cuda()\n",
        "    # orig = uniform_noise(source_dataset[i][0].unsqueeze(0).cuda(), 0.025)\n",
        "    img = model.AE(model.F(orig))\n",
        "    # img.max()\n",
        "    mediapy.show_images([\n",
        "        orig.cpu().detach().numpy().squeeze(),\n",
        "        img.cpu().detach().numpy().squeeze()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8_-0iSSje4w"
      },
      "source": [
        "# Inference\n",
        "\n",
        "就跟前幾次作業一樣。這裡我使用pd來生產csv，因為看起來比較潮(?)\n",
        "\n",
        "此外，200 epochs的Accuracy可能會不太穩定，可以多丟幾次或train久一點。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wly5AgH2jePv"
      },
      "source": [
        "model = DomainAdapt()\n",
        "model.load().cuda()\n",
        "\n",
        "result = []\n",
        "model.eval()\n",
        "logits = []\n",
        "target_dataset.transforms=test_transform\n",
        "for i, (test_data, _) in enumerate(tqdm(test_dataloader)):\n",
        "    test_data = test_data.cuda()\n",
        "\n",
        "    class_logits = model(test_data)\n",
        "    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
        "    logits.append(class_logits.cpu().detach())\n",
        "    result.append(x)\n",
        "\n",
        "import pandas as pd\n",
        "result = np.concatenate(result)\n",
        "logits = torch.cat(logits, axis=0)\n",
        "# Generate your submission\n",
        "def save_result(result):\n",
        "    print(np.unique(result, return_counts=True)[1])\n",
        "    df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
        "    df.to_csv('DaNN_submission.csv', index=False)\n",
        "\n",
        "save_result(result)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:11<00:00,  8.64it/s]\n",
            "[ 9762  9148 12338 10024  9533 10286 12810  7726 10623  7750]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## label balance"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "prob = F.softmax(logits, 1).cpu().numpy()\n",
        "label = prob.argsort(1)\n",
        "top2 = np.take_along_axis(prob, label[:, [-1, -2]], 1)\n",
        "ratio = top2[:, 1]/top2[:, 0]\n",
        "mask = np.ones((len(label)), dtype=bool)\n",
        "res = np.zeros((len(label)), dtype=int)\n",
        "count = np.zeros(n_class, dtype=int)\n",
        "max_n = len(res)//n_class\n",
        "for i in range(n_class):\n",
        "    # i = 1\n",
        "    m = label[:, -1] == i\n",
        "    num = int(m.sum())\n",
        "    idx = ratio[m].argsort()\n",
        "    sel = np.arange(len(label))[m][idx[:max_n]]\n",
        "    res[sel] = i\n",
        "    mask[sel] = False\n",
        "\n",
        "# sel = ratio[mask] >= 0.5\n",
        "# idx = np.arange(len(label))[mask]\n",
        "# res[idx[sel]] = label[idx[sel], -2]\n",
        "# res[idx[~sel]] = label[idx[~sel], -1]\n",
        "res[mask] = label[mask, -2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9762  9148 12338 10024  9533 10286 12810  7726 10623  7750]\n"
          ]
        }
      ],
      "source": [
        "save_result(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtXnEMUNCE78"
      },
      "source": [
        "# Training Statistics\n",
        "\n",
        "- Number of parameters:\n",
        "  - Feature Extractor: 2, 142, 336\n",
        "  - Label Predictor: 530, 442\n",
        "  - Domain Classifier: 1, 055, 233\n",
        "\n",
        "- Simple\n",
        " - Training time on colab: ~ 1 hr\n",
        "- Medium\n",
        " - Training time on colab: 2 ~ 4 hr\n",
        "- Strong\n",
        " - Training time on colab: 5 ~ 6 hrs\n",
        "- Boss\n",
        " - **Unmeasurable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duk7k43Am9xH"
      },
      "source": [
        "# Learning Curve (Strong Baseline)\n",
        "* This method is slightly different from colab.\n",
        "\n",
        "![Loss Curve](https://i.imgur.com/vIujQyo.png)\n",
        "\n",
        "# Accuracy Curve (Strong Baseline)\n",
        "* Note that you cannot access testing accuracy. But this plot tells you that even though the model overfits the training data, the testing accuracy is still improving, and that's why you need to train more epochs.\n",
        "\n",
        "![Acc Curve](https://i.imgur.com/4W1otXG.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6UfXzef-wNl"
      },
      "source": [
        "# Q&A\n",
        "\n",
        "有任何問題 Domain Adaptation 的問題可以寄信到ntu-ml-2021spring-ta@googlegroups.com。\n",
        "\n",
        "時間允許的話我會更新在這裡。\n",
        "\n",
        "# Special Thanks\n",
        "這次的作業其實是我出在 2019FALL 的 ML Final Project，以下是我認為在 Final Report 不錯的幾組，有興趣的話歡迎大家參考看看。\n",
        "\n",
        "[NTU_r08942071_太神啦 / 組長: 劉正仁同學](https://drive.google.com/open?id=11uNDcz7_eMS8dMQxvnWsbrdguu9k4c-c)\n",
        "\n",
        "[NTU_r08921a08_CAT / 組長: 廖子毅同學](https://drive.google.com/open?id=1xIkSs8HAShdcfV1E0NEnf4JDbL7POZTf)\n"
      ]
    }
  ]
}