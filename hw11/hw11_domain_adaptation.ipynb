{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw11_domain_adaptation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python385jvsc74a57bd008f5944c08cf889f43480de463c4b71999d47c1cfd2d200a929d500fe2e3b01b",
      "display_name": "Python 3.8.5 64-bit ('torch': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cFq_TgWlQ_"
      },
      "source": [
        "# Homework 11 - Transfer Learning (Domain Adversarial Training)\n",
        "\n",
        "> Author: Arvin Liu (r09922071@ntu.edu.tw)\n",
        "\n",
        "若有任何問題，歡迎來信至助教信箱 ntu-ml-2021spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNiZCGrIYKdR"
      },
      "source": [
        "# Readme\n",
        "\n",
        "\n",
        "這份作業的任務是Transfer Learning中的Domain Adversarial Training。\n",
        "\n",
        "<img src=\"https://i.imgur.com/iMVIxCH.png\" width=\"500px\">\n",
        "\n",
        "> 也就是左下角的那一塊。\n",
        "\n",
        "## Scenario and Why Domain Adversarial Training\n",
        "你現在有Source Data + label，其中Source Data和Target Data可能有點關係，所以你想要訓練一個model做在Source Data上並Predict在Target Data上。\n",
        "\n",
        "但這樣有什麼樣的問題? 相信大家學過Anomaly Detection就會知道，如果有data是在Source Data沒有出現過的(或稱Abnormal的)，那麼model大部分都會因為不熟悉這個data而可能亂做一發。 \n",
        "\n",
        "以下我們將model拆成Feature Extractor(上半部)和Classifier(下半部)來作例子:\n",
        "<img src=\"https://i.imgur.com/IL0PxCY.png\" width=\"500px\">\n",
        "\n",
        "整個Model在學習Source Data的時候，Feature Extrator因為看過很多次Source Data，所以所抽取出來的Feature可能就頗具意義，例如像圖上的藍色Distribution，已經將圖片分成各個Cluster，所以這個時候Classifier就可以依照這個Cluster去預測結果。\n",
        "\n",
        "但是在做Target Data的時候，Feature Extractor會沒看過這樣的Data，導致輸出的Target Feature可能不屬於在Source Feature Distribution上，這樣的Feature給Classifier預測結果顯然就不會做得好。\n",
        "\n",
        "## Domain Adversarial Training of Nerural Networks (DaNN)\n",
        "基於如此，是不是只要讓Soucre Data和Target Data經過Feature Extractor都在同個Distribution上，就會做得好了呢? 這就是DaNN的主要核心。\n",
        "\n",
        "<img src=\"https://i.imgur.com/vrOE5a6.png\" width=\"500px\">\n",
        "\n",
        "我們追加一個Domain Classifier，在學習的過程中，讓Domain Classifier去判斷經過Feature Extractor後的Feature是源自於哪個domain，讓Feature Extractor學習如何產生Feature以**騙過**Domain Classifier。 持久下來，通常Feature Extractor都會打贏Domain Classifier。(因為Domain Classifier的Input來自於Feature Extractor，而且對Feature Extractor來說Domain&Classification的任務並沒有衝突。)\n",
        "\n",
        "如此一來，我們就可以確信不管是哪一個Domain，Feature Extractor都會把它產生在同一個Feature Distribution上。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-qnUkspmap3"
      },
      "source": [
        "# Data Introduce\n",
        "\n",
        "這次的任務是Source Data: 真實照片，Target Data: 手畫塗鴉。\n",
        "\n",
        "我們必須讓model看過真實照片以及標籤，嘗試去預測手畫塗鴉的標籤為何。\n",
        "\n",
        "資料位於[這裡](https://drive.google.com/open?id=12-07DSquGdzN3JBHBChN4nMo3i8BqTiL)，以下的code分別為下載和觀看這次的資料大概長甚麼樣子。\n",
        "\n",
        "特別注意一點: **這次的source和target data的圖片都是平衡的，你們可以使用這個資訊做其他事情。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "0_uO-ZSDoR6i",
        "outputId": "546b0695-ef92-4317-b9d8-3786946b5136"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def no_axis_show(img, title='', cmap=None):\n",
        "  # imshow, and set the interpolation mode to be \"nearest\"。\n",
        "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
        "  # do not show the axes in the images.\n",
        "  fig.axes.get_xaxis().set_visible(False)\n",
        "  fig.axes.get_yaxis().set_visible(False)\n",
        "  plt.title(title)\n",
        "\n",
        "# titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "# plt.figure(figsize=(18, 18))\n",
        "# for i in range(10):\n",
        "#   plt.subplot(1, 10, i+1)\n",
        "#   fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "# plt.figure(figsize=(18, 18))\n",
        "# for i in range(10):\n",
        "#   plt.subplot(1, 10, i+1)\n",
        "#   fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "3eMs7DbVt4Ee",
        "outputId": "6b400470-daec-4cd8-986b-041ae6ddf459"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moXQw9To5TqZ"
      },
      "source": [
        "# Special Domain Knowledge\n",
        "\n",
        "因為大家塗鴉的時候通常只會畫輪廓，我們可以根據這點將source data做點邊緣偵測處理，讓source data更像target data一點。\n",
        "\n",
        "## Canny Edge Detection\n",
        "算法這邊不贅述，只教大家怎麼用。若有興趣歡迎參考wiki或[這裡](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19)。\n",
        "\n",
        "cv2.Canny使用非常方便，只需要兩個參數: low_threshold, high_threshold。\n",
        "\n",
        "```cv2.Canny(image, low_threshold, high_threshold)```\n",
        "\n",
        "簡單來說就是當邊緣值超過high_threshold，我們就確定它是edge。如果只有超過low_threshold，那就先判斷一下再決定是不是edge。\n",
        "\n",
        "以下我們直接拿source data做做看。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mn2MkDLV7E2-",
        "outputId": "8700d78d-77cf-499c-b58e-e4f9f500bd38"
      },
      "source": [
        "import cv2\n",
        "# import matplotlib.pyplot as plt\n",
        "# titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
        "# plt.figure(figsize=(18, 18))\n",
        "\n",
        "# original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
        "# plt.subplot(1, 5, 1)\n",
        "# no_axis_show(original_img, title='original')\n",
        "\n",
        "# gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "# plt.subplot(1, 5, 2)\n",
        "# no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "# gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
        "# plt.subplot(1, 5, 2)\n",
        "# no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
        "\n",
        "# canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
        "# plt.subplot(1, 5, 3)\n",
        "# no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
        "\n",
        "# canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
        "# plt.subplot(1, 5, 4)\n",
        "# no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
        "\n",
        "# canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
        "# plt.subplot(1, 5, 5)\n",
        "# no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8THSdt_hmwYh"
      },
      "source": [
        "# Data Process\n",
        "\n",
        "在這裡我故意將data用成可以使用torchvision.ImageFolder的形式，所以只要使用該函式便可以做出一個datasets。\n",
        "\n",
        "transform的部分請參考以下註解。\n",
        "<!-- \n",
        "#### 一些細節\n",
        "\n",
        "在一般的版本上，對灰階圖片使用RandomRotation使用```transforms.RandomRotation(15)```即可。但在colab上需要加上```fill=(0,)```才可運行。\n",
        "在n98上執行需要把```fill=(0,)```拿掉才可運行。 -->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZHIBGknmi8Z"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "source_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n",
        "    transforms.Grayscale(),\n",
        "    # cv2 do not support skimage.Image, so we transform it to np.array, \n",
        "    # and then adopt cv2.Canny algorithm.\n",
        "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), np.random.randint(170, 200), np.random.randint(250, 300))),\n",
        "    # Transform np.array back to the skimage.Image.\n",
        "    transforms.ToPILImage(),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(15, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "target_transform = transforms.Compose([\n",
        "    # Turn RGB to grayscale.\n",
        "    transforms.Grayscale(),\n",
        "    # Resize: size of source data is 32x32, thus we need to \n",
        "    #  enlarge the size of target data from 28x28 to 32x32。\n",
        "    transforms.Resize((32, 32)),\n",
        "    # 50% Horizontal Flip. (For Augmentation)\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
        "    # if there's empty pixel after rotation.\n",
        "    transforms.RandomRotation(15, fill=(0,)),\n",
        "    # Transform to tensor for model inputs.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
        "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
        "\n",
        "batch_size = 100\n",
        "source_dataloader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "target_dataloader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "target_dataloader_nshuf = DataLoader(\n",
        "    target_dataset, batch_size=100, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_dataloader = DataLoader(target_dataset, batch_size=1000, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_class = len(source_dataset.class_to_idx.values())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdwDEMrOycs5"
      },
      "source": [
        "# Model\n",
        "\n",
        "Feature Extractor: 典型的VGG-like疊法。\n",
        "\n",
        "Label Predictor / Domain Classifier: MLP到尾。\n",
        "\n",
        "相信作業寫到這邊大家對以下的Layer都很熟悉，因此不再贅述。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uw2eP09z-pD"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x).squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DomainAdapt(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.F = FeatureExtractor()\n",
        "        self.C = LabelPredictor()\n",
        "        self.D = DomainClassifier()\n",
        "    \n",
        "    def setOptim(self, F, C, D):\n",
        "        self.optim_F = F\n",
        "        self.optim_C = C\n",
        "        self.optim_D = D\n",
        "    \n",
        "    def setLR(self, F, C, D):\n",
        "        self.lr_F = F\n",
        "        self.lr_C = C\n",
        "        self.lr_D = D\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.C(self.F(x))\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optim_F.zero_grad()\n",
        "        self.optim_C.zero_grad()\n",
        "        self.optim_D.zero_grad()\n",
        "\n",
        "    def stepFC(self):\n",
        "        self.optim_F.step()\n",
        "        self.optim_C.step()\n",
        "\n",
        "    def stepF(self):\n",
        "        self.optim_F.step()\n",
        "    \n",
        "    def stepC(self):\n",
        "        self.optim_C.step()\n",
        "    \n",
        "    def stepD(self):\n",
        "        self.optim_D.step()\n",
        "\n",
        "    def stepLR(self):\n",
        "        self.lr_F.step()\n",
        "        self.lr_C.step()\n",
        "        self.lr_D.step()\n",
        "\n",
        "    def load(self, isD=True):\n",
        "        self.F.load_state_dict(torch.load('extractor_model.bin'))\n",
        "        self.C.load_state_dict(torch.load('predictor_model.bin'))\n",
        "        if isD: self.D.load_state_dict(torch.load('discriminator_model.bin'))\n",
        "        return self\n",
        "    \n",
        "    def save(self, isD=True):\n",
        "        torch.save(self.F.state_dict(), f'extractor_model.bin')\n",
        "        torch.save(self.C.state_dict(), f'predictor_model.bin')\n",
        "        if isD: torch.save(self.D.state_dict(), f'discriminator_model.bin')\n",
        "        \n",
        "    # def cuda(self):\n",
        "    #     self.F.cuda()\n",
        "    #     self.C.cuda()\n",
        "    #     self.D.cuda()\n",
        "    #     return self\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxdBIPhF0Icb"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "這裡我們選用Adam來當Optimizer。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epoch = 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrxKelBy0PJ7"
      },
      "source": [
        "model = DomainAdapt().load().cuda()\n",
        "# model = DomainAdapt().cuda()\n",
        "\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "class_sep_crit = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "optimizer_F = optim.AdamW(model.F.parameters(), lr=lr, weight_decay=1e-4)\n",
        "lr_F = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_F, T_max=n_epoch, eta_min=1e-6, verbose=False)\n",
        "\n",
        "optimizer_C = optim.AdamW(model.C.parameters(), lr=lr, weight_decay=1e-3)\n",
        "lr_C = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_C, T_max=n_epoch, eta_min=1e-6, verbose=False)\n",
        "\n",
        "optimizer_D = optim.AdamW(model.D.parameters(), lr=lr, weight_decay=1e-3)\n",
        "lr_D = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_D, T_max=n_epoch, eta_min=1e-6, verbose=False)\n",
        "\n",
        "model.setOptim(optimizer_F, optimizer_C, optimizer_D)\n",
        "model.setLR(lr_F, lr_C, lr_D)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuAE4cqJ0itR"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "\n",
        "## 如何實作DaNN?\n",
        "\n",
        "理論上，在原始paper中是加上Gradient Reversal Layer，並將Feature Extractor / Label Predictor / Domain Classifier 一起train，但其實我們也可以交換的train Domain Classfier & Feature Extractor(就像在train GAN的Generator & Discriminator一樣)，這也是可行的。\n",
        "\n",
        "在code實現中，我們採取後者的方式，畢竟GAN是之前的作業，應該會比較熟悉:)。\n",
        "\n",
        "## 小提醒\n",
        "* 原文中的lambda(控制Domain Adversarial Loss的係數)是有Adaptive的版本，如果有興趣可以參考[原文](https://arxiv.org/pdf/1505.07818.pdf)。\n",
        "* 因為我們完全沒有target的label，所以結果如何，只好丟kaggle看看囉:)?"
      ]
    },
    {
      "source": [
        "## Semi-supervised"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SubsetCustomLabel():\n",
        "    def __init__(self, dataset, labels, indices):\n",
        "        # super().__init__(dataset, indices)\n",
        "        self.dataset = dataset\n",
        "        self.labels = labels\n",
        "        self.indices = indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset[self.indices[idx]][0], self.labels[idx]\n",
        "\n",
        "    def set_transform(self, transform):\n",
        "        self.dataset.transform = transform\n",
        "\n",
        "\n",
        "# def SubsetCustomLabel(dataset, labels, indices):\n",
        "#     subset = Subset(dataset, indices)\n",
        "#     subset.labels = labels\n",
        "\n",
        "#     subset.__len__ = __len__\n",
        "#     subset.__getitem__ = __getitem__\n",
        "#     subset.set_transform = set_transform\n",
        "#     return subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pseudo_labels(model, dataloader, threshold=0.5):\n",
        "    model.eval()\n",
        "    # Define softmax function.\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    idx = []\n",
        "    targets = []\n",
        "    count = torch.zeros(n_class, dtype=torch.float32)\n",
        "\n",
        "    # Iterate over the dataset by batches.\n",
        "    c = 0\n",
        "    for i, (img, ans) in enumerate(tqdm(dataloader, desc='PseudoLabels', leave=False)):\n",
        "        with torch.no_grad():\n",
        "            logits = model(img.cuda())\n",
        "\n",
        "        # Obtain the probability distributions by applying softmax on logits.\n",
        "        probs = softmax(logits)\n",
        "        # Filter the data and construct a new dataset.\n",
        "        probs1 = probs.max(dim=1).values\n",
        "        \n",
        "        select = (probs1 > threshold)\n",
        "        c = probs[select].sum(dim=0).cpu()\n",
        "        count += c\n",
        "        if not (select.any()):\n",
        "            continue\n",
        "        probs_arg = probs[select].argmax(dim=1)\n",
        "        targets.append(probs_arg)\n",
        "        idx += (torch.where(select)[0] + batch_size*i).tolist()\n",
        "\n",
        "    # custom subset\n",
        "    if len(targets) == 0:\n",
        "        return None, None\n",
        "    targets = torch.cat(targets, dim=0).cpu().tolist()\n",
        "    new = SubsetCustomLabel(dataloader.dataset, targets, idx)\n",
        "    model.train()\n",
        "    return new, count\n",
        "\n",
        "def get_semi_set(model, threshold=0.5):\n",
        "    # target_dataset.transform = tfm_weak\n",
        "    # source_dataset.transform = tfm_weak\n",
        "    \n",
        "    pseudo_dataset, count = get_pseudo_labels(\n",
        "        model, target_dataloader_nshuf, threshold=threshold)\n",
        "    if pseudo_dataset is None:\n",
        "        return []\n",
        "    # pseudo_loader = DataLoader(pseudo_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "   \n",
        "    # target_dataset.transform = tfm_strong\n",
        "    # train_set.transform = tfm_strong\n",
        "    return pseudo_dataset, count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_cls_epoch(dataloader, weight):\n",
        "    '''\n",
        "      Args:\n",
        "        source_dataloader: source data的dataloader\n",
        "        target_dataloader: target data的dataloader\n",
        "        lamb: control the balance of domain adaptatoin and classification.\n",
        "    '''\n",
        "\n",
        "    # D loss: Domain Classifier的loss\n",
        "    # F loss: Feature Extrator & Label Predictor的loss\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "    class_count = torch.zeros((n_class)).cuda()\n",
        "\n",
        "    for i, (data, ans) in enumerate((dataloader)):\n",
        "        ans = ans.cuda()\n",
        "        class_logits = model(data.cuda())\n",
        "        predict_label = torch.argmax(class_logits, dim=1)\n",
        "        \n",
        "        loss_balance = predict_label.bincount(minlength=n_class).float()\n",
        "        class_count += loss_balance\n",
        "        loss_balance = loss_balance/loss_balance.sum()\n",
        "        loss_balance = ((loss_balance-loss_balance.mean()).abs()).sum()\n",
        "\n",
        "        loss_cls = class_sep_crit(class_logits, ans)\n",
        "        loss_cls = (weight[ans]*loss_cls).mean()\n",
        "\n",
        "\n",
        "        loss = loss_cls + loss_balance*2\n",
        "        running_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        model.stepFC()\n",
        "        model.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(predict_label == ans).item()\n",
        "        total_num += data.shape[0]\n",
        "\n",
        "        # print(i, end='\\r')\n",
        "    class_count = class_count.cpu()*n_class / total_num\n",
        "    return running_loss / (i+1), total_hit / total_num, class_count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRAFFKvX0p9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57fdb3d3-88f1-4cc1-94e5-fd6c40377263",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
        "    '''\n",
        "      Args:\n",
        "        source_dataloader: source data的dataloader\n",
        "        target_dataloader: target data的dataloader\n",
        "        lamb: control the balance of domain adaptatoin and classification.\n",
        "    '''\n",
        "\n",
        "    # D loss: Domain Classifier的loss\n",
        "    # F loss: Feature Extrator & Label Predictor的loss\n",
        "\n",
        "    running_D_loss, running_F_loss = 0.0, 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "    # confusion = torch.zeros((n_class, n_class), dtype=float)\n",
        "    class_count = torch.zeros((n_class)).cuda()\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "        target_data = target_data.cuda()\n",
        "        \n",
        "        # Mixed the source data and target data, or it'll mislead the running params\n",
        "        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n",
        "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
        "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
        "        # set domain label of source data to be 1.\n",
        "        domain_label[:source_data.shape[0]] = 1\n",
        "\n",
        "        # ======================= Step 1 : train domain classifier\n",
        "        feature = model.F(mixed_data)\n",
        "        # We don't need to train feature extractor in step 1.\n",
        "        # Thus we detach the feature neuron to avoid backpropgation.\n",
        "\n",
        "        domain_logits = model.D(feature.detach())\n",
        "        loss = domain_criterion(domain_logits, domain_label)\n",
        "        running_D_loss += loss.item()\n",
        "        loss.backward()\n",
        "        model.stepD()\n",
        "\n",
        "        # ======================= Step 2 : train feature extractor and label classifier\n",
        "        class_logits = model.C(feature[:source_data.shape[0]])\n",
        "        domain_logits = model.D(feature)\n",
        "        predict_label = torch.argmax(class_logits, dim=1)\n",
        "        # domain_label = torch.full([source_data.shape[0] + target_data.shape[0], 1], 0.5).cuda()\n",
        "\n",
        "        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n",
        "\n",
        "        loss_balance = predict_label.bincount(minlength=n_class).float()\n",
        "        class_count += loss_balance\n",
        "        loss_balance = loss_balance/loss_balance.sum()\n",
        "        loss_balance = ((loss_balance-loss_balance.mean()).abs()).sum()\n",
        "\n",
        "        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n",
        "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label) + loss_balance*2\n",
        "        running_F_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        model.stepFC()\n",
        "        model.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(predict_label == source_label).item()\n",
        "        total_num += source_data.shape[0]\n",
        "\n",
        "        # print(i, end='\\r')\n",
        "    class_count = class_count.cpu()*n_class / total_num\n",
        "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num, class_count\n",
        "\n",
        "\n",
        "\n",
        "# train 200 epochs\n",
        "weight = torch.ones(n_class)\n",
        "# class_criterion = crosss\n",
        "train_acc = 0.0\n",
        "semi_flg = False\n",
        "for epoch in (range(n_epoch)):\n",
        "    # You should chooose lamnda cleverly.\n",
        "    # if True:\n",
        "    if (train_acc >= 0.9 or semi_flg):\n",
        "        pseudo_dataset, count = get_semi_set(model, 0.6)\n",
        "        print(f\"Get pseudo label: {len(pseudo_dataset)}\")\n",
        "        print(\", \".join([f\"{int(c):4d}\" for c in count.tolist()]))\n",
        "        weight = (count.mean()/count).cuda()\n",
        "    else:\n",
        "        pseudo_dataset = None\n",
        "    \n",
        "    if pseudo_dataset:\n",
        "        semi_flg = True\n",
        "        dataloader = DataLoader(ConcatDataset([pseudo_dataset, source_dataset]),\n",
        "        # dataloader = DataLoader(pseudo_dataset,\n",
        "            batch_size=batch_size*10, shuffle=True, num_workers=4, pin_memory=True)\n",
        "        train_loss, train_acc, class_count = train_cls_epoch(dataloader, weight)\n",
        "        print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_loss, train_acc))\n",
        "    \n",
        "    else:\n",
        "        train_D_loss, train_F_loss, train_acc, class_count = train_epoch(\n",
        "            source_dataloader, target_dataloader, lamb=1.)\n",
        "        # weight = 1+(1-class_count)**2\n",
        "        # class_criterion = nn.CrossEntropyLoss(weight=weight.cuda())\n",
        "\n",
        "        print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n",
        "    \n",
        "    model.save()\n",
        "    model.stepLR()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".2437, acc 0.9914\n",
            "                                                                Get pseudo label: 99498\n",
            "12126, 9809, 9101, 9461, 8742, 10947, 10106, 9246, 10903, 9053\n",
            "epoch 168: train loss: 0.2353, acc 0.9916\n",
            "Get pseudo label: 99507\n",
            "12212, 9849, 9147, 9415, 8699, 10916, 10110, 9176, 10791, 9188\n",
            "epoch 169: train loss: 0.2387, acc 0.9915\n",
            "                                                                Get pseudo label: 99469\n",
            "12251, 9758, 9203, 9314, 8699, 10981, 10119, 9164, 10728, 9246\n",
            "epoch 170: train loss: 0.2398, acc 0.9914\n",
            "                                                                Get pseudo label: 99463\n",
            "12107, 9726, 9216, 9286, 8738, 10995, 10117, 9305, 10711, 9258\n",
            "epoch 171: train loss: 0.2369, acc 0.9913\n",
            "                                                                Get pseudo label: 99541\n",
            "12110, 9901, 9179, 9394, 8677, 10956, 10101, 9167, 10755, 9296\n",
            "epoch 172: train loss: 0.2345, acc 0.9919\n",
            "                                                                Get pseudo label: 99552\n",
            "12039, 10051, 9192, 9389, 8666, 10922, 10109, 9221, 10721, 9237\n",
            "epoch 173: train loss: 0.2355, acc 0.9916\n",
            "                                                                Get pseudo label: 99526\n",
            "12191, 9989, 9157, 9344, 8669, 10916, 10103, 9292, 10628, 9232\n",
            "epoch 174: train loss: 0.2347, acc 0.9920\n",
            "                                                                 Get pseudo label: 99559\n",
            "12166, 10003, 9140, 9286, 8662, 10889, 10089, 9350, 10665, 9303\n",
            "epoch 175: train loss: 0.2390, acc 0.9921\n",
            "                                                                Get pseudo label: 99577\n",
            "12032, 10066, 9101, 9392, 8652, 10929, 10100, 9219, 10712, 9369\n",
            "epoch 176: train loss: 0.2379, acc 0.9920\n",
            "                                                                Get pseudo label: 99577\n",
            "11824, 10118, 9121, 9431, 8678, 10946, 10110, 9264, 10777, 9302\n",
            "epoch 177: train loss: 0.2340, acc 0.9920\n",
            "                                                                Get pseudo label: 99584\n",
            "11838, 10047, 9111, 9419, 8691, 10927, 10112, 9351, 10749, 9334\n",
            "epoch 178: train loss: 0.2336, acc 0.9923\n",
            "                                                                Get pseudo label: 99605\n",
            "11924, 10083, 9127, 9431, 8665, 10933, 10110, 9272, 10727, 9329\n",
            "epoch 179: train loss: 0.2254, acc 0.9922\n",
            "                                                                 Get pseudo label: 99578\n",
            "12000, 10095, 9196, 9339, 8643, 10911, 10112, 9295, 10708, 9272\n",
            "epoch 180: train loss: 0.2290, acc 0.9928\n",
            "                                                                Get pseudo label: 99618\n",
            "12155, 9992, 9138, 9365, 8593, 10911, 10112, 9351, 10686, 9310\n",
            "epoch 181: train loss: 0.2337, acc 0.9922\n",
            "                                                                Get pseudo label: 99612\n",
            "12216, 9969, 9128, 9351, 8580, 10955, 10105, 9290, 10701, 9312\n",
            "epoch 182: train loss: 0.2349, acc 0.9928\n",
            "                                                                Get pseudo label: 99572\n",
            "12251, 9949, 9160, 9283, 8576, 10913, 10105, 9337, 10632, 9360\n",
            "epoch 183: train loss: 0.2332, acc 0.9924\n",
            "                                                                Get pseudo label: 99636\n",
            "12183, 9991, 9148, 9340, 8644, 10953, 10105, 9335, 10627, 9306\n",
            "epoch 184: train loss: 0.2326, acc 0.9932\n",
            "                                                                Get pseudo label: 99665\n",
            "12151, 9973, 9096, 9364, 8682, 10946, 10106, 9339, 10701, 9302\n",
            "epoch 185: train loss: 0.2299, acc 0.9931\n",
            "                                                                Get pseudo label: 99666\n",
            "12127, 10025, 9141, 9380, 8678, 10898, 10106, 9322, 10714, 9270\n",
            "epoch 186: train loss: 0.2401, acc 0.9929\n",
            "                                                                Get pseudo label: 99612\n",
            "11833, 10133, 9166, 9370, 8714, 10869, 10116, 9312, 10835, 9258\n",
            "epoch 187: train loss: 0.2275, acc 0.9930\n",
            "                                                                Get pseudo label: 99613\n",
            "11937, 10050, 9142, 9439, 8703, 10912, 10118, 9192, 10774, 9343\n",
            "epoch 188: train loss: 0.2337, acc 0.9933\n",
            "                                                                Get pseudo label: 99672\n",
            "11913, 10056, 9130, 9380, 8690, 10930, 10118, 9343, 10770, 9338\n",
            "epoch 189: train loss: 0.2243, acc 0.9928\n",
            "                                                                Get pseudo label: 99634\n",
            "11897, 10104, 9100, 9438, 8700, 10919, 10114, 9329, 10780, 9248\n",
            "epoch 190: train loss: 0.2287, acc 0.9929\n",
            "                                                                Get pseudo label: 99653\n",
            "11957, 10100, 9140, 9404, 8701, 10894, 10114, 9346, 10756, 9236\n",
            "epoch 191: train loss: 0.2363, acc 0.9937\n",
            "                                                                Get pseudo label: 99666\n",
            "11968, 10086, 9107, 9457, 8662, 10920, 10107, 9284, 10769, 9300\n",
            "epoch 192: train loss: 0.2323, acc 0.9928\n",
            "                                                                Get pseudo label: 99649\n",
            "12137, 10057, 9116, 9391, 8626, 10885, 10098, 9304, 10740, 9290\n",
            "epoch 193: train loss: 0.2258, acc 0.9934\n",
            "                                                                Get pseudo label: 99668\n",
            "12039, 10048, 9146, 9398, 8691, 10900, 10113, 9342, 10743, 9244\n",
            "epoch 194: train loss: 0.2326, acc 0.9933\n",
            "Get pseudo label: 99656\n",
            "12003, 10016, 9143, 9382, 8674, 10920, 10112, 9324, 10708, 9370\n",
            "epoch 195: train loss: 0.2293, acc 0.9941\n",
            "                                                                Get pseudo label: 99678\n",
            "12054, 10003, 9148, 9417, 8659, 10926, 10112, 9300, 10699, 9356\n",
            "epoch 196: train loss: 0.2314, acc 0.9934\n",
            "                                                                Get pseudo label: 99656\n",
            "12197, 10011, 9113, 9422, 8662, 10891, 10110, 9338, 10629, 9277\n",
            "epoch 197: train loss: 0.2269, acc 0.9940\n",
            "                                                                Get pseudo label: 99671\n",
            "12169, 10049, 9085, 9449, 8633, 10868, 10110, 9351, 10664, 9288\n",
            "epoch 198: train loss: 0.2244, acc 0.9938\n",
            "                                                                Get pseudo label: 99699\n",
            "12062, 10090, 9117, 9401, 8668, 10869, 10111, 9382, 10670, 9325\n",
            "epoch 199: train loss: 0.2290, acc 0.9934\n",
            "                                                                Get pseudo label: 99696\n",
            "12055, 10066, 9144, 9425, 8678, 10908, 10114, 9280, 10726, 9295\n",
            "epoch 200: train loss: 0.2315, acc 0.9933\n",
            "                                                                Get pseudo label: 99686\n",
            "12177, 10071, 9144, 9399, 8680, 10851, 10114, 9304, 10646, 9294\n",
            "epoch 201: train loss: 0.2278, acc 0.9938\n",
            "                                                                Get pseudo label: 99719\n",
            "12221, 10079, 9141, 9331, 8684, 10869, 10113, 9317, 10674, 9286\n",
            "epoch 202: train loss: 0.2306, acc 0.9938\n",
            "                                                                Get pseudo label: 99709\n",
            "12068, 10075, 9135, 9287, 8690, 10918, 10115, 9362, 10714, 9341\n",
            "epoch 203: train loss: 0.2288, acc 0.9938\n",
            "                                                                Get pseudo label: 99707\n",
            "12150, 10044, 9129, 9311, 8682, 10910, 10112, 9315, 10731, 9319\n",
            "epoch 204: train loss: 0.2299, acc 0.9943\n",
            "                                                                Get pseudo label: 99725\n",
            "12064, 10081, 9143, 9302, 8670, 10870, 10111, 9396, 10796, 9288\n",
            "epoch 205: train loss: 0.2300, acc 0.9943\n",
            "                                                                Get pseudo label: 99709\n",
            "12010, 10068, 9147, 9334, 8674, 10896, 10116, 9323, 10769, 9367\n",
            "epoch 206: train loss: 0.2222, acc 0.9944\n",
            "                                                                Get pseudo label: 99723\n",
            "12057, 10044, 9134, 9365, 8685, 10916, 10112, 9276, 10752, 9377\n",
            "epoch 207: train loss: 0.2301, acc 0.9947\n",
            "                                                                Get pseudo label: 99745\n",
            "12133, 9983, 9145, 9368, 8656, 10945, 10113, 9269, 10750, 9377\n",
            "epoch 208: train loss: 0.2266, acc 0.9947\n",
            "                                                                Get pseudo label: 99753\n",
            "12085, 10072, 9176, 9314, 8675, 10906, 10111, 9312, 10775, 9322\n",
            "epoch 209: train loss: 0.2270, acc 0.9946\n",
            "                                                                Get pseudo label: 99750\n",
            "12061, 10091, 9153, 9350, 8670, 10937, 10108, 9284, 10787, 9306\n",
            "epoch 210: train loss: 0.2266, acc 0.9944\n",
            "                                                                Get pseudo label: 99729\n",
            "11977, 10065, 9164, 9324, 8690, 10954, 10111, 9325, 10838, 9277\n",
            "epoch 211: train loss: 0.2292, acc 0.9942\n",
            "                                                                Get pseudo label: 99722\n",
            "12014, 10104, 9182, 9287, 8701, 10903, 10112, 9380, 10810, 9224\n",
            "epoch 212: train loss: 0.2278, acc 0.9944\n",
            "                                                                Get pseudo label: 99751\n",
            "12033, 10096, 9166, 9354, 8672, 10917, 10110, 9357, 10802, 9238\n",
            "epoch 213: train loss: 0.2307, acc 0.9947\n",
            "                                                                 Get pseudo label: 99765\n",
            "12050, 10055, 9157, 9364, 8666, 10925, 10110, 9344, 10805, 9284\n",
            "epoch 214: train loss: 0.2240, acc 0.9949\n",
            "                                                                Get pseudo label: 99737\n",
            "12062, 10160, 9160, 9360, 8670, 10839, 10112, 9303, 10765, 9302\n",
            "epoch 215: train loss: 0.2249, acc 0.9950\n",
            "                                                                Get pseudo label: 99775\n",
            "12144, 10091, 9191, 9339, 8675, 10867, 10111, 9283, 10761, 9308\n",
            "epoch 216: train loss: 0.2262, acc 0.9950\n",
            "                                                                Get pseudo label: 99744\n",
            "12162, 10081, 9203, 9330, 8639, 10887, 10102, 9355, 10726, 9254\n",
            "epoch 217: train loss: 0.2248, acc 0.9946\n",
            "                                                                Get pseudo label: 99801\n",
            "12132, 10072, 9163, 9363, 8677, 10896, 10105, 9325, 10763, 9301\n",
            "epoch 218: train loss: 0.2300, acc 0.9950\n",
            "                                                                Get pseudo label: 99806\n",
            "12051, 10119, 9169, 9346, 8664, 10892, 10112, 9353, 10773, 9322\n",
            "epoch 219: train loss: 0.2279, acc 0.9948\n",
            "                                                                Get pseudo label: 99810\n",
            "12052, 10129, 9167, 9343, 8670, 10906, 10110, 9375, 10786, 9268\n",
            "epoch 220: train loss: 0.2230, acc 0.9951\n",
            "                                                                Get pseudo label: 99791\n",
            "12052, 10126, 9155, 9323, 8660, 10891, 10114, 9378, 10767, 9321\n",
            "epoch 221: train loss: 0.2243, acc 0.9953\n",
            "                                                                Get pseudo label: 99788\n",
            "12081, 10136, 9182, 9307, 8670, 10888, 10112, 9367, 10749, 9291\n",
            "epoch 222: train loss: 0.2259, acc 0.9951\n",
            "                                                                Get pseudo label: 99766\n",
            "12119, 10093, 9170, 9255, 8665, 10908, 10111, 9387, 10748, 9306\n",
            "epoch 223: train loss: 0.2265, acc 0.9953\n",
            "                                                                Get pseudo label: 99761\n",
            "12105, 10045, 9156, 9259, 8687, 10943, 10111, 9441, 10757, 9252\n",
            "epoch 224: train loss: 0.2283, acc 0.9952\n",
            "                                                                 Get pseudo label: 99790\n",
            "12119, 10076, 9162, 9253, 8683, 10923, 10116, 9389, 10757, 9307\n",
            "epoch 225: train loss: 0.2264, acc 0.9952\n",
            "                                                                Get pseudo label: 99768\n",
            "12076, 10077, 9165, 9253, 8666, 10905, 10114, 9407, 10756, 9344\n",
            "epoch 226: train loss: 0.2213, acc 0.9951\n",
            "                                                                Get pseudo label: 99780\n",
            "12127, 10095, 9168, 9227, 8673, 10870, 10110, 9422, 10779, 9304\n",
            "epoch 227: train loss: 0.2305, acc 0.9953\n",
            "                                                                Get pseudo label: 99796\n",
            "12204, 10082, 9160, 9261, 8649, 10875, 10107, 9342, 10745, 9366\n",
            "epoch 228: train loss: 0.2273, acc 0.9953\n",
            "                                                                 Get pseudo label: 99812\n",
            "12231, 10056, 9172, 9255, 8660, 10885, 10111, 9397, 10720, 9321\n",
            "epoch 229: train loss: 0.2251, acc 0.9953\n",
            "                                                                Get pseudo label: 99810\n",
            "12223, 10050, 9159, 9267, 8671, 10910, 10108, 9404, 10712, 9300\n",
            "epoch 230: train loss: 0.2217, acc 0.9955\n",
            "                                                                Get pseudo label: 99839\n",
            "12187, 10075, 9157, 9319, 8683, 10901, 10113, 9387, 10730, 9282\n",
            "epoch 231: train loss: 0.2311, acc 0.9955\n",
            "                                                                 Get pseudo label: 99806\n",
            "12116, 10098, 9163, 9341, 8698, 10892, 10112, 9345, 10742, 9294\n",
            "epoch 232: train loss: 0.2236, acc 0.9956\n",
            "                                                                Get pseudo label: 99815\n",
            "12198, 10061, 9144, 9381, 8690, 10895, 10112, 9295, 10713, 9323\n",
            "epoch 233: train loss: 0.2210, acc 0.9955\n",
            "Get pseudo label: 99807\n",
            "12170, 10061, 9140, 9390, 8686, 10921, 10111, 9320, 10724, 9279\n",
            "epoch 234: train loss: 0.2244, acc 0.9960\n",
            "                                                                Get pseudo label: 99814\n",
            "12102, 10090, 9147, 9379, 8686, 10918, 10112, 9363, 10779, 9234\n",
            "epoch 235: train loss: 0.2220, acc 0.9957\n",
            "                                                                Get pseudo label: 99816\n",
            "12089, 10110, 9152, 9375, 8672, 10920, 10112, 9375, 10758, 9247\n",
            "epoch 236: train loss: 0.2301, acc 0.9959\n",
            "                                                                Get pseudo label: 99806\n",
            "12086, 10125, 9159, 9378, 8674, 10893, 10110, 9336, 10768, 9272\n",
            "epoch 237: train loss: 0.2245, acc 0.9961\n",
            "                                                                Get pseudo label: 99835\n",
            "12156, 10089, 9152, 9383, 8690, 10890, 10113, 9300, 10757, 9301\n",
            "epoch 238: train loss: 0.2212, acc 0.9959\n",
            "                                                                Get pseudo label: 99833\n",
            "12212, 10065, 9150, 9376, 8671, 10908, 10109, 9293, 10734, 9310\n",
            "epoch 239: train loss: 0.2204, acc 0.9962\n",
            "                                                                Get pseudo label: 99830\n",
            "12198, 10089, 9165, 9372, 8670, 10906, 10107, 9291, 10742, 9284\n",
            "epoch 240: train loss: 0.2227, acc 0.9957\n",
            "                                                                Get pseudo label: 99849\n",
            "12123, 10063, 9161, 9368, 8685, 10927, 10112, 9346, 10789, 9269\n",
            "epoch 241: train loss: 0.2233, acc 0.9962\n",
            "                                                                Get pseudo label: 99836\n",
            "12127, 10065, 9163, 9380, 8679, 10921, 10111, 9351, 10765, 9270\n",
            "epoch 242: train loss: 0.2226, acc 0.9962\n",
            "                                                                Get pseudo label: 99841\n",
            "12135, 10069, 9161, 9371, 8675, 10910, 10114, 9348, 10759, 9295\n",
            "epoch 243: train loss: 0.2259, acc 0.9963\n",
            "                                                                Get pseudo label: 99834\n",
            "12121, 10062, 9157, 9378, 8677, 10927, 10113, 9329, 10786, 9278\n",
            "epoch 244: train loss: 0.2210, acc 0.9962\n",
            "                                                                Get pseudo label: 99843\n",
            "12138, 10057, 9163, 9369, 8671, 10930, 10111, 9331, 10767, 9302\n",
            "epoch 245: train loss: 0.2234, acc 0.9962\n",
            "                                                                Get pseudo label: 99852\n",
            "12131, 10091, 9160, 9380, 8655, 10914, 10111, 9336, 10774, 9296\n",
            "epoch 246: train loss: 0.2220, acc 0.9961\n",
            "                                                                 Get pseudo label: 99859\n",
            "12181, 10073, 9157, 9359, 8664, 10914, 10111, 9333, 10758, 9304\n",
            "epoch 247: train loss: 0.2230, acc 0.9964\n",
            "                                                                Get pseudo label: 99853\n",
            "12164, 10100, 9155, 9373, 8678, 10908, 10113, 9317, 10755, 9285\n",
            "epoch 248: train loss: 0.2255, acc 0.9963\n",
            "Get pseudo label: 99835\n",
            "12146, 10093, 9157, 9380, 8677, 10914, 10112, 9315, 10763, 9273\n",
            "epoch 249: train loss: 0.2176, acc 0.9963\n",
            "                                                                Get pseudo label: 99851\n",
            "12142, 10099, 9148, 9382, 8683, 10912, 10112, 9331, 10769, 9269\n",
            "epoch 250: train loss: 0.2239, acc 0.9963\n",
            "                                                                Get pseudo label: 99860\n",
            "12153, 10079, 9152, 9371, 8674, 10917, 10113, 9313, 10772, 9311\n",
            "epoch 251: train loss: 0.2201, acc 0.9962\n",
            "                                                                Get pseudo label: 99835\n",
            "12150, 10076, 9164, 9353, 8675, 10907, 10112, 9344, 10769, 9280\n",
            "epoch 252: train loss: 0.2265, acc 0.9965\n",
            "                                                                Get pseudo label: 99857\n",
            "12157, 10089, 9169, 9351, 8674, 10892, 10112, 9362, 10754, 9291\n",
            "epoch 253: train loss: 0.2205, acc 0.9967\n",
            "                                                                Get pseudo label: 99858\n",
            "12155, 10075, 9165, 9365, 8680, 10910, 10113, 9341, 10753, 9297\n",
            "epoch 254: train loss: 0.2284, acc 0.9959\n",
            "                                                                Get pseudo label: 99848\n",
            "12156, 10086, 9154, 9379, 8672, 10904, 10110, 9331, 10767, 9284\n",
            "epoch 255: train loss: 0.2206, acc 0.9964\n",
            "                                                                Get pseudo label: 99879\n",
            "12143, 10121, 9147, 9403, 8670, 10890, 10112, 9325, 10770, 9293\n",
            "epoch 256: train loss: 0.2192, acc 0.9963\n",
            "Get pseudo label: 99874\n",
            "12174, 10094, 9159, 9383, 8683, 10888, 10114, 9315, 10747, 9312\n",
            "epoch 257: train loss: 0.2226, acc 0.9961\n",
            "Get pseudo label: 99875\n",
            "12185, 10097, 9157, 9370, 8675, 10895, 10112, 9358, 10733, 9288\n",
            "epoch 258: train loss: 0.2270, acc 0.9965\n",
            "                                                                Get pseudo label: 99843\n",
            "12171, 10091, 9151, 9377, 8671, 10908, 10113, 9326, 10729, 9299\n",
            "epoch 259: train loss: 0.2248, acc 0.9963\n",
            "                                                                Get pseudo label: 99879\n",
            "12183, 10101, 9159, 9374, 8672, 10902, 10112, 9330, 10739, 9301\n",
            "epoch 260: train loss: 0.2259, acc 0.9962\n",
            "                                                                Get pseudo label: 99842\n",
            "12168, 10084, 9161, 9360, 8675, 10904, 10113, 9327, 10744, 9301\n",
            "epoch 261: train loss: 0.2236, acc 0.9963\n",
            "                                                                Get pseudo label: 99879\n",
            "12187, 10099, 9169, 9346, 8679, 10898, 10112, 9347, 10731, 9306\n",
            "epoch 262: train loss: 0.2219, acc 0.9967\n",
            "                                                                Get pseudo label: 99878\n",
            "12162, 10102, 9165, 9367, 8682, 10902, 10113, 9337, 10747, 9296\n",
            "epoch 263: train loss: 0.2230, acc 0.9966\n",
            "                                                                Get pseudo label: 99844\n",
            "12180, 10087, 9161, 9382, 8675, 10889, 10109, 9320, 10730, 9306\n",
            "epoch 264: train loss: 0.2211, acc 0.9967\n",
            "                                                                Get pseudo label: 99859\n",
            "12154, 10083, 9164, 9374, 8682, 10913, 10110, 9340, 10746, 9287\n",
            "epoch 265: train loss: 0.2190, acc 0.9970\n",
            "                                                                 Get pseudo label: 99841\n",
            "12130, 10087, 9163, 9366, 8679, 10904, 10111, 9365, 10756, 9275\n",
            "epoch 266: train loss: 0.2179, acc 0.9969\n",
            "                                                                Get pseudo label: 99864\n",
            "12124, 10094, 9160, 9368, 8682, 10913, 10111, 9349, 10763, 9295\n",
            "epoch 267: train loss: 0.2236, acc 0.9967\n",
            "                                                                Get pseudo label: 99888\n",
            "12142, 10078, 9163, 9366, 8673, 10921, 10112, 9357, 10762, 9309\n",
            "epoch 268: train loss: 0.2246, acc 0.9967\n",
            "                                                                Get pseudo label: 99885\n",
            "12147, 10088, 9162, 9363, 8675, 10914, 10111, 9359, 10755, 9307\n",
            "epoch 269: train loss: 0.2202, acc 0.9969\n",
            "Get pseudo label: 99874\n",
            "12126, 10084, 9164, 9371, 8678, 10914, 10111, 9327, 10781, 9313\n",
            "epoch 270: train loss: 0.2228, acc 0.9966\n",
            "                                                                Get pseudo label: 99848\n",
            "12139, 10068, 9160, 9368, 8669, 10915, 10110, 9341, 10758, 9315\n",
            "epoch 271: train loss: 0.2257, acc 0.9968\n",
            "                                                                Get pseudo label: 99872\n",
            "12120, 10093, 9164, 9364, 8671, 10905, 10112, 9346, 10773, 9319\n",
            "epoch 272: train loss: 0.2295, acc 0.9966\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9862fc73e9ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# if True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.9\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msemi_flg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mpseudo_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_semi_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Get pseudo label: {len(pseudo_dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{int(c):4d}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9b6eb062d6b7>\u001b[0m in \u001b[0;36mget_semi_set\u001b[0;34m(model, threshold)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# source_dataset.transform = tfm_weak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     pseudo_dataset, count = get_pseudo_labels(\n\u001b[0m\u001b[1;32m     42\u001b[0m         model, target_dataloader_nshuf, threshold=threshold)\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpseudo_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9b6eb062d6b7>\u001b[0m in \u001b[0;36mget_pseudo_labels\u001b[0;34m(model, dataloader, threshold)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mselect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprobs1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8_-0iSSje4w"
      },
      "source": [
        "# Inference\n",
        "\n",
        "就跟前幾次作業一樣。這裡我使用pd來生產csv，因為看起來比較潮(?)\n",
        "\n",
        "此外，200 epochs的Accuracy可能會不太穩定，可以多丟幾次或train久一點。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wly5AgH2jePv"
      },
      "source": [
        "model = DomainAdapt()\n",
        "model.load().cuda()\n",
        "\n",
        "result = []\n",
        "model.eval()\n",
        "logits = []\n",
        "for i, (test_data, _) in enumerate(tqdm(test_dataloader)):\n",
        "    test_data = test_data.cuda()\n",
        "\n",
        "    class_logits = model(test_data)\n",
        "    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
        "    logits.append(class_logits.cpu().detach())\n",
        "    result.append(x)\n",
        "\n",
        "import pandas as pd\n",
        "result = np.concatenate(result)\n",
        "logits = torch.cat(logits, axis=0)\n",
        "\n",
        "# Generate your submission\n",
        "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
        "df.to_csv('DaNN_submission.csv',index=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtXnEMUNCE78"
      },
      "source": [
        "# Training Statistics\n",
        "\n",
        "- Number of parameters:\n",
        "  - Feature Extractor: 2, 142, 336\n",
        "  - Label Predictor: 530, 442\n",
        "  - Domain Classifier: 1, 055, 233\n",
        "\n",
        "- Simple\n",
        " - Training time on colab: ~ 1 hr\n",
        "- Medium\n",
        " - Training time on colab: 2 ~ 4 hr\n",
        "- Strong\n",
        " - Training time on colab: 5 ~ 6 hrs\n",
        "- Boss\n",
        " - **Unmeasurable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duk7k43Am9xH"
      },
      "source": [
        "# Learning Curve (Strong Baseline)\n",
        "* This method is slightly different from colab.\n",
        "\n",
        "![Loss Curve](https://i.imgur.com/vIujQyo.png)\n",
        "\n",
        "# Accuracy Curve (Strong Baseline)\n",
        "* Note that you cannot access testing accuracy. But this plot tells you that even though the model overfits the training data, the testing accuracy is still improving, and that's why you need to train more epochs.\n",
        "\n",
        "![Acc Curve](https://i.imgur.com/4W1otXG.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6UfXzef-wNl"
      },
      "source": [
        "# Q&A\n",
        "\n",
        "有任何問題 Domain Adaptation 的問題可以寄信到ntu-ml-2021spring-ta@googlegroups.com。\n",
        "\n",
        "時間允許的話我會更新在這裡。\n",
        "\n",
        "# Special Thanks\n",
        "這次的作業其實是我出在 2019FALL 的 ML Final Project，以下是我認為在 Final Report 不錯的幾組，有興趣的話歡迎大家參考看看。\n",
        "\n",
        "[NTU_r08942071_太神啦 / 組長: 劉正仁同學](https://drive.google.com/open?id=11uNDcz7_eMS8dMQxvnWsbrdguu9k4c-c)\n",
        "\n",
        "[NTU_r08921a08_CAT / 組長: 廖子毅同學](https://drive.google.com/open?id=1xIkSs8HAShdcfV1E0NEnf4JDbL7POZTf)\n"
      ]
    }
  ]
}